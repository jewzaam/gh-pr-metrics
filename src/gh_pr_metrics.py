#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub Pull Request Metrics Tool

A command-line tool to generate CSV reports containing key metrics for all pull
requests in a GitHub repository within a specified time range.
"""

import argparse
import json
import logging
import os
import subprocess
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

from dateutil import parser as date_parser

from github_api import (
    GitHubAPIError,
    API_CALLS_PER_PR,
)

from managers import (
    Config,
    ConditionalBotConfig,
    load_config,
    StateManager,
    QuotaManager,
    CSVManager,
    LoggingManager,
)

from data import (
    PRFetcher,
    MetricsGenerator,
    count_comments_from_json,
    get_review_metrics_from_json,
    determine_pr_status,
    get_ready_for_review_time as _get_ready_for_review_time_new,
    PR_NUMBER_FIELD,
)


# Backward-compatible wrapper for get_ready_for_review_time
# (old tests use old signature, will be updated in Step 8)
def get_ready_for_review_time(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> str:
    """Backward-compatible wrapper for get_ready_for_review_time."""
    return _get_ready_for_review_time_new(pr, owner, repo, token, github_client, logger)


# Re-export for backward compatibility with tests
__all__ = [
    "Config",
    "ConditionalBotConfig",
    "load_config",
    "StateManager",
    "QuotaManager",
    "CSVManager",
    "LoggingManager",
    "PRFetcher",
    "MetricsGenerator",
    "count_comments_from_json",
    "get_review_metrics_from_json",
    "determine_pr_status",
    "get_ready_for_review_time",
    "PR_NUMBER_FIELD",
]


# Version
__version__ = "0.1.0"

# Constants
DEFAULT_WORKERS = 4
DEFAULT_RAW_DATA_DIR = "data/raw"
STATE_FILE = Path.home() / ".gh-pr-metrics-state.yaml"
MAX_CHUNK_DAYS = 30  # Maximum days to process in a single chunk


# ============================================================================
# Configuration Management (now in managers.config_manager)
# ============================================================================

# Imported from managers package

# ============================================================================
# State Management (now in managers.state_manager)
# ============================================================================

# Imported from managers package

# ============================================================================
# CSV Management (now in managers.csv_manager)
# ============================================================================

# Imported from managers package

# ============================================================================
# PR Data Fetching and JSON Building
# ============================================================================


def build_pr_json_data(pr: Dict[str, Any], owner: str, repo: str) -> Dict[str, Any]:
    """
    Build complete PR JSON data including all comments and reviews.

    Args:
        pr: Base PR object from GitHub API
        owner: Repository owner
        repo: Repository name

    Returns:
        Complete PR data dictionary ready for JSON serialization
    """
    pr_number = pr["number"]

    # Fetch issue comments
    issue_comments = []
    fetch_errors = []
    try:
        comments = github_client.fetch_issue_comments(pr["comments_url"])
        for comment in comments:
            # Handle cases where user field is None (deleted/ghost users)
            user = comment.get("user") or {}
            issue_comments.append(
                {
                    "id": comment.get("id"),
                    "user": user.get("login"),
                    "user_type": user.get("type"),
                    "body": comment.get("body", ""),
                    "created_at": comment.get("created_at"),
                    "updated_at": comment.get("updated_at"),
                }
            )
    except GitHubAPIError as e:
        logger.warning("Failed to fetch issue comments for PR #%d: %s", pr_number, e)
        fetch_errors.append(f"issue_comments: {e}")

    # Fetch review comments
    review_comments = []
    try:
        comments = github_client.fetch_review_comments(pr["review_comments_url"])
        for comment in comments:
            # Handle cases where user field is None (deleted/ghost users)
            user = comment.get("user") or {}
            review_comments.append(
                {
                    "id": comment.get("id"),
                    "user": user.get("login"),
                    "user_type": user.get("type"),
                    "body": comment.get("body", ""),
                    "path": comment.get("path"),
                    "line": comment.get("line"),
                    "created_at": comment.get("created_at"),
                    "updated_at": comment.get("updated_at"),
                }
            )
    except GitHubAPIError as e:
        logger.warning("Failed to fetch review comments for PR #%d: %s", pr_number, e)
        fetch_errors.append(f"review_comments: {e}")

    # Fetch reviews
    reviews = []
    try:
        review_list = github_client.fetch_reviews(owner, repo, pr_number)
        for review in review_list:
            # Handle cases where user field is None (deleted/ghost users)
            user = review.get("user") or {}
            reviews.append(
                {
                    "id": review.get("id"),
                    "user": user.get("login"),
                    "user_type": user.get("type"),
                    "state": review.get("state"),
                    "body": review.get("body", ""),
                    "submitted_at": review.get("submitted_at"),
                }
            )
    except GitHubAPIError as e:
        logger.warning("Failed to fetch reviews for PR #%d: %s", pr_number, e)
        fetch_errors.append(f"reviews: {e}")

    # Build complete PR data
    return {
        "pr_number": pr_number,
        "title": pr["title"],
        "author": pr.get("user", {}).get("login"),
        "author_type": pr.get("user", {}).get("type"),
        "state": pr["state"],
        "created_at": pr["created_at"],
        "updated_at": pr["updated_at"],
        "merged_at": pr.get("merged_at"),
        "closed_at": pr.get("closed_at"),
        "draft": pr.get("draft", False),
        "url": pr["html_url"],
        "additions": pr.get("additions", 0),
        "deletions": pr.get("deletions", 0),
        "changed_files": pr.get("changed_files", 0),
        "issue_comments": issue_comments,
        "review_comments": review_comments,
        "reviews": reviews,
    }


def write_pr_json(raw_data_dir: str, owner: str, repo: str, pr_data: Dict[str, Any]) -> None:
    """
    Write PR data to JSON file in provider-aware directory structure.

    Args:
        raw_data_dir: Base directory for raw data (e.g., "data/raw")
        owner: Repository owner
        repo: Repository name
        pr_data: Complete PR data dictionary
    """
    # Provider-aware path: raw_data_dir/github.com/owner/repo/
    output_dir = Path(raw_data_dir) / "github.com" / owner / repo
    output_dir.mkdir(parents=True, exist_ok=True)

    pr_number = pr_data["pr_number"]
    json_file = output_dir / f"{pr_number}.json"

    try:
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(pr_data, f, indent=2, ensure_ascii=False)
        logger.debug("Wrote PR #%d JSON to %s", pr_number, json_file)
    except Exception as e:
        logger.warning("Failed to write PR #%d JSON: %s", pr_number, e)


# ============================================================================
# Global Manager Instances
# ============================================================================

quota_manager = QuotaManager()
logger = LoggingManager(quota_manager)
state_manager = StateManager(logger=logger)
github_client = None  # Initialized in main() with token, quota_manager, and logger
csv_manager = CSVManager(logger=logger)


# ============================================================================
# PR Processing Functions
# ============================================================================


def count_comments(
    pr: Dict[str, Any],
    owner: str,
    repo: str,
    token: Optional[str],
    config: Config,
    reviews: List[Dict[str, Any]] = None,
) -> tuple[int, int, int, str, str]:
    """
    Count total comments, non-AI bot comments, and AI bot comments.
    Returns (total_comments, non_ai_bot_comments, ai_bot_comments,
             non_ai_bot_login_names, ai_bot_login_names).
    Non-AI bot and AI bot login names are comma-separated strings.
    Uses config for AI bot detection (both always and conditional patterns).

    Args:
        reviews: Optional pre-fetched reviews list to avoid duplicate API calls
    """
    total_comments = 0
    non_ai_bot_comments = 0
    ai_bot_comments = 0
    non_ai_bot_logins = set()
    ai_bot_logins = set()

    # Issue comments (general PR comments)
    comments_url = pr["comments_url"]
    try:
        comments = github_client.fetch_issue_comments(comments_url)
        for comment in comments:
            total_comments += 1
            user = comment.get("user") or {}
            if user.get("type") == "Bot":
                login = user.get("login")
                if not login:
                    continue  # Skip if user login is not available
                comment_body = comment.get("body", "")

                is_ai = config.is_ai_bot(login, comment_body)
                logger.debug(
                    "PR #%d issue comment: bot=%s, is_ai=%s, body_preview=%s",
                    pr["number"],
                    login,
                    is_ai,
                    comment_body[:100] if comment_body else "(empty)",
                )

                if is_ai:
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch comments for PR #%d: %s", pr["number"], e)

    # Review comments (inline code comments)
    review_comments_url = pr["review_comments_url"]
    try:
        review_comments = github_client.fetch_review_comments(review_comments_url)
        for comment in review_comments:
            total_comments += 1
            user = comment.get("user") or {}
            if user.get("type") == "Bot":
                login = user.get("login")
                if not login:
                    continue  # Skip if user login is not available
                comment_body = comment.get("body", "")

                is_ai = config.is_ai_bot(login, comment_body)
                logger.debug(
                    "PR #%d review comment: bot=%s, is_ai=%s, body_preview=%s",
                    pr["number"],
                    login,
                    is_ai,
                    comment_body[:100] if comment_body else "(empty)",
                )

                if is_ai:
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch review comments for PR #%d: %s", pr["number"], e)

    # Review bodies (summary text in reviews)
    # Use pre-fetched reviews if provided, otherwise fetch
    if reviews is None:
        try:
            reviews = github_client.fetch_reviews(owner, repo, pr["number"])
        except GitHubAPIError as e:
            logger.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
            reviews = []

    for review in reviews:
        # Only count reviews with body text as comments
        review_body = review.get("body", "")
        user = review.get("user") or {}
        if review_body and user.get("type") == "Bot":
            total_comments += 1
            login = user.get("login")
            if not login:
                continue  # Skip if user login is not available

            is_ai = config.is_ai_bot(login, review_body)
            logger.debug(
                "PR #%d review body: bot=%s, is_ai=%s, body_preview=%s",
                pr["number"],
                login,
                is_ai,
                review_body[:100] if review_body else "(empty)",
            )

            if is_ai:
                ai_bot_logins.add(login)
                ai_bot_comments += 1
            else:
                non_ai_bot_logins.add(login)
                non_ai_bot_comments += 1

    # Convert to comma-separated strings, sorted for consistency
    non_ai_bot_login_names = ",".join(sorted(non_ai_bot_logins))
    ai_bot_login_names = ",".join(sorted(ai_bot_logins))

    return (
        total_comments,
        non_ai_bot_comments,
        ai_bot_comments,
        non_ai_bot_login_names,
        ai_bot_login_names,
    )


def get_review_metrics(
    pr: Dict[str, Any],
    owner: str,
    repo: str,
    token: Optional[str],
    reviews: List[Dict[str, Any]] = None,
) -> tuple[int, int, int]:
    """
    Get review metrics: changes requested count, unique change requesters, approvals.
    Returns (changes_requested_count, unique_change_requesters, approval_count).

    Args:
        reviews: Optional pre-fetched reviews list to avoid duplicate API calls
    """
    # Use pre-fetched reviews if provided, otherwise fetch
    if reviews is None:
        try:
            reviews = github_client.fetch_reviews(owner, repo, pr["number"])
        except GitHubAPIError as e:
            logger.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
            return 0, 0, 0

    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r["user"]["login"]
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review["user"]["login"]
            # Later reviews override earlier ones
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def process_pr(
    pr: Dict[str, Any],
    pr_json_data: Dict[str, Any],
    owner: str,
    repo: str,
    token: Optional[str],
    config: Config,
) -> Dict[str, Any]:
    """
    Process a single PR and extract metrics from fetched JSON data.

    Args:
        pr: Base PR object
        pr_json_data: Complete PR data from build_pr_json_data (includes comments, reviews)
        owner: Repository owner
        repo: Repository name
        token: GitHub token
        config: Configuration object

    Returns:
        Metrics dictionary for CSV
    """
    pr_number = pr["number"]
    logger.debug("Processing PR #%d: %s", pr_number, pr["title"])

    errors = []
    metrics: Dict[str, Any] = {
        PR_NUMBER_FIELD: pr_number,
        "title": pr["title"],
        "author": pr["user"]["login"] if pr.get("user") else "",
        "created_at": pr["created_at"],
        "url": pr["html_url"],
    }

    # Ready for review time
    try:
        metrics["ready_for_review_at"] = get_ready_for_review_time(pr, owner, repo, token)
    except Exception as e:
        logger.debug("Error getting ready time for PR #%d: %s", pr_number, e)
        metrics["ready_for_review_at"] = pr["created_at"]
        errors.append(f"ready_time: {e}")

    # Use reviews from pr_json_data
    reviews = pr_json_data.get("reviews", [])

    # Comment counts (including review bodies) - use data from pr_json_data
    try:
        total_comments, non_ai_bot_comments, ai_bot_comments, non_ai_bot_names, ai_bot_names = (
            count_comments_from_json(pr_json_data, config)
        )
        metrics["total_comment_count"] = total_comments
        metrics["non_ai_bot_comment_count"] = non_ai_bot_comments
        metrics["ai_bot_comment_count"] = ai_bot_comments
        metrics["non_ai_bot_login_names"] = non_ai_bot_names
        metrics["ai_bot_login_names"] = ai_bot_names
    except Exception as e:
        logger.debug("Error counting comments for PR #%d: %s", pr_number, e)
        metrics["total_comment_count"] = 0
        metrics["non_ai_bot_comment_count"] = 0
        metrics["ai_bot_comment_count"] = 0
        metrics["non_ai_bot_login_names"] = ""
        metrics["ai_bot_login_names"] = ""
        errors.append(f"comments: {e}")

    # Review metrics - use reviews from pr_json_data
    try:
        changes_requested, unique_requesters, approvals = get_review_metrics_from_json(reviews)
        metrics["changes_requested_count"] = changes_requested
        metrics["unique_change_requesters"] = unique_requesters
        metrics["approval_count"] = approvals
    except Exception as e:
        logger.debug("Error getting reviews for PR #%d: %s", pr_number, e)
        metrics["changes_requested_count"] = 0
        metrics["unique_change_requesters"] = 0
        metrics["approval_count"] = 0
        errors.append(f"reviews: {e}")

    # Status
    metrics["status"] = determine_pr_status(pr)

    # Merged and closed timestamps
    metrics["merged_at"] = pr.get("merged_at") or ""
    metrics["closed_at"] = pr.get("closed_at") or ""

    # Calculate derived time metrics
    try:
        created_at = date_parser.parse(pr["created_at"])
        ready_at = date_parser.parse(metrics["ready_for_review_at"])

        # Determine end time (merged, closed, or now)
        if metrics["merged_at"]:
            end_time = date_parser.parse(metrics["merged_at"])
        elif metrics["closed_at"]:
            end_time = date_parser.parse(metrics["closed_at"])
        else:
            end_time = datetime.now(timezone.utc)

        # days_open: created → end
        days_open = (end_time - created_at).total_seconds() / 86400
        metrics["days_open"] = round(days_open, 2)

        # days_in_review: ready_for_review → end
        days_in_review = (end_time - ready_at).total_seconds() / 86400
        metrics["days_in_review"] = round(days_in_review, 2)

    except Exception as e:
        logger.debug("Error calculating time metrics for PR #%d: %s", pr_number, e)
        metrics["days_open"] = ""
        metrics["days_in_review"] = ""
        errors.append(f"time_metrics: {e}")

    # Complexity metrics
    metrics["lines_added"] = pr.get("additions", 0)
    metrics["lines_deleted"] = pr.get("deletions", 0)
    metrics["files_changed"] = pr.get("changed_files", 0)
    metrics["total_line_changes"] = metrics["lines_added"] + metrics["lines_deleted"]

    # Include fetch errors from PR JSON data if present
    if "fetch_errors" in pr_json_data:
        errors.extend(pr_json_data["fetch_errors"])

    # Errors
    metrics["errors"] = "; ".join(errors) if errors else ""

    return metrics


# ============================================================================
# Main Business Logic
# ============================================================================


def main() -> int:
    """
    Main entry point with subcommand architecture.

    Returns:
        Exit code (0 for success, non-zero for failure)
    """
    try:
        # Parse arguments
        command_name, args = parse_arguments()

        # Import commands
        from commands import InitCommand, FetchCommand, CsvCommand

        # Map command name to class
        command_map = {
            "init": InitCommand,
            "fetch": FetchCommand,
            "csv": CsvCommand,
        }

        # Get command class
        command_class = command_map.get(command_name)
        if not command_class:
            print(f"Unknown command: {command_name}", file=sys.stderr)
            return 1

        # Instantiate and run command
        command = command_class()
        command.setup(args)
        return command.run(args)

    except KeyboardInterrupt:
        print("\nInterrupted by user", file=sys.stderr)
        return 130
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        import traceback

        traceback.print_exc()
        return 1


# ============================================================================
# Utility Functions
# ============================================================================


def setup_logging(debug: bool = False, log_file: str = "gh-pr-metrics.log") -> None:
    """Configure logging based on debug flag with file and stderr output."""
    level = logging.DEBUG if debug else logging.INFO

    # Create formatters
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # Setup root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)

    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Add stderr handler
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(level)
    stderr_handler.setFormatter(formatter)
    root_logger.addHandler(stderr_handler)

    # Add file handler
    try:
        file_handler = logging.FileHandler(log_file, mode="a", encoding="utf-8")
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    except Exception as e:
        logging.warning(f"Could not create log file {log_file}: {e}")


def estimate_api_calls_for_prs(pr_count: int) -> int:
    """
    Estimate API calls needed to process pr_count PRs.
    Each PR requires: timeline events, reviews, review comments, issue comments.
    """
    return pr_count * API_CALLS_PER_PR


def get_github_token() -> Optional[str]:
    """Get GitHub token from environment."""
    return os.getenv("GITHUB_TOKEN")


def get_repo_from_git() -> tuple[Optional[str], Optional[str]]:
    """
    Get repository owner and name from git config.
    Returns (owner, repo) or (None, None) if not found.
    """
    try:
        remote_url = subprocess.check_output(
            ["git", "config", "--get", "remote.origin.url"],
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()

        # Parse SSH URL (git@github.com:owner/repo.git)
        if remote_url.startswith("git@"):
            _, path = remote_url.split(":", 1)
        # Parse HTTPS URL (https://github.com/owner/repo.git)
        elif remote_url.startswith("https://"):
            parts = remote_url.split("/")
            if len(parts) >= 5:
                path = f"{parts[3]}/{parts[4]}"
            else:
                return None, None
        else:
            return None, None

        # Remove .git suffix
        if path.endswith(".git"):
            path = path[:-4]

        owner, repo = path.split("/", 1)
        return owner, repo
    except Exception as e:
        logger.debug("Failed to get repo from git: %s", e)
        return None, None


def parse_timestamp(timestamp_str: str) -> datetime:
    """
    Parse various timestamp formats to datetime (always returns timezone-aware).

    Raises ValueError with clear message if timestamp cannot be parsed.
    """
    if not timestamp_str or not timestamp_str.strip():
        raise ValueError("Timestamp string is empty or None")

    try:
        # Try Unix timestamp first
        dt = datetime.fromtimestamp(float(timestamp_str))
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, OverflowError):
        try:
            # Try ISO 8601 / RFC 3339
            dt = date_parser.parse(timestamp_str)
            # Make timezone-aware if naive
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError) as e:
            raise ValueError(f"Invalid timestamp format: {timestamp_str}") from e


def expand_output_pattern(pattern: str, owner: str, repo: str) -> str:
    """
    Expand output pattern with owner and repo placeholders.
    Supports {owner} and {repo} placeholders.
    """
    return pattern.replace("{owner}", owner).replace("{repo}", repo)


# ============================================================================
# New Main Entry Point - Subcommand Architecture
# ============================================================================


def parse_arguments() -> tuple[str, argparse.Namespace]:
    """
    Parse command-line arguments with subcommand support.

    Returns:
        Tuple of (command_name, parsed_args)
    """
    parser = argparse.ArgumentParser(
        description="GitHub Pull Request Metrics Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # Global arguments
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging",
    )
    parser.add_argument(
        "--config",
        default=".gh-pr-metrics.yaml",
        help="Configuration file path (default: .gh-pr-metrics.yaml)",
    )
    parser.add_argument(
        "--version",
        action="version",
        version="gh-pr-metrics 0.1.0",
    )

    # Subcommands
    subparsers = parser.add_subparsers(dest="command", required=True, help="Command to execute")

    # Import commands
    from commands import InitCommand, FetchCommand, CsvCommand

    # Init command
    init_parser = subparsers.add_parser("init", help="Initialize repository tracking")
    init_cmd = InitCommand()
    init_cmd.add_arguments(init_parser)

    # Fetch command
    fetch_parser = subparsers.add_parser("fetch", help="Fetch PR data from GitHub")
    fetch_cmd = FetchCommand()
    fetch_cmd.add_arguments(fetch_parser)

    # CSV command
    csv_parser = subparsers.add_parser("csv", help="Generate CSV from cached JSON")
    csv_cmd = CsvCommand()
    csv_cmd.add_arguments(csv_parser)

    # Parse arguments
    args = parser.parse_args()

    return args.command, args
