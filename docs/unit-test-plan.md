# Unit Test Plan for GitHub Pull Request Metrics Tool

Generated By: Cursor (Claude Opus 4)

## Overview

This document outlines the comprehensive unit testing strategy for the GitHub Pull Request Metrics tool. The test plan follows established patterns from similar tools and ensures robust coverage of all functionality.

## Test Structure

### Test Organization
```
tests/
├── __init__.py
├── test_argument_parsing.py    # CLI argument parsing and validation
├── test_api_client.py          # GitHub API client functionality
├── test_data_processing.py     # PR data processing and metrics calculation
├── test_csv_generation.py      # CSV output generation
├── test_date_handling.py       # Date parsing and time range logic
├── test_error_handling.py      # Error conditions and recovery
├── test_main.py               # Main application flow
└── test_utils.py              # Utility functions
```

### Integration Tests (Separate)
```
integration_tests/
├── __init__.py
├── test_github_api.py         # Real GitHub API interactions
└── test_end_to_end.py         # Complete workflow testing
```

## Unit Test Categories

### 1. Argument Parsing Tests (`test_argument_parsing.py`)

#### 1.1 Repository Configuration
- **Test default repository detection from git config**
  - Mock subprocess calls to git commands
  - Test SSH URL parsing (git@github.com:owner/repo.git)
  - Test HTTPS URL parsing (https://github.com/owner/repo.git)
  - Test when no git repository is found
  
- **Test explicit repository parameters**
  - Valid owner/repo combinations
  - Invalid repository formats
  - Empty or missing values
  - Special characters in repo names

#### 1.2 Time Range Parsing
- **Test default time ranges**
  - Default start (1 year ago)
  - Default end (current time)
  
- **Test custom time formats**
  - ISO 8601 format (2024-01-01T00:00:00Z)
  - RFC 3339 format
  - Unix timestamps
  - Human-readable formats (if supported)
  - Invalid date formats
  - Start date after end date

#### 1.3 Output Configuration
- **Test output destinations**
  - Default stdout output
  - File path specification
  - Invalid file paths
  - Permission errors (mocked)

### 2. API Client Tests (`test_api_client.py`)

#### 2.1 Authentication
- **Test authentication methods**
  - No authentication (public repos)
  - Environment variable token
  - Invalid tokens
  - Missing tokens for private repos

#### 2.2 API Requests
- **Test request construction**
  - Correct headers
  - Proper URL formation
  - Query parameters
  - Pagination parameters

#### 2.3 Response Handling
- **Test successful responses**
  - Single page responses
  - Multi-page responses with pagination
  - Empty responses
  
- **Test error responses**
  - 404 Not Found
  - 401 Unauthorized
  - 403 Forbidden (rate limit)
  - 422 Validation Failed
  - 500 Server Error
  - Network timeouts

### 3. Data Processing Tests (`test_data_processing.py`)

#### 3.1 PR Data Extraction
- **Test PR basic information**
  - PR number, title, author
  - Created timestamp
  - Current state/status
  - URL construction

#### 3.2 Ready for Review Calculation
- **Test ready_for_review timestamp logic**
  - PR never in draft (use created_at)
  - PR converted from draft (find event)
  - Multiple draft/ready transitions
  - Missing event data

#### 3.3 Comment Counting
- **Test comment aggregation**
  - Issue comments only
  - Review comments only
  - Mixed comment types
  - Zero comments
- **Test bot comment detection**
  - Bot comments identified by type == "Bot"
  - Human comments (type == "User")
  - Mixed bot and human comments
  - Bot login names collected (comma-separated)
- **Test AI bot comment detection**
  - AI bot (cursor[bot]) identified correctly
  - AI bot comments counted separately
  - AI bot login names collected
  - Non-AI bot comments excluded from AI count

#### 3.4 Review Metrics
- **Test change request counting**
  - Single reviewer, single request
  - Multiple reviewers
  - Same reviewer multiple requests
  - No change requests
  
- **Test approval counting**
  - Single approval
  - Multiple approvals
  - Reviewer changing from request to approval
  - Most recent review state per reviewer

#### 3.5 Status Determination
- **Test PR status mapping**
  - Open PRs
  - Draft PRs
  - Closed without merge
  - Merged PRs
  - Edge cases

### 4. CSV Generation Tests (`test_csv_generation.py`)

#### 4.1 CSV Format
- **Test CSV structure**
  - Header row generation
  - Column ordering
  - Empty data handling
  
#### 4.2 Data Escaping
- **Test special character handling**
  - Commas in titles
  - Quotes in content
  - Newlines in descriptions
  - Unicode characters
  - Null/None values

#### 4.3 Output Writing
- **Test output destinations**
  - StringIO for stdout simulation
  - File writing (mocked)
  - Large dataset handling

### 5. Date Handling Tests (`test_date_handling.py`)

#### 5.1 Date Parsing
- **Test various input formats**
  - ISO 8601 with timezone
  - ISO 8601 without timezone
  - Unix timestamps
  - Relative dates (if supported)
  - Invalid formats

#### 5.2 Time Range Validation
- **Test range validation**
  - Valid ranges
  - Start after end
  - Future dates
  - Very old dates

#### 5.3 Timezone Handling
- **Test timezone conversions**
  - UTC normalization
  - Local timezone handling
  - Daylight saving time

### 6. Error Handling Tests (`test_error_handling.py`)

#### 6.1 Input Validation Errors
- **Test validation failures**
  - Invalid repository format
  - Invalid date format
  - Invalid output path
  - Missing required arguments

#### 6.2 API Errors
- **Test API failure handling**
  - Rate limit exceeded
  - Repository not found
  - Network failures
  - Authentication failures
  - Partial data failures

#### 6.3 Recovery Mechanisms
- **Test error recovery**
  - Retry logic for transient failures
  - Graceful degradation
  - Error logging
  - Partial result handling

### 7. Main Application Tests (`test_main.py`)

#### 7.1 Application Flow
- **Test complete workflows**
  - Successful execution
  - Early termination scenarios
  - Interrupt handling
  - Progress reporting

#### 7.2 Integration Points
- **Test component integration**
  - Argument parsing to API calls
  - API results to data processing
  - Processed data to CSV output
  - Error propagation

### 8. Utility Tests (`test_utils.py`)

#### 8.1 Helper Functions
- **Test utility functions**
  - Date formatting
  - String manipulation
  - Data validation
  - URL construction

## Mock Strategy

### External Dependencies
- **GitHub API**: Use `requests_mock` or similar
- **File System**: Use `mock_open` and `tempfile`
- **System Time**: Use `freezegun` or `mock`
- **Subprocess (git)**: Use `unittest.mock`
- **Environment Variables**: Use `mock.patch.dict`

### Test Data
- Create fixtures for:
  - Sample PR responses
  - Pagination headers
  - Event timelines
  - Review data
  - Error responses

## Coverage Goals

### Minimum Coverage Requirements
- **Overall**: 80% code coverage
- **Core Logic**: 90% coverage
- **Error Handling**: 85% coverage
- **Utilities**: 75% coverage

### Coverage Exclusions
- Main entry point boilerplate
- Debug/logging statements
- Platform-specific code branches

## Test Execution

### Running Tests
```bash
# Run all unit tests
python -m pytest tests/

# Run with coverage
python -m pytest tests/ --cov=gh_pr_metrics --cov-report=html

# Run specific test file
python -m pytest tests/test_api_client.py

# Run with verbose output
python -m pytest tests/ -v

# Run only marked tests
python -m pytest tests/ -m "not slow"
```

### Continuous Integration
- Run on all Python versions (3.8+)
- Run on multiple OS (Linux, macOS, Windows)
- Generate coverage reports
- Fail on coverage decrease

## Test Patterns and Best Practices

### Test Structure
```python
def test_descriptive_name(self):
    """Test that [specific behavior] works correctly."""
    # Arrange
    mock_data = create_test_data()
    
    # Act
    result = function_under_test(mock_data)
    
    # Assert
    assert result == expected_value
```

### Mocking Best Practices
- Mock at the boundaries (external services)
- Use realistic test data
- Test both success and failure paths
- Verify mock interactions when relevant

### Assertion Guidelines
- Use specific assertions
- Test one concept per test
- Include edge cases
- Use parameterized tests for similar scenarios

## Integration Test Strategy

### When to Use Integration Tests
- Testing real GitHub API behavior
- Validating end-to-end workflows
- Performance testing with real data
- Compatibility testing

### Integration Test Considerations
- Use separate test repository
- Respect API rate limits
- Make tests idempotent
- Use environment flags to skip in CI

## Future Test Considerations

### Performance Testing
- Large repository handling (10k+ PRs)
- Memory usage profiling
- API call optimization

### Security Testing
- Token handling
- Input sanitization
- CSV injection prevention

### Compatibility Testing
- Different Python versions
- Various GitHub Enterprise versions
- Platform-specific behavior
