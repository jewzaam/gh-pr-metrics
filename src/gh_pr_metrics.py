#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub Pull Request Metrics Tool

A command-line tool to generate CSV reports containing key metrics for all pull
requests in a GitHub repository within a specified time range.
"""

import argparse
import csv
import logging
import os
import subprocess
import sys
import tempfile
import threading
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

try:
    import argcomplete

    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False

import requests
from dateutil import parser as date_parser


# Version
__version__ = "0.1.0"

# Constants
GITHUB_API_BASE = "https://api.github.com"
DEFAULT_DAYS_BACK = 365
DEFAULT_WORKERS = 4
STATE_FILE = Path.home() / ".gh-pr-metrics-state.yaml"
PR_NUMBER_FIELD = "pr_number"
MAX_CHUNK_DAYS = 30  # Maximum days to process in a single chunk
API_CALLS_PER_PR = 4  # Estimate: timeline events, reviews, comments, review comments
API_SAFETY_BUFFER = 10  # Reserve for safety (unauthenticated limit is 60/hour total)
API_QUOTA_RESERVE_PCT = 0.05  # Reserve 5% of total quota, don't exhaust completely


class GitHubAPIError(Exception):
    """Exception raised for GitHub API errors."""

    pass


class QuotaManager:
    """
    Manages GitHub API quota tracking and enforcement.

    Tracks quota from response headers, provides quota checking,
    and supports waiting for quota reset.
    """

    def __init__(self):
        """Initialize quota manager with zero quota."""
        self._remaining = 0
        self._limit = 0
        self._reset = 0  # Unix timestamp when quota resets
        self._lock = threading.Lock()

    def get_quota_prefix(self) -> str:
        """Get quota prefix for logging (thread-safe)."""
        with self._lock:
            if self._limit == 0:
                return "[API ----/----]"

            # Left-pad remaining to match length of limit
            limit_str = str(self._limit)
            remaining_str = str(self._remaining).rjust(len(limit_str))
            return f"[API {remaining_str}/{limit_str}]"

    def update_from_headers(self, headers: dict) -> None:
        """Update quota tracking from API response headers (thread-safe)."""
        remaining = headers.get("X-RateLimit-Remaining")
        limit = headers.get("X-RateLimit-Limit")
        reset = headers.get("X-RateLimit-Reset")

        with self._lock:
            if remaining is not None:
                self._remaining = int(remaining)
            if limit is not None:
                self._limit = int(limit)
            if reset is not None:
                self._reset = int(reset)

    def get_current_quota(self) -> tuple[int, int, int]:
        """Get current quota (thread-safe). Returns (remaining, limit, reset)."""
        with self._lock:
            return self._remaining, self._limit, self._reset

    def initialize(self, token: Optional[str] = None) -> Dict[str, Any]:
        """
        Initialize quota by calling /rate_limit API.
        Returns dict with 'remaining', 'limit', 'reset' keys.
        """
        headers = {
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
        }
        if token:
            headers["Authorization"] = f"Bearer {token}"

        try:
            response = requests.get(f"{GITHUB_API_BASE}/rate_limit", headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()

            core = data.get("resources", {}).get("core", {})
            remaining = core.get("remaining", 0)
            limit = core.get("limit", 0)
            reset = core.get("reset", 0)

            # Update tracking (thread-safe)
            with self._lock:
                self._remaining = remaining
                self._limit = limit
                self._reset = reset

            return {
                "remaining": remaining,
                "limit": limit,
                "reset": reset,
            }
        except Exception as e:
            logging.debug("Could not check rate limit: %s", e)
            return {}

    def calculate_max_prs(self) -> int:
        """
        Calculate maximum number of PRs processable with current quota.
        Reserves % of total quota to avoid complete exhaustion.
        """
        remaining, limit, _ = self.get_current_quota()

        if limit == 0:
            return 100  # Not initialized yet, conservative default

        # Reserve 5% of total quota (don't exhaust completely)
        reserve = int(limit * API_QUOTA_RESERVE_PCT)
        effective_buffer = max(API_SAFETY_BUFFER, reserve)

        available_for_prs = max(0, remaining - effective_buffer)
        max_prs = available_for_prs // API_CALLS_PER_PR

        return max_prs

    def wait_for_reset(self) -> bool:
        """
        Wait until API quota resets.
        Returns True if successfully waited.
        """
        import time

        remaining, limit, reset_timestamp = self.get_current_quota()

        # Add buffer for safety
        reset_timestamp = reset_timestamp + 15

        if reset_timestamp == 0:
            logger.error("Cannot determine quota reset time (not initialized)")
            return False

        reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
        now = datetime.now(timezone.utc)
        wait_seconds = (reset_time - now).total_seconds()

        if wait_seconds <= 0:
            logger.info("Quota already reset, refreshing...")
            return True

        wait_minutes = wait_seconds / 60
        logger.info(f"Waiting {wait_minutes:.1f} minutes for quota reset...")
        logger.info(f"Will resume at {reset_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")

        time.sleep(wait_seconds)

        logger.info("Quota reset complete, refreshing quota like startup...")
        return True

    def check_sufficient(
        self, estimated_calls: int, repo_ctx: str, chunk_info: str = ""
    ) -> tuple[bool, int]:
        """
        Check if sufficient API rate limit available for estimated calls.
        Does NOT make an API call - uses current quota.

        Returns (sufficient, max_prs_possible).
        repo_ctx: Context string like "[owner/repo]" for logging
        chunk_info: Optional string like "Chunk 1: " for context
        """
        # Use current quota (no API call unless not initialized)
        remaining, limit, _ = self.get_current_quota()

        if limit == 0:
            # Quota not initialized yet - initialize it now (one-time API call)
            logger.warning("[%s] Quota not initialized, fetching current status", repo_ctx)
            rate_info = self.initialize(get_github_token())
            if not rate_info:
                # Failed to get quota - cannot proceed safely
                logger.error("[%s] Failed to fetch quota, cannot verify rate limit", repo_ctx)
                return False, 0
            remaining = rate_info["remaining"]
            limit = rate_info["limit"]

        # Calculate max PRs we can handle
        max_prs = self.calculate_max_prs()

        # Calculate effective buffer (5% of total or fixed buffer, whichever is larger)
        reserve = int(limit * API_QUOTA_RESERVE_PCT)
        effective_buffer = max(API_SAFETY_BUFFER, reserve)

        # Show estimated calls for this operation
        logger.info("[%s] %sEstimated API calls: ~%d", repo_ctx, chunk_info, estimated_calls)

        if remaining <= (estimated_calls + effective_buffer):
            logger.error(
                "[%s] %sInsufficient API rate limit: need ~%d calls + %d buffer "
                "(reserve=%d), only %d available",
                repo_ctx,
                chunk_info,
                estimated_calls,
                effective_buffer,
                reserve,
                remaining,
            )
            logger.warning(
                "[%s] %sCurrent quota allows processing ~%d PRs max", repo_ctx, chunk_info, max_prs
            )
            return False, max_prs

        return True, max_prs


class StateManager:
    """
    Manages state file for tracking repository processing progress.

    State file stores last update timestamp and CSV file path per repository.
    """

    def __init__(self, state_file_path: Path = None, logger=None):
        """Initialize state manager with path to state file and logger."""
        self._state_file = state_file_path or STATE_FILE
        self._logger = logger

    def load(self) -> Dict[str, Any]:
        """Load state file containing last update dates per repo."""
        if not self._state_file.exists():
            return {}

        try:
            with open(self._state_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
                return data
        except Exception as e:
            if self._logger:
                self._logger.warning("Failed to load state file %s: %s", self._state_file, e)
            return {}

    def save(self, state: Dict[str, Any]) -> None:
        """Save state file with last update dates per repo."""
        try:
            with open(self._state_file, "w", encoding="utf-8") as f:
                yaml.safe_dump(state, f, default_flow_style=False, sort_keys=True)

            if self._logger:
                self._logger.debug("Saved state file to %s", self._state_file)
        except Exception as e:
            if self._logger:
                self._logger.error("Failed to save state file %s: %s", self._state_file, e)
            raise

    def get_repo_remote_url(self, owner: str, repo: str) -> str:
        """
        Get repository remote URL key for state tracking.
        Uses format: https://github.com/owner/repo
        """
        return f"https://github.com/{owner}/{repo}"

    def get_repo_state(self, owner: str, repo: str) -> Optional[Dict[str, Any]]:
        """
        Get state for a repository (timestamp and csv_file).
        Returns dict with 'timestamp' and 'csv_file' keys, or None if not found.
        Raises ValueError if state is malformed.
        """
        state = self.load()
        repo_key = self.get_repo_remote_url(owner, repo)

        if repo_key not in state:
            return None

        entry = state[repo_key]

        if not isinstance(entry, dict):
            raise ValueError(
                f"Invalid state format for {repo_key} in state file. "
                f"Expected dict with 'timestamp' and 'csv_file', got {type(entry).__name__}"
            )

        timestamp_str = entry.get("timestamp", "")
        csv_file = entry.get("csv_file")

        if not timestamp_str:
            raise ValueError(
                f"Missing or empty 'timestamp' field for {repo_key} in state file. "
                f"State file location: {self._state_file}"
            )

        if not csv_file:
            raise ValueError(
                f"Missing or empty 'csv_file' field for {repo_key} in state file. "
                f"State file location: {self._state_file}"
            )

        try:
            timestamp = parse_timestamp(timestamp_str)
        except ValueError as e:
            raise ValueError(
                f"Invalid timestamp for {repo_key} in state file: {e}. "
                f"State file location: {self._state_file}"
            ) from e

        return {"timestamp": timestamp, "csv_file": csv_file}

    def update_repo(self, owner: str, repo: str, timestamp: datetime, csv_file: str) -> None:
        """Update state file with new last update date and csv file for a repository."""
        state = self.load()
        repo_key = self.get_repo_remote_url(owner, repo)
        # Store as UTC timestamp without timezone component
        utc_timestamp = timestamp.astimezone(timezone.utc).replace(tzinfo=None)
        state[repo_key] = {"timestamp": utc_timestamp.isoformat(), "csv_file": csv_file}
        self.save(state)

    def get_all_tracked_repos(self) -> List[Dict[str, Any]]:
        """
        Get all tracked repositories from state file.
        Returns list of dicts with 'url', 'owner', 'repo', 'timestamp', 'csv_file'.
        """
        state = self.load()
        repos = []

        for repo_url, entry in state.items():
            # Parse URL to get owner/repo
            # Format: https://github.com/owner/repo
            try:
                parts = repo_url.rstrip("/").split("/")
                if len(parts) >= 2:
                    owner = parts[-2]
                    repo = parts[-1]

                    if isinstance(entry, dict):
                        timestamp = parse_timestamp(entry.get("timestamp", ""))
                        csv_file = entry.get("csv_file")
                    else:
                        if self._logger:
                            self._logger.warning("Skipping %s: invalid state format", repo_url)
                        continue

                    repos.append(
                        {
                            "url": repo_url,
                            "owner": owner,
                            "repo": repo,
                            "timestamp": timestamp,
                            "csv_file": csv_file,
                        }
                    )
            except Exception as e:
                if self._logger:
                    self._logger.warning("Failed to parse state entry for %s: %s", repo_url, e)
                continue

        return repos


class GitHubClient:
    """
    GitHub API client for making authenticated requests.

    Handles all communication with GitHub API and automatically
    updates quota tracking from response headers.
    """

    def __init__(self, token: Optional[str] = None, quota_manager=None, logger=None):
        """Initialize GitHub client with optional token, quota manager, and logger."""
        self._token = token
        self._quota_manager = quota_manager
        self._logger = logger

    def make_request(self, url: str, params: Optional[Dict[str, Any]] = None) -> Any:
        """Make a GitHub API request with error handling."""
        headers = {
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
        }
        if self._token:
            headers["Authorization"] = f"Bearer {self._token}"

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()

            # Update quota tracking from response headers
            if self._quota_manager:
                self._quota_manager.update_from_headers(response.headers)

            return response.json()
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                # Check if it's actually a rate limit error
                rate_limit = e.response.headers.get("X-RateLimit-Remaining")
                if rate_limit == "0":
                    reset_time = e.response.headers.get("X-RateLimit-Reset", "unknown")
                    auth_status = "authenticated" if self._token else "unauthenticated"
                    raise GitHubAPIError(
                        f"GitHub API rate limit exceeded ({auth_status}). "
                        f"Rate limit resets at {reset_time}. "
                        f"Limit: {e.response.headers.get('X-RateLimit-Limit', 'unknown')}"
                    )
                else:
                    # 403 but not rate limit - likely permissions
                    raise GitHubAPIError(
                        "GitHub API forbidden: insufficient permissions or invalid token."
                    )
            elif e.response.status_code == 401:
                raise GitHubAPIError("GitHub API unauthorized: invalid or expired token.")
            elif e.response.status_code == 404:
                raise GitHubAPIError("Repository not found or insufficient permissions.")
            else:
                raise GitHubAPIError(f"GitHub API error: {e}")
        except requests.exceptions.RequestException as e:
            raise GitHubAPIError(f"Network error: {e}")

    def _parse_link_header(self, link_header: str) -> Optional[str]:
        """Parse Link header to extract next page URL."""
        if not link_header:
            return None

        # Link header format: <url>; rel="next", <url>; rel="last"
        for link in link_header.split(","):
            if 'rel="next"' in link:
                # Extract URL between < and >
                start = link.find("<")
                end = link.find(">")
                if start != -1 and end != -1:
                    return link[start + 1 : end]
        return None

    def _make_paginated_request(
        self, url: str, params: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Make paginated API requests, following Link headers."""
        all_items = []
        current_url = url
        current_params = params or {}
        current_params["per_page"] = 100  # Request maximum per page

        while current_url:
            headers = {
                "Accept": "application/vnd.github+json",
                "X-GitHub-Api-Version": "2022-11-28",
            }
            if self._token:
                headers["Authorization"] = f"Bearer {self._token}"

            try:
                response = requests.get(
                    current_url, headers=headers, params=current_params, timeout=30
                )
                response.raise_for_status()

                # Update quota tracking
                if self._quota_manager:
                    self._quota_manager.update_from_headers(response.headers)

                data = response.json()

                # Handle both list and single item responses
                if isinstance(data, list):
                    all_items.extend(data)
                else:
                    all_items.append(data)
                    break  # Single item, no pagination

                # Check for next page
                link_header = response.headers.get("Link", "")
                next_url = self._parse_link_header(link_header)

                if next_url:
                    current_url = next_url
                    current_params = None  # Next URL already includes params
                else:
                    break  # No more pages

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    rate_limit = e.response.headers.get("X-RateLimit-Remaining")
                    if rate_limit == "0":
                        reset_time = e.response.headers.get("X-RateLimit-Reset", "unknown")
                        auth_status = "authenticated" if self._token else "unauthenticated"
                        raise GitHubAPIError(
                            f"GitHub API rate limit exceeded ({auth_status}). "
                            f"Rate limit resets at {reset_time}. "
                            f"Limit: {e.response.headers.get('X-RateLimit-Limit', 'unknown')}"
                        )
                    else:
                        raise GitHubAPIError(
                            "GitHub API forbidden: insufficient permissions or invalid token."
                        )
                elif e.response.status_code == 401:
                    raise GitHubAPIError("GitHub API unauthorized: invalid or expired token.")
                elif e.response.status_code == 404:
                    raise GitHubAPIError("Repository not found or insufficient permissions.")
                else:
                    raise GitHubAPIError(f"GitHub API error: {e}")
            except requests.exceptions.RequestException as e:
                raise GitHubAPIError(f"Network error: {e}")

        return all_items

    def fetch_all_prs(
        self,
        owner: str,
        repo: str,
        start_date: datetime,
        end_date: datetime,
    ) -> List[Dict[str, Any]]:
        """
        Fetch all pull requests in date range, sorted by updated_at ascending.

        Internally fetches descending (newest first) and stops at start_date
        to avoid wasting API calls on old PRs. Returns PRs in ascending order.
        """
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls"
        all_prs = []
        page = 1
        per_page = 100

        while True:
            params = {
                "state": "all",
                "sort": "updated",
                "direction": "desc",  # Newest first
                "page": page,
                "per_page": per_page,
            }

            prs = self.make_request(url, params)

            if not prs:
                break

            # Track if we got a full page (to know if more exist)
            got_full_page = len(prs) == per_page

            for pr in prs:
                updated_at = date_parser.parse(pr["updated_at"])

                # Stop if we've gone past the start date
                if updated_at < start_date:
                    # Found the first PR before start_date, stop fetching more
                    # by forcing got_full_page to false
                    got_full_page = False
                    break

                # Skip PRs after end date
                if updated_at > end_date:
                    continue

                # Include PRs within date range
                all_prs.append(pr)

            # If we didn't get a full page, we're done
            if not got_full_page:
                break

            page += 1

        # Reverse entire collection to return oldest-first
        all_prs.reverse()
        return all_prs

    def list_repos(self, owner: str) -> List[str]:
        """
        List all repositories for an owner (user or organization).
        Returns list of repository names.
        """
        repos = []
        page = 1
        per_page = 100

        # Try as organization first
        url_base = f"{GITHUB_API_BASE}/orgs/{owner}/repos"
        is_org = True

        while True:
            params = {"page": page, "per_page": per_page, "type": "all"}

            try:
                response = self.make_request(url_base, params)
            except GitHubAPIError as e:
                # If org request fails with 404, try as user
                if "not found" in str(e).lower() and is_org:
                    if self._logger:
                        self._logger.debug("Not an organization, trying as user: %s", owner)
                    url_base = f"{GITHUB_API_BASE}/users/{owner}/repos"
                    is_org = False
                    page = 1  # Reset page counter
                    continue
                else:
                    raise

            if not response:
                break

            for repo in response:
                repos.append(repo["name"])

            # Break if we got fewer results than requested
            if len(response) < per_page:
                break

            page += 1

        return repos

    def validate_repo_access(self, owner: str, repo: str) -> bool:
        """
        Validate that we have access to a repository.
        Returns True if accessible, False otherwise.
        """
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}"

        try:
            self.make_request(url)
            return True
        except GitHubAPIError as e:
            if self._logger:
                self._logger.debug("Cannot access %s/%s: %s", owner, repo, e)
            return False

    def fetch_timeline_events(self, owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:
        """Fetch all timeline events for a PR with pagination."""
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/issues/{pr_number}/events"
        return self._make_paginated_request(url)

    def fetch_issue_comments(self, comments_url: str) -> List[Dict[str, Any]]:
        """Fetch all issue comments with pagination."""
        return self._make_paginated_request(comments_url)

    def fetch_review_comments(self, review_comments_url: str) -> List[Dict[str, Any]]:
        """Fetch all review comments with pagination."""
        return self._make_paginated_request(review_comments_url)

    def fetch_reviews(self, owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:
        """Fetch all reviews for a PR with pagination."""
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls/{pr_number}/reviews"
        return self._make_paginated_request(url)

    def fetch_single_pr(self, owner: str, repo: str, pr_number: int) -> Dict[str, Any]:
        """Fetch a single pull request by number."""
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls/{pr_number}"
        return self.make_request(url)


class CSVManager:
    """
    Manages CSV I/O operations for PR metrics.

    Handles reading existing CSV files, writing metrics data,
    and managing field definitions.
    """

    def __init__(self, logger=None):
        """Initialize CSV manager with logger."""
        self._logger = logger

    def get_fieldnames(self) -> List[str]:
        """Get CSV field names in order."""
        return [
            PR_NUMBER_FIELD,
            "title",
            "author",
            "created_at",
            "ready_for_review_at",
            "merged_at",
            "closed_at",
            "days_open",
            "days_in_review",
            "total_comment_count",
            "non_ai_bot_comment_count",
            "ai_bot_comment_count",
            "non_ai_bot_login_names",
            "ai_bot_login_names",
            "changes_requested_count",
            "unique_change_requesters",
            "approval_count",
            "status",
            "url",
            "errors",
            "lines_added",
            "lines_deleted",
            "files_changed",
            "total_line_changes",
        ]

    def read_csv(self, csv_file: str) -> Dict[int, Dict[str, Any]]:
        """
        Read existing CSV file and return dict keyed by PR number.
        Returns empty dict if file doesn't exist or can't be read.
        """
        if not os.path.exists(csv_file):
            return {}

        try:
            existing_data = {}
            with open(csv_file, "r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # Key by PR number for O(1) lookups
                    pr_number = int(row[PR_NUMBER_FIELD])
                    existing_data[pr_number] = row

            if self._logger:
                self._logger.info("Loaded %d existing PRs from %s", len(existing_data), csv_file)
            return existing_data
        except Exception as e:
            if self._logger:
                self._logger.warning("Failed to read existing CSV %s: %s", csv_file, e)
            return {}

    def write_csv(
        self,
        metrics: List[Dict[str, Any]],
        output_file: Optional[str],
        merge_mode: bool = False,
        force_write: bool = False,
    ) -> None:
        """
        Write metrics to CSV file or stdout.

        If merge_mode is True and output_file exists, existing PR data is loaded,
        updated with new metrics, and written back.

        If force_write is True, write headers even if metrics list is empty.
        """
        if not metrics and not force_write:
            if self._logger:
                self._logger.warning("No pull requests to output")
            return

        fieldnames = self.get_fieldnames()

        # Merge with existing data if requested
        if merge_mode and output_file:
            existing_data = self.read_csv(output_file)

            # Update existing data with new metrics
            for metric in metrics:
                pr_number = metric[PR_NUMBER_FIELD]
                existing_data[pr_number] = metric

            # Convert back to list
            all_metrics = list(existing_data.values())
            if self._logger:
                self._logger.info(
                    "Merged data: %d total PRs (%d new/updated)", len(all_metrics), len(metrics)
                )
        else:
            all_metrics = metrics

        if output_file:
            # Write to temp file first, then atomically rename
            output_path = Path(output_file)
            temp_fd, temp_path = tempfile.mkstemp(
                dir=output_path.parent if output_path.parent.exists() else tempfile.gettempdir(),
                prefix=".tmp_pr_metrics_",
                suffix=".csv",
            )

            try:
                with os.fdopen(temp_fd, "w", newline="", encoding="utf-8") as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_metrics)

                # Atomic rename
                os.rename(temp_path, output_file)
                if self._logger:
                    self._logger.info("Writing output to %s", output_file)
            except Exception as e:
                # Keep temp file for debugging
                if self._logger:
                    self._logger.error(
                        "Failed to write CSV output. Temp file retained at: %s", temp_path
                    )
                raise e
        else:
            writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_metrics)


class LoggingManager:
    """
    Manages application logging with quota-aware prefixes.

    Wraps standard logging to inject API quota status into all log messages.
    """

    def __init__(self, quota_manager: QuotaManager):
        """Initialize logging manager with reference to quota manager."""
        self._quota_manager = quota_manager

    def info(self, msg: str, *args, **kwargs) -> None:
        """Log INFO with API quota prefix."""
        logging.info(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def warning(self, msg: str, *args, **kwargs) -> None:
        """Log WARNING with API quota prefix."""
        logging.warning(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def error(self, msg: str, *args, **kwargs) -> None:
        """Log ERROR with API quota prefix."""
        logging.error(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def debug(self, msg: str, *args, **kwargs) -> None:
        """Log DEBUG with API quota prefix."""
        logging.debug(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)


# ============================================================================
# Global Manager Instances
# ============================================================================

quota_manager = QuotaManager()
logger = LoggingManager(quota_manager)
state_manager = StateManager(logger=logger)
github_client = None  # Initialized in main() with token, quota_manager, and logger
csv_manager = CSVManager(logger=logger)


# ============================================================================
# PR Processing Functions
# ============================================================================


def get_ready_for_review_time(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> str:
    """Get the timestamp when PR was last marked ready for review."""
    # If never a draft, use created_at
    if not pr.get("draft", False) and pr.get("created_at"):
        # Check events to see if it was ever a draft
        try:
            events = github_client.fetch_timeline_events(owner, repo, pr["number"])
            ready_events = [e for e in events if e.get("event") == "ready_for_review"]
            if ready_events:
                # Use the latest ready_for_review event
                return ready_events[-1]["created_at"]
        except GitHubAPIError as e:
            logger.debug("Failed to fetch events for PR #%d: %s", pr["number"], e)

    return pr["created_at"]


def count_comments(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], ai_bot_pattern: str = None
) -> tuple[int, int, int, str, str]:
    """
    Count total comments, non-AI bot comments, and AI bot comments.
    Returns (total_comments, non_ai_bot_comments, ai_bot_comments,
             non_ai_bot_login_names, ai_bot_login_names).
    Non-AI bot and AI bot login names are comma-separated strings.
    ai_bot_pattern: regex pattern to identify AI bots
    """
    import re

    total_comments = 0
    non_ai_bot_comments = 0
    ai_bot_comments = 0
    non_ai_bot_logins = set()
    ai_bot_logins = set()

    # Issue comments (general PR comments)
    comments_url = pr["comments_url"]
    try:
        comments = github_client.fetch_issue_comments(comments_url)
        for comment in comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                if ai_bot_pattern and re.match(ai_bot_pattern, login):
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch comments for PR #%d: %s", pr["number"], e)

    # Review comments (inline code comments)
    review_comments_url = pr["review_comments_url"]
    try:
        review_comments = github_client.fetch_review_comments(review_comments_url)
        for comment in review_comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                if ai_bot_pattern and re.match(ai_bot_pattern, login):
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch review comments for PR #%d: %s", pr["number"], e)

    # Convert to comma-separated strings, sorted for consistency
    non_ai_bot_login_names = ",".join(sorted(non_ai_bot_logins))
    ai_bot_login_names = ",".join(sorted(ai_bot_logins))

    return (
        total_comments,
        non_ai_bot_comments,
        ai_bot_comments,
        non_ai_bot_login_names,
        ai_bot_login_names,
    )


def get_review_metrics(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> tuple[int, int, int]:
    """
    Get review metrics: changes requested count, unique change requesters, approvals.
    Returns (changes_requested_count, unique_change_requesters, approval_count).
    """
    try:
        reviews = github_client.fetch_reviews(owner, repo, pr["number"])
    except GitHubAPIError as e:
        logger.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
        return 0, 0, 0

    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r["user"]["login"]
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review["user"]["login"]
            # Later reviews override earlier ones
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def determine_pr_status(pr: Dict[str, Any]) -> str:
    """Determine the current status of the PR."""
    if pr.get("draft"):
        return "draft"
    elif pr.get("merged_at"):
        return "merged"
    elif pr.get("state") == "closed":
        return "closed"
    else:
        return "open"


def process_pr(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], ai_bot_pattern: str = None
) -> Dict[str, Any]:
    """Process a single PR and extract all metrics."""
    pr_number = pr["number"]
    logger.debug("Processing PR #%d: %s", pr_number, pr["title"])

    errors = []
    metrics: Dict[str, Any] = {
        PR_NUMBER_FIELD: pr_number,
        "title": pr["title"],
        "author": pr["user"]["login"] if pr.get("user") else "",
        "created_at": pr["created_at"],
        "url": pr["html_url"],
    }

    # Ready for review time
    try:
        metrics["ready_for_review_at"] = get_ready_for_review_time(pr, owner, repo, token)
    except Exception as e:
        logger.debug("Error getting ready time for PR #%d: %s", pr_number, e)
        metrics["ready_for_review_at"] = pr["created_at"]
        errors.append(f"ready_time: {e}")

    # Comment counts
    try:
        total_comments, non_ai_bot_comments, ai_bot_comments, non_ai_bot_names, ai_bot_names = (
            count_comments(pr, owner, repo, token, ai_bot_pattern)
        )
        metrics["total_comment_count"] = total_comments
        metrics["non_ai_bot_comment_count"] = non_ai_bot_comments
        metrics["ai_bot_comment_count"] = ai_bot_comments
        metrics["non_ai_bot_login_names"] = non_ai_bot_names
        metrics["ai_bot_login_names"] = ai_bot_names
    except Exception as e:
        logger.debug("Error counting comments for PR #%d: %s", pr_number, e)
        metrics["total_comment_count"] = 0
        metrics["non_ai_bot_comment_count"] = 0
        metrics["ai_bot_comment_count"] = 0
        metrics["non_ai_bot_login_names"] = ""
        metrics["ai_bot_login_names"] = ""
        errors.append(f"comments: {e}")

    # Review metrics
    try:
        changes_requested, unique_requesters, approvals = get_review_metrics(pr, owner, repo, token)
        metrics["changes_requested_count"] = changes_requested
        metrics["unique_change_requesters"] = unique_requesters
        metrics["approval_count"] = approvals
    except Exception as e:
        logger.debug("Error getting reviews for PR #%d: %s", pr_number, e)
        metrics["changes_requested_count"] = 0
        metrics["unique_change_requesters"] = 0
        metrics["approval_count"] = 0
        errors.append(f"reviews: {e}")

    # Status
    metrics["status"] = determine_pr_status(pr)

    # Merged and closed timestamps
    metrics["merged_at"] = pr.get("merged_at") or ""
    metrics["closed_at"] = pr.get("closed_at") or ""

    # Calculate derived time metrics
    try:
        created_at = date_parser.parse(pr["created_at"])
        ready_at = date_parser.parse(metrics["ready_for_review_at"])

        # Determine end time (merged, closed, or now)
        if metrics["merged_at"]:
            end_time = date_parser.parse(metrics["merged_at"])
        elif metrics["closed_at"]:
            end_time = date_parser.parse(metrics["closed_at"])
        else:
            end_time = datetime.now(timezone.utc)

        # days_open: created → end
        days_open = (end_time - created_at).total_seconds() / 86400
        metrics["days_open"] = round(days_open, 2)

        # days_in_review: ready_for_review → end
        days_in_review = (end_time - ready_at).total_seconds() / 86400
        metrics["days_in_review"] = round(days_in_review, 2)

    except Exception as e:
        logger.debug("Error calculating time metrics for PR #%d: %s", pr_number, e)
        metrics["days_open"] = ""
        metrics["days_in_review"] = ""
        errors.append(f"time_metrics: {e}")

    # Complexity metrics
    metrics["lines_added"] = pr.get("additions", 0)
    metrics["lines_deleted"] = pr.get("deletions", 0)
    metrics["files_changed"] = pr.get("changed_files", 0)
    metrics["total_line_changes"] = metrics["lines_added"] + metrics["lines_deleted"]

    # Errors
    metrics["errors"] = "; ".join(errors) if errors else ""

    return metrics


# ============================================================================
# Main Business Logic
# ============================================================================


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate CSV reports of GitHub pull request metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze PRs from current repository (last year)
  gh-pr-metrics

  # Specify repository explicitly
  gh-pr-metrics --owner microsoft --repo vscode

  # Custom time range
  gh-pr-metrics --start 2024-01-01 --end 2024-12-31

  # Output to file (stores path in state for future updates)
  gh-pr-metrics --output metrics.csv

  # Initialize repositories (validates access, creates state entries)
  gh-pr-metrics --init --owner ansible --output "data/{owner}-{repo}.csv" --start "2024-11-01"

  # Update specific repo (uses stored CSV path from state)
  gh-pr-metrics --owner microsoft --repo vscode --update

  # Update all tracked repositories
  gh-pr-metrics --update-all

Environment Variables:
  GITHUB_TOKEN    GitHub personal access token for authentication
        """,
    )

    parser.add_argument("--owner", help="Repository owner (default: auto-detect from git)")
    parser.add_argument("--repo", help="Repository name (default: auto-detect from git)")
    parser.add_argument(
        "--pr",
        type=int,
        help="Update a specific PR number only (surgically update existing CSV)",
    )
    parser.add_argument(
        "--start",
        help="Start timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: 365 days ago",
    )
    parser.add_argument(
        "--end",
        help="End timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: now",
    )
    parser.add_argument("--output", "-o", help="Output file path (default: stdout)")
    parser.add_argument(
        "--init",
        action="store_true",
        help="Initialize repositories in state file. Validates access and stores --start date. "
        "Requires --owner and --output. If only --owner provided, initializes all repos. "
        "Output supports patterns like 'data/{owner}-{repo}.csv'",
    )
    parser.add_argument(
        "--update",
        action="store_true",
        help="Update mode: fetch only PRs since last update and merge with existing CSV. "
        "Requires --owner and --repo. Uses CSV path stored in state file. "
        "Cannot be used with --output (uses stored path).",
    )
    parser.add_argument(
        "--update-all",
        action="store_true",
        help="Update all tracked repositories. Cannot be used with --owner/--repo or --output.",
    )
    parser.add_argument(
        "--wait",
        action="store_true",
        help="Wait for rate limit to reset if quota exhausted, then continue processing.",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument(
        "--workers",
        type=int,
        default=DEFAULT_WORKERS,
        help=f"Number of parallel workers for processing PRs (default: {DEFAULT_WORKERS})",
    )
    parser.add_argument(
        "--ai-bot-regex",
        default="cursor\\[bot\\]|claude\\[bot\\]|Copilot",
        help="Regex pattern to identify AI bots (default: cursor[bot]|claude[bot]|Copilot)",
    )
    parser.add_argument(
        "--log-file",
        default="gh-pr-metrics.log",
        help="Log file path (default: gh-pr-metrics.log in current directory)",
    )
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    return parser.parse_args()


def show_update_all_progress_summary(
    tracked_repos: List[Dict[str, Any]], completed_repos: int, stopped_at: Optional[str] = None
) -> None:
    """
    Show progress summary for update-all operation.

    Args:
        tracked_repos: List of all tracked repositories
        completed_repos: Number of repos completed in this pass
        stopped_at: Optional repo name where processing stopped (if partial)
    """
    logger.info("=" * 80)
    logger.info("Progress Summary:")

    if stopped_at:
        logger.info("  Completed this pass: %d repos", completed_repos)
        logger.info("  Stopped at (partial): %s", stopped_at)
    else:
        logger.info("  Completed: %d repos (not started yet)", completed_repos)

    # Analyze state file to show what's current
    now = datetime.now(timezone.utc)
    cutoff = now - timedelta(hours=12)

    recently_updated = 0
    needs_processing = 0

    for repo_info in tracked_repos:
        if repo_info["timestamp"] > cutoff:
            recently_updated += 1
        else:
            needs_processing += 1

    logger.info("  Recently updated (< 12 hours): %d repos", recently_updated)
    logger.info("  Needs processing (> 12 hours): %d repos", needs_processing)


def update_single_pr(
    owner: str,
    repo: str,
    pr_number: int,
    output_file: str,
    token: Optional[str],
    ai_bot_regex: str,
) -> int:
    """
    Update a single PR in an existing CSV file.

    Returns 0 on success, 1 on error.
    """
    logger.info("Fetching PR #%d from %s/%s", pr_number, owner, repo)

    try:
        pr = github_client.fetch_single_pr(owner, repo, pr_number)
    except GitHubAPIError as e:
        logger.error("Failed to fetch PR #%d: %s", pr_number, e)
        return 1

    logger.info("Processing PR #%d", pr_number)
    try:
        pr_metrics = process_pr(pr, owner, repo, token, ai_bot_regex)
    except Exception as e:
        logger.error("Failed to process PR #%d: %s", pr_number, e)
        return 1

    # Read existing CSV
    if not Path(output_file).exists():
        logger.error("CSV file not found: %s", output_file)
        logger.error("Use regular mode first to create the CSV file")
        return 1

    # Read existing CSV as dict keyed by PR number
    logger.info("Reading existing CSV: %s", output_file)
    existing_data = csv_manager.read_csv(output_file)

    # Update the PR in the dictionary
    if pr_number in existing_data:
        logger.info("Updating existing PR #%d in CSV", pr_number)
    else:
        logger.info("Adding new PR #%d to CSV", pr_number)

    existing_data[pr_number] = pr_metrics

    # Convert dict to list and write back to CSV
    all_metrics = list(existing_data.values())
    logger.info("Writing updated CSV: %s", output_file)
    csv_manager.write_csv(all_metrics, output_file, merge_mode=False)

    logger.info("Successfully updated PR #%d", pr_number)
    return 0


def process_repository(
    owner: str,
    repo: str,
    output_file: str,
    start_date: datetime,
    end_date: datetime,
    token: Optional[str],
    workers: int,
    ai_bot_regex: str,
    merge_mode: bool = False,
) -> tuple[int, int, int]:
    """
    Process a single repository and generate/update metrics CSV using page-based processing.

    Fetches PRs one page at a time (sorted by updated date), processes each page,
    and checks rate limit before fetching the next page. Saves progress after each page.

    Returns (exit_code, chunks_completed, total_chunks_fetched).
    exit_code: 0 on success, 1 on error/stopped
    chunks_completed: Number of chunks successfully processed
    total_chunks_fetched: Total chunks fetched (may be more than completed if stopped mid-chunk)
    """
    repo_ctx = f"{owner}/{repo}"

    logger.info("[%s] Starting processing", repo_ctx)
    logger.info("[%s] Date range: %s to %s", repo_ctx, start_date.isoformat(), end_date.isoformat())

    # Check quota before starting any work
    max_prs_can_process = quota_manager.calculate_max_prs()
    if max_prs_can_process == 0:
        remaining, limit, _ = quota_manager.get_current_quota()
        logger.error(
            "[%s] Cannot process any PRs with current quota (exhausted or below reserve)",
            repo_ctx,
        )
        logger.error(
            "[%s] Current quota: %d/%d remaining",
            repo_ctx,
            remaining,
            limit,
        )
        # Note: per-repo wait not implemented, only at update-all level
        return 1, 0, 0

    logger.info(
        "[%s] Quota check: can process up to ~%d PRs before hitting reserve",
        repo_ctx,
        max_prs_can_process,
    )

    logger.info("[%s] Fetching PRs sorted by updated date (oldest first)", repo_ctx)

    # Step 1: Fetch all PRs in date range (internally descending, returns ascending)
    total_chunks_fetched = 0  # Will be calculated from PR count
    chunks_completed = 0  # Initialize early for error handlers

    try:
        all_prs = github_client.fetch_all_prs(owner, repo, start_date, end_date)

        if not all_prs:
            logger.info("[%s] No pull requests found in the specified date range", repo_ctx)
            state_manager.update_repo(owner, repo, end_date, output_file)
            return 0, 1, 0

        logger.info("[%s] Collected %d PRs total", repo_ctx, len(all_prs))
        # Estimate chunks fetched (for return value compatibility)
        total_chunks_fetched = (len(all_prs) + 99) // 100  # Round up

        # Step 2: Process PRs in chunks with incremental saves
        # PRs are already in ascending order (oldest-first) from fetch_all_prs
        chunk_size = 100
        chunk_num = 0
        last_processed_timestamp = start_date

        for chunk_start in range(0, len(all_prs), chunk_size):
            chunk_num += 1
            chunk_end = min(chunk_start + chunk_size, len(all_prs))
            prs_chunk = all_prs[chunk_start:chunk_end]
            total_prs = len(prs_chunk)

            chunk_info_str = f"Chunk {chunk_num}/{total_chunks_fetched}: "

            logger.info("[%s] %sProcessing %d PRs", repo_ctx, chunk_info_str, total_prs)

            # Check rate limit and truncate chunk if needed
            estimated_calls = estimate_api_calls_for_prs(total_prs)
            sufficient, max_prs = quota_manager.check_sufficient(
                estimated_calls, repo_ctx, chunk_info_str
            )

            if not sufficient:
                # Can't process full chunk, but maybe we can process some PRs
                if max_prs > 0:
                    # Truncate to what we can handle
                    logger.warning(
                        "[%s] Chunk %d: Truncating from %d to %d PRs due to quota limit",
                        repo_ctx,
                        chunk_num,
                        total_prs,
                        max_prs,
                    )
                    prs_chunk = prs_chunk[:max_prs]
                    total_prs = len(prs_chunk)
                else:
                    # Can't process any PRs
                    logger.error(
                        "[%s] Stopping at chunk %d due to insufficient rate limit",
                        repo_ctx,
                        chunk_num,
                    )
                    logger.error(
                        "[%s] Progress saved. Resume with --update (or --update-all) "
                        "after rate limit resets.",
                        repo_ctx,
                    )
                    return 1, chunks_completed, total_chunks_fetched

            # Process this chunk of PRs
            metrics = []
            completed = 0

            logger.info(
                "[%s] %sProcessing %d PRs with %d workers",
                repo_ctx,
                chunk_info_str,
                total_prs,
                workers,
            )

            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(process_pr, pr, owner, repo, token, ai_bot_regex): pr
                    for pr in prs_chunk
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    pr_number = pr["number"]
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)
                        logger.info(
                            "[%s] %s: Completed PR %d/%d: #%d (updated: %s)",
                            repo_ctx,
                            chunk_info_str,
                            completed,
                            total_prs,
                            pr_number,
                            pr.get("updated_at", "unknown"),
                        )
                    except Exception as e:
                        logger.error(
                            "[%s] %s: Failed to process PR #%d: %s",
                            repo_ctx,
                            chunk_info_str,
                            pr_number,
                            e,
                        )

            # Write output for this chunk (always merge after first chunk)
            chunk_merge_mode = merge_mode or (chunk_num > 1)
            csv_manager.write_csv(metrics, output_file, merge_mode=chunk_merge_mode)

            # Find the timestamp to set in the state file
            if prs_chunk:
                # Default to the updated_at of the last PR in this chunk
                last_processed_timestamp = date_parser.parse(prs_chunk[-1]["updated_at"])

            # Check if this is the last chunk and we had sufficient quota
            is_last_chunk = chunk_end >= len(all_prs)
            if is_last_chunk and sufficient:
                # Completed all PRs with sufficient quota - set to query time
                last_processed_timestamp = end_date

            # Update state file
            state_manager.update_repo(owner, repo, last_processed_timestamp, output_file)
            chunks_completed += 1
            logger.info(
                "[%s] %s: Processed %d PRs, updated state to %s",
                repo_ctx,
                chunk_info_str,
                len(metrics),
                last_processed_timestamp.isoformat(),
            )

            # If quota insufficient and more chunks remain, stop with error
            if not sufficient and not is_last_chunk:
                # Stopped mid-processing due to quota - return error code
                logger.error(
                    "[%s] Stopped at chunk %d/%d due to quota exhaustion",
                    repo_ctx,
                    chunks_completed,
                    total_chunks_fetched,
                )
                return 1, chunks_completed, total_chunks_fetched

    except GitHubAPIError as e:
        logger.error("[%s] GitHub API error: %s", repo_ctx, e)
        return 1, chunks_completed, total_chunks_fetched
    except Exception as e:
        logger.error("[%s] Unexpected error: %s", repo_ctx, e, exc_info=True)
        return 1, chunks_completed, total_chunks_fetched

    # Successfully completed all chunks
    logger.info("[%s] Successfully completed all %d chunks", repo_ctx, chunks_completed)
    return 0, chunks_completed, total_chunks_fetched


def main() -> int:
    """Main entry point."""
    args = parse_arguments()
    setup_logging(args.debug, args.log_file)

    logger.info("GitHub PR Metrics Tool v%s", __version__)
    logger.info("Logging to: %s", args.log_file)
    logger.debug("Arguments: %s", args)

    # Validate argument combinations
    if args.init and not args.owner:
        logger.error("--init requires --owner")
        return 1

    if args.init and not args.output:
        logger.error("--init requires --output (supports patterns like 'data/{owner}-{repo}.csv')")
        return 1

    if args.init and args.end:
        logger.error("--init cannot be used with --end (uses current time)")
        return 1

    if args.init and (args.update or args.update_all):
        logger.error("--init cannot be used with --update or --update-all")
        return 1

    if args.update and args.output:
        logger.error("--update and --output cannot be used together (update uses stored CSV path)")
        return 1

    if args.update and (args.start or args.end):
        logger.error(
            "--update cannot be used with --start or --end (uses last update date from state)"
        )
        return 1

    if args.update_all and args.output:
        logger.error("--update-all and --output cannot be used together (uses stored CSV paths)")
        return 1

    if args.update_all and (args.start or args.end):
        logger.error(
            "--update-all cannot be used with --start or --end (uses last update dates from state)"
        )
        return 1

    if args.update_all and (args.owner or args.repo):
        logger.error("--update-all cannot be used with --owner or --repo")
        return 1

    if args.update and args.update_all:
        logger.error("--update and --update-all cannot be used together")
        return 1

    if args.wait and not args.update_all:
        logger.error("--wait can only be used with --update-all")
        return 1

    if args.pr and (args.init or args.update or args.update_all):
        logger.error("--pr cannot be used with --init, --update, or --update-all")
        return 1

    if args.pr and not args.output:
        logger.error("--pr requires --output to specify the CSV file to update")
        return 1

    # Get GitHub token and initialize client
    token = get_github_token()
    global github_client
    github_client = GitHubClient(token, quota_manager, logger)

    if not token:
        logger.warning(
            "No GITHUB_TOKEN found. API rate limits will be restrictive "
            "for unauthenticated requests."
        )
        # Force single worker to avoid hitting rate limits too quickly
        workers = 1 if args.workers > 1 else args.workers
        if args.workers > 1:
            logger.warning("Forcing --workers=1 due to missing GITHUB_TOKEN")
    else:
        workers = args.workers

    # Check and display rate limit status
    rate_info = quota_manager.initialize(token)
    if rate_info:
        remaining = rate_info.get("remaining", "unknown")
        limit = rate_info.get("limit", "unknown")
        reset_timestamp = rate_info.get("reset", 0)

        if reset_timestamp:
            reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
            now = datetime.now(timezone.utc)
            time_until_reset = reset_time - now
            minutes_until_reset = time_until_reset.total_seconds() / 60
            reset_str = f"{minutes_until_reset:.1f} minutes"
        else:
            reset_str = "unknown"

        logger.info(
            "Rate limit: %s/%s remaining (resets in %s)",
            remaining,
            limit,
            reset_str,
        )

    # Handle init mode
    if args.init:
        owner = args.owner
        repos_to_init = []

        # If repo specified, just use that one
        if args.repo:
            repos_to_init = [args.repo]
            logger.info("Initializing single repository: %s/%s", owner, args.repo)
        else:
            # List all repos for owner
            logger.info("Listing all repositories for owner: %s", owner)
            try:
                repos_to_init = github_client.list_repos(owner)
                logger.info("Found %d repositories for %s", len(repos_to_init), owner)
            except GitHubAPIError as e:
                logger.error("Failed to list repositories for %s: %s", owner, e)
                return 1

        if not repos_to_init:
            logger.error("No repositories found for owner: %s", owner)
            return 1

        # Determine start date
        if args.start:
            start_date = parse_timestamp(args.start)
        else:
            start_date = datetime.now(timezone.utc) - timedelta(days=DEFAULT_DAYS_BACK)

        logger.info("Using start date: %s", start_date.isoformat())

        # Filter out already-tracked repos to get accurate count
        repos_needing_validation = []
        already_tracked_count = 0
        for repo in repos_to_init:
            if state_manager.get_repo_state(owner, repo):
                already_tracked_count += 1
            else:
                repos_needing_validation.append(repo)

        if already_tracked_count > 0:
            logger.info(
                "%d repositories already tracked, %d need validation",
                already_tracked_count,
                len(repos_needing_validation),
            )

        # Check rate limit before validating repos (1 API call per repo)
        if repos_needing_validation:
            estimated_calls = len(repos_needing_validation)
            sufficient, _ = quota_manager.check_sufficient(estimated_calls, f"{owner}/*")
            if not sufficient:
                logger.error(
                    "Insufficient API rate limit to validate %d repositories. "
                    "Each repo requires 1 API call for access validation.",
                    len(repos_needing_validation),
                )
                return 1

        # Validate access and initialize each repo
        successful = 0
        failed = 0
        skipped = 0

        for repo in repos_to_init:
            # Check if already tracked
            existing_state = state_manager.get_repo_state(owner, repo)
            if existing_state:
                logger.info(
                    "[%s/%s] Already tracked (last update: %s), skipping",
                    owner,
                    repo,
                    existing_state["timestamp"].isoformat(),
                )
                skipped += 1
                continue

            # Validate access
            logger.info("[%s/%s] Validating access...", owner, repo)
            if not github_client.validate_repo_access(owner, repo):
                logger.warning("[%s/%s] Cannot access repository, skipping", owner, repo)
                failed += 1
                continue

            # Expand output pattern
            csv_file = expand_output_pattern(args.output, owner, repo)

            # Create parent directory if needed
            csv_path = Path(csv_file)
            if csv_path.parent and not csv_path.parent.exists():
                logger.info("Creating directory: %s", csv_path.parent)
                csv_path.parent.mkdir(parents=True, exist_ok=True)

            # Update state file (CSV will be created on first update with actual data)
            state_manager.update_repo(owner, repo, start_date, csv_file)
            logger.info(
                "[%s/%s] Initialized (start date: %s)",
                owner,
                repo,
                start_date.isoformat(),
            )
            successful += 1

        # Summary
        logger.info("=" * 80)
        logger.info("Initialization complete:")
        logger.info("  Successful: %d", successful)
        logger.info("  Skipped (already tracked): %d", skipped)
        logger.info("  Failed (no access): %d", failed)

        if failed > 0:
            return 1
        return 0

    # Handle update-all mode
    if args.update_all:
        # Process all repos - reload state and restart on any failure
        while True:
            # CRITICAL: Reload state file on each iteration to get updated timestamps
            try:
                tracked_repos = state_manager.get_all_tracked_repos()
            except Exception as e:
                logger.error("Failed to load state file: %s", e)
                return 1

            if not tracked_repos:
                logger.error(
                    "No tracked repositories found in state file. "
                    "Run without --update-all first to track repositories."
                )
                return 1

            # Sort repositories by timestamp (oldest first)
            tracked_repos.sort(key=lambda r: r["timestamp"])

            logger.info("Updating %d tracked repositories", len(tracked_repos))

            # Check quota before starting update-all
            max_prs = quota_manager.calculate_max_prs()
            if max_prs == 0:
                remaining, limit, _ = quota_manager.get_current_quota()
                logger.error(
                    "Cannot start update-all: quota exhausted or below reserve (%d/%d remaining)",
                    remaining,
                    limit,
                )
                if args.wait:
                    # Show progress summary before waiting
                    show_update_all_progress_summary(tracked_repos, 0)
                    logger.info("Will wait for quota reset, then start processing")

                    if not quota_manager.wait_for_reset():
                        return 1
                    # After waiting, refresh quota and reload state
                    quota_manager.initialize(token)
                    max_prs = quota_manager.calculate_max_prs()
                    if max_prs == 0:
                        logger.error("Quota still exhausted after reset, aborting")
                        return 1
                    logger.info("Resuming update-all with refreshed quota")
                    continue  # Restart loop to reload state
                else:
                    logger.error("Use --wait to automatically wait for reset, or try again later")
                    return 1

            logger.info("Quota check: can process up to ~%d PRs total across all repos", max_prs)

            completed_repos = 0
            failed_repo = None

            for repo_info in tracked_repos:
                owner = repo_info["owner"]
                repo = repo_info["repo"]
                csv_file = repo_info["csv_file"]
                last_update = repo_info["timestamp"]

                if not csv_file:
                    logger.warning("Skipping %s/%s: no CSV file stored in state", owner, repo)
                    continue

                logger.info("=" * 80)

                start_date = last_update
                end_date = datetime.now(timezone.utc)

                exit_code, chunks_done, chunks_fetched = process_repository(
                    owner,
                    repo,
                    csv_file,
                    start_date,
                    end_date,
                    token,
                    workers,
                    args.ai_bot_regex,
                    merge_mode=True,
                )

                if exit_code == 0:
                    completed_repos += 1
                else:
                    # Repo partially processed - record and stop this pass
                    failed_repo = f"{owner}/{repo}"
                    logger.warning(
                        "Partially processed %s (%d chunks completed, quota exhausted)",
                        failed_repo,
                        chunks_done,
                    )
                    break

            # Check if we completed all repos
            if failed_repo is None:
                logger.info("=" * 80)
                logger.info("Successfully processed all %d repositories", completed_repos)
                return 0

            # Failed at least one repo
            if args.wait:
                # Show progress summary before waiting
                show_update_all_progress_summary(tracked_repos, completed_repos, failed_repo)
                logger.info("Will wait for quota reset, then restart from beginning")

                if not quota_manager.wait_for_reset():
                    logger.error("Failed to wait for quota reset, aborting")
                    return 1
                # Refresh quota and restart from beginning
                quota_manager.initialize(token)
                logger.info("Quota refreshed, restarting update-all from beginning")
                continue  # Restart while loop
            else:
                logger.error(
                    "Incomplete: processed %d repos, stopped at %s (quota exhausted). "
                    "Use --wait to retry from beginning, or run again later.",
                    completed_repos,
                    failed_repo,
                )
                return 1

    # Get repository info
    owner = args.owner
    repo = args.repo

    if not owner or not repo:
        detected_owner, detected_repo = get_repo_from_git()
        owner = owner or detected_owner
        repo = repo or detected_repo

    if not owner or not repo:
        logger.error(
            "Could not determine repository. "
            "Use --owner and --repo or run from a git repository."
        )
        return 1

    # Handle update mode for specific repo
    if args.update:
        if not owner or not repo:
            logger.error("--update requires --owner and --repo to be specified")
            return 1

        try:
            repo_state = state_manager.get_repo_state(owner, repo)
        except ValueError as e:
            logger.error("State file validation failed: %s", e)
            return 1

        if not repo_state:
            logger.error(
                "Repository %s/%s not found in state file. "
                "Run without --update first to track this repository.",
                owner,
                repo,
            )
            return 1

        csv_file = repo_state["csv_file"]
        start_date = repo_state["timestamp"]
        end_date = datetime.now(timezone.utc)

        exit_code, _, _ = process_repository(
            owner,
            repo,
            csv_file,
            start_date,
            end_date,
            token,
            workers,
            args.ai_bot_regex,
            merge_mode=True,
        )
        return exit_code

    # Handle single PR update mode
    if args.pr:
        output_file = expand_output_pattern(args.output, owner, repo)
        exit_code = update_single_pr(owner, repo, args.pr, output_file, token, args.ai_bot_regex)
        return exit_code

    # Regular mode (non-update)
    if not args.output:
        output_file = None  # stdout
    else:
        # Expand output pattern with owner/repo placeholders
        output_file = expand_output_pattern(args.output, owner, repo)

    # Get time range from args or defaults
    end_date = parse_timestamp(args.end) if args.end else datetime.now(timezone.utc)
    start_date = (
        parse_timestamp(args.start) if args.start else end_date - timedelta(days=DEFAULT_DAYS_BACK)
    )

    if start_date >= end_date:
        logger.error("Start date must be before end date")
        return 1

    # Only store state if output_file is specified (not stdout)
    if output_file:
        # Create parent directory if needed
        output_path = Path(output_file)
        if output_path.parent and not output_path.parent.exists():
            logger.info("Creating directory: %s", output_path.parent)
            output_path.parent.mkdir(parents=True, exist_ok=True)

        exit_code, _, _ = process_repository(
            owner, repo, output_file, start_date, end_date, token, workers, args.ai_bot_regex
        )
        return exit_code
    else:
        # stdout mode - fetch all PRs then process (no state tracking)
        try:
            all_prs = github_client.fetch_all_prs(owner, repo, start_date, end_date)

            if not all_prs:
                logger.warning("No pull requests found in the specified date range")
                return 0

            metrics = []
            total_prs = len(all_prs)
            completed = 0

            logger.info("Processing %d PRs with %d workers", total_prs, workers)

            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(process_pr, pr, owner, repo, token, args.ai_bot_regex): pr
                    for pr in all_prs
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)
                        logger.info(
                            "Completed PR %d/%d: #%d (updated: %s)",
                            completed,
                            total_prs,
                            pr["number"],
                            pr.get("updated_at", "unknown"),
                        )
                    except Exception as e:
                        logger.error("Failed to process PR #%d: %s", pr["number"], e)

            csv_manager.write_csv(metrics, None, merge_mode=False)
            logger.info("Successfully processed %d pull requests", len(metrics))
            return 0

        except GitHubAPIError as e:
            logger.error("GitHub API error: %s", e)
            return 1
        except Exception as e:
            logger.error("Unexpected error: %s", e, exc_info=args.debug)
            return 1


# ============================================================================
# Utility Functions
# ============================================================================


def setup_logging(debug: bool = False, log_file: str = "gh-pr-metrics.log") -> None:
    """Configure logging based on debug flag with file and stderr output."""
    level = logging.DEBUG if debug else logging.INFO

    # Create formatters
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # Setup root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)

    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Add stderr handler
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(level)
    stderr_handler.setFormatter(formatter)
    root_logger.addHandler(stderr_handler)

    # Add file handler
    try:
        file_handler = logging.FileHandler(log_file, mode="a", encoding="utf-8")
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    except Exception as e:
        logging.warning(f"Could not create log file {log_file}: {e}")


def estimate_api_calls_for_prs(pr_count: int) -> int:
    """
    Estimate API calls needed to process pr_count PRs.
    Each PR requires: timeline events, reviews, review comments, issue comments.
    """
    return pr_count * API_CALLS_PER_PR


def get_github_token() -> Optional[str]:
    """Get GitHub token from environment."""
    return os.getenv("GITHUB_TOKEN")


def get_repo_from_git() -> tuple[Optional[str], Optional[str]]:
    """
    Get repository owner and name from git config.
    Returns (owner, repo) or (None, None) if not found.
    """
    try:
        remote_url = subprocess.check_output(
            ["git", "config", "--get", "remote.origin.url"],
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()

        # Parse SSH URL (git@github.com:owner/repo.git)
        if remote_url.startswith("git@"):
            _, path = remote_url.split(":", 1)
        # Parse HTTPS URL (https://github.com/owner/repo.git)
        elif remote_url.startswith("https://"):
            parts = remote_url.split("/")
            if len(parts) >= 5:
                path = f"{parts[3]}/{parts[4]}"
            else:
                return None, None
        else:
            return None, None

        # Remove .git suffix
        if path.endswith(".git"):
            path = path[:-4]

        owner, repo = path.split("/", 1)
        return owner, repo
    except Exception as e:
        logger.debug("Failed to get repo from git: %s", e)
        return None, None


def parse_timestamp(timestamp_str: str) -> datetime:
    """
    Parse various timestamp formats to datetime (always returns timezone-aware).

    Raises ValueError with clear message if timestamp cannot be parsed.
    """
    if not timestamp_str or not timestamp_str.strip():
        raise ValueError("Timestamp string is empty or None")

    try:
        # Try Unix timestamp first
        dt = datetime.fromtimestamp(float(timestamp_str))
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, OverflowError):
        try:
            # Try ISO 8601 / RFC 3339
            dt = date_parser.parse(timestamp_str)
            # Make timezone-aware if naive
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError) as e:
            raise ValueError(f"Invalid timestamp format: {timestamp_str}") from e


def expand_output_pattern(pattern: str, owner: str, repo: str) -> str:
    """
    Expand output pattern with owner and repo placeholders.
    Supports {owner} and {repo} placeholders.
    """
    return pattern.replace("{owner}", owner).replace("{repo}", repo)


if __name__ == "__main__":
    sys.exit(main())
