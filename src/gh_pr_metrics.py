#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub Pull Request Metrics Tool

A command-line tool to generate CSV reports containing key metrics for all pull
requests in a GitHub repository within a specified time range.
"""

import argparse
import csv
import logging
import os
import subprocess
import sys
import tempfile
import threading
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

try:
    import argcomplete

    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False

import requests
from dateutil import parser as date_parser


# Version
__version__ = "0.1.0"

# Constants
GITHUB_API_BASE = "https://api.github.com"
DEFAULT_DAYS_BACK = 365
DEFAULT_WORKERS = 4
STATE_FILE = Path.home() / ".gh-pr-metrics-state.yaml"
PR_NUMBER_FIELD = "pr_number"
MAX_CHUNK_DAYS = 30  # Maximum days to process in a single chunk
API_CALLS_PER_PR = 4  # Estimate: timeline events, reviews, comments, review comments
API_SAFETY_BUFFER = 10  # Reserve for safety (unauthenticated limit is 60/hour total)
API_QUOTA_RESERVE_PCT = 0.05  # Reserve 5% of total quota, don't exhaust completely


class GitHubAPIError(Exception):
    """Exception raised for GitHub API errors."""

    pass


class QuotaManager:
    """
    Manages GitHub API quota tracking and enforcement.

    Tracks quota from response headers, provides quota checking,
    and supports waiting for quota reset.
    """

    def __init__(self):
        """Initialize quota manager with zero quota."""
        self._remaining = 0
        self._limit = 0
        self._reset = 0  # Unix timestamp when quota resets
        self._lock = threading.Lock()

    def get_quota_prefix(self) -> str:
        """Get quota prefix for logging (thread-safe)."""
        with self._lock:
            if self._limit == 0:
                return "[API ----/----]"

            # Left-pad remaining to match length of limit
            limit_str = str(self._limit)
            remaining_str = str(self._remaining).rjust(len(limit_str))
            return f"[API {remaining_str}/{limit_str}]"

    def update_from_headers(self, headers: dict) -> None:
        """Update quota tracking from API response headers (thread-safe)."""
        remaining = headers.get("X-RateLimit-Remaining")
        limit = headers.get("X-RateLimit-Limit")
        reset = headers.get("X-RateLimit-Reset")

        with self._lock:
            if remaining is not None:
                self._remaining = int(remaining)
            if limit is not None:
                self._limit = int(limit)
            if reset is not None:
                self._reset = int(reset)

    def get_current_quota(self) -> tuple[int, int, int]:
        """Get current quota (thread-safe). Returns (remaining, limit, reset)."""
        with self._lock:
            return self._remaining, self._limit, self._reset

    def initialize(self, token: Optional[str] = None) -> Dict[str, Any]:
        """
        Initialize quota by calling /rate_limit API.
        Returns dict with 'remaining', 'limit', 'reset' keys.
        """
        headers = {
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
        }
        if token:
            headers["Authorization"] = f"Bearer {token}"

        try:
            response = requests.get(f"{GITHUB_API_BASE}/rate_limit", headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()

            core = data.get("resources", {}).get("core", {})
            remaining = core.get("remaining", 0)
            limit = core.get("limit", 0)
            reset = core.get("reset", 0)

            # Update tracking (thread-safe)
            with self._lock:
                self._remaining = remaining
                self._limit = limit
                self._reset = reset

            return {
                "remaining": remaining,
                "limit": limit,
                "reset": reset,
            }
        except Exception as e:
            logging.debug("Could not check rate limit: %s", e)
            return {}

    def calculate_max_prs(self) -> int:
        """
        Calculate maximum number of PRs processable with current quota.
        Reserves % of total quota to avoid complete exhaustion.
        """
        remaining, limit, _ = self.get_current_quota()

        if limit == 0:
            return 100  # Not initialized yet, conservative default

        # Reserve 5% of total quota (don't exhaust completely)
        reserve = int(limit * API_QUOTA_RESERVE_PCT)
        effective_buffer = max(API_SAFETY_BUFFER, reserve)

        available_for_prs = max(0, remaining - effective_buffer)
        max_prs = available_for_prs // API_CALLS_PER_PR

        return max_prs

    def wait_for_reset(self) -> bool:
        """
        Wait until API quota resets.
        Returns True if successfully waited.
        """
        import time

        remaining, limit, reset_timestamp = self.get_current_quota()

        # Add buffer for safety
        reset_timestamp = reset_timestamp + 15

        if reset_timestamp == 0:
            logger.error("Cannot determine quota reset time (not initialized)")
            return False

        reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
        now = datetime.now(timezone.utc)
        wait_seconds = (reset_time - now).total_seconds()

        if wait_seconds <= 0:
            logger.info("Quota already reset, refreshing...")
            return True

        wait_minutes = wait_seconds / 60
        logger.info(f"Waiting {wait_minutes:.1f} minutes for quota reset...")
        logger.info(f"Will resume at {reset_time.strftime('%Y-%m-%d %H:%M:%S UTC')}")

        time.sleep(wait_seconds)

        logger.info("Quota reset complete, refreshing quota like startup...")
        return True

    def check_sufficient(
        self, estimated_calls: int, repo_ctx: str, chunk_info: str = ""
    ) -> tuple[bool, int]:
        """
        Check if sufficient API rate limit available for estimated calls.
        Does NOT make an API call - uses current quota.

        Returns (sufficient, max_prs_possible).
        repo_ctx: Context string like "[owner/repo]" for logging
        chunk_info: Optional string like "Page 1: " for context
        """
        # Use current quota (no API call unless not initialized)
        remaining, limit, _ = self.get_current_quota()

        if limit == 0:
            # Quota not initialized yet - initialize it now (one-time API call)
            logger.warning("[%s] Quota not initialized, fetching current status", repo_ctx)
            rate_info = self.initialize(get_github_token())
            if not rate_info:
                # Failed to get quota - cannot proceed safely
                logger.error("[%s] Failed to fetch quota, cannot verify rate limit", repo_ctx)
                return False, 0
            remaining = rate_info["remaining"]
            limit = rate_info["limit"]

        # Calculate max PRs we can handle
        max_prs = self.calculate_max_prs()

        # Calculate effective buffer (5% of total or fixed buffer, whichever is larger)
        reserve = int(limit * API_QUOTA_RESERVE_PCT)
        effective_buffer = max(API_SAFETY_BUFFER, reserve)

        # Show estimated calls for this operation
        logger.info("[%s] %sEstimated API calls: ~%d", repo_ctx, chunk_info, estimated_calls)

        if remaining <= (estimated_calls + effective_buffer):
            logger.error(
                "[%s] %sInsufficient API rate limit: need ~%d calls + %d buffer "
                "(reserve=%d), only %d available",
                repo_ctx,
                chunk_info,
                estimated_calls,
                effective_buffer,
                reserve,
                remaining,
            )
            logger.warning(
                "[%s] %sCurrent quota allows processing ~%d PRs max", repo_ctx, chunk_info, max_prs
            )
            return False, max_prs

        return True, max_prs


class StateManager:
    """
    Manages state file for tracking repository processing progress.

    State file stores last update timestamp and CSV file path per repository.
    """

    def __init__(self, state_file_path: Path = None):
        """Initialize state manager with path to state file."""
        self._state_file = state_file_path or STATE_FILE

    def load(self) -> Dict[str, Any]:
        """Load state file containing last update dates per repo."""
        if not self._state_file.exists():
            return {}

        try:
            with open(self._state_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
                return data
        except Exception as e:
            logger.warning("Failed to load state file %s: %s", self._state_file, e)
            return {}

    def save(self, state: Dict[str, Any]) -> None:
        """Save state file with last update dates per repo."""
        try:
            with open(self._state_file, "w", encoding="utf-8") as f:
                yaml.safe_dump(state, f, default_flow_style=False, sort_keys=True)

            logger.debug("Saved state file to %s", self._state_file)
        except Exception as e:
            logger.error("Failed to save state file %s: %s", self._state_file, e)
            raise

    def get_repo_remote_url(self, owner: str, repo: str) -> str:
        """
        Get repository remote URL key for state tracking.
        Uses format: https://github.com/owner/repo
        """
        return f"https://github.com/{owner}/{repo}"

    def get_repo_state(self, owner: str, repo: str) -> Optional[Dict[str, Any]]:
        """
        Get state for a repository (timestamp and csv_file).
        Returns dict with 'timestamp' and 'csv_file' keys, or None if not found.
        Raises ValueError if state is malformed.
        """
        state = self.load()
        repo_key = self.get_repo_remote_url(owner, repo)

        if repo_key not in state:
            return None

        entry = state[repo_key]

        if not isinstance(entry, dict):
            raise ValueError(
                f"Invalid state format for {repo_key} in state file. "
                f"Expected dict with 'timestamp' and 'csv_file', got {type(entry).__name__}"
            )

        timestamp_str = entry.get("timestamp", "")
        csv_file = entry.get("csv_file")

        if not timestamp_str:
            raise ValueError(
                f"Missing or empty 'timestamp' field for {repo_key} in state file. "
                f"State file location: {self._state_file}"
            )

        if not csv_file:
            raise ValueError(
                f"Missing or empty 'csv_file' field for {repo_key} in state file. "
                f"State file location: {self._state_file}"
            )

        try:
            timestamp = parse_timestamp(timestamp_str)
        except ValueError as e:
            raise ValueError(
                f"Invalid timestamp for {repo_key} in state file: {e}. "
                f"State file location: {self._state_file}"
            ) from e

        return {"timestamp": timestamp, "csv_file": csv_file}

    def update_repo(self, owner: str, repo: str, timestamp: datetime, csv_file: str) -> None:
        """Update state file with new last update date and csv file for a repository."""
        state = self.load()
        repo_key = self.get_repo_remote_url(owner, repo)
        # Store as UTC timestamp without timezone component
        utc_timestamp = timestamp.astimezone(timezone.utc).replace(tzinfo=None)
        state[repo_key] = {"timestamp": utc_timestamp.isoformat(), "csv_file": csv_file}
        self.save(state)

    def get_all_tracked_repos(self) -> List[Dict[str, Any]]:
        """
        Get all tracked repositories from state file.
        Returns list of dicts with 'url', 'owner', 'repo', 'timestamp', 'csv_file'.
        """
        state = self.load()
        repos = []

        for repo_url, entry in state.items():
            # Parse URL to get owner/repo
            # Format: https://github.com/owner/repo
            try:
                parts = repo_url.rstrip("/").split("/")
                if len(parts) >= 2:
                    owner = parts[-2]
                    repo = parts[-1]

                    if isinstance(entry, dict):
                        timestamp = parse_timestamp(entry.get("timestamp", ""))
                        csv_file = entry.get("csv_file")
                    else:
                        logger.warning("Skipping %s: invalid state format", repo_url)
                        continue

                    repos.append(
                        {
                            "url": repo_url,
                            "owner": owner,
                            "repo": repo,
                            "timestamp": timestamp,
                            "csv_file": csv_file,
                        }
                    )
            except Exception as e:
                logger.warning("Failed to parse state entry for %s: %s", repo_url, e)
                continue

        return repos


class GitHubClient:
    """
    GitHub API client for making authenticated requests.

    Handles all communication with GitHub API and automatically
    updates quota tracking from response headers.
    """

    def __init__(self, token: Optional[str] = None):
        """Initialize GitHub client with optional token."""
        self._token = token

    def make_request(self, url: str, params: Optional[Dict[str, Any]] = None) -> Any:
        """Make a GitHub API request with error handling."""
        headers = {
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
        }
        if self._token:
            headers["Authorization"] = f"Bearer {self._token}"

        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            response.raise_for_status()

            # Update quota tracking from response headers
            quota_manager.update_from_headers(response.headers)

            return response.json()
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                # Check if it's actually a rate limit error
                rate_limit = e.response.headers.get("X-RateLimit-Remaining")
                if rate_limit == "0":
                    reset_time = e.response.headers.get("X-RateLimit-Reset", "unknown")
                    auth_status = "authenticated" if self._token else "unauthenticated"
                    raise GitHubAPIError(
                        f"GitHub API rate limit exceeded ({auth_status}). "
                        f"Rate limit resets at {reset_time}. "
                        f"Limit: {e.response.headers.get('X-RateLimit-Limit', 'unknown')}"
                    )
                else:
                    # 403 but not rate limit - likely permissions
                    raise GitHubAPIError(
                        "GitHub API forbidden: insufficient permissions or invalid token."
                    )
            elif e.response.status_code == 401:
                raise GitHubAPIError("GitHub API unauthorized: invalid or expired token.")
            elif e.response.status_code == 404:
                raise GitHubAPIError("Repository not found or insufficient permissions.")
            else:
                raise GitHubAPIError(f"GitHub API error: {e}")
        except requests.exceptions.RequestException as e:
            raise GitHubAPIError(f"Network error: {e}")

    def fetch_prs_page(
        self,
        owner: str,
        repo: str,
        start_date: datetime,
        end_date: datetime,
        page: int = 1,
        per_page: int = 100,
    ) -> tuple[List[Dict[str, Any]], bool]:
        """
        Fetch a single page of pull requests sorted by updated date.
        Returns (prs_in_range, has_more_pages).
        """
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls"

        params = {
            "state": "all",
            "sort": "updated",
            "direction": "asc",  # Oldest first
            "page": page,
            "per_page": per_page,
        }

        prs = self.make_request(url, params)

        if not prs:
            return [], False

        prs_in_range = []

        for pr in prs:
            updated_at = date_parser.parse(pr["updated_at"])

            # Stop if we're before the start date
            if updated_at < start_date:
                return prs_in_range, False

            # Only include PRs within date range
            if start_date <= updated_at <= end_date:
                prs_in_range.append(pr)

        # If we got a full page, there might be more
        has_more = len(prs) == per_page

        return prs_in_range, has_more

    def list_repos(self, owner: str) -> List[str]:
        """
        List all repositories for an owner (user or organization).
        Returns list of repository names.
        """
        repos = []
        page = 1
        per_page = 100

        # Try as organization first
        url_base = f"{GITHUB_API_BASE}/orgs/{owner}/repos"
        is_org = True

        while True:
            params = {"page": page, "per_page": per_page, "type": "all"}

            try:
                response = self.make_request(url_base, params)
            except GitHubAPIError as e:
                # If org request fails with 404, try as user
                if "not found" in str(e).lower() and is_org:
                    logger.debug("Not an organization, trying as user: %s", owner)
                    url_base = f"{GITHUB_API_BASE}/users/{owner}/repos"
                    is_org = False
                    page = 1  # Reset page counter
                    continue
                else:
                    raise

            if not response:
                break

            for repo in response:
                repos.append(repo["name"])

            # Break if we got fewer results than requested
            if len(response) < per_page:
                break

            page += 1

        return repos

    def validate_repo_access(self, owner: str, repo: str) -> bool:
        """
        Validate that we have access to a repository.
        Returns True if accessible, False otherwise.
        """
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}"

        try:
            self.make_request(url)
            return True
        except GitHubAPIError as e:
            logger.debug("Cannot access %s/%s: %s", owner, repo, e)
            return False

    def fetch_timeline_events(self, owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:
        """Fetch timeline events for a PR."""
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/issues/{pr_number}/events"
        return self.make_request(url)

    def fetch_issue_comments(self, comments_url: str) -> List[Dict[str, Any]]:
        """Fetch issue comments using the comments_url from PR data."""
        return self.make_request(comments_url)

    def fetch_review_comments(self, review_comments_url: str) -> List[Dict[str, Any]]:
        """Fetch review comments using the review_comments_url from PR data."""
        return self.make_request(review_comments_url)

    def fetch_reviews(self, owner: str, repo: str, pr_number: int) -> List[Dict[str, Any]]:
        """Fetch reviews for a PR."""
        url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls/{pr_number}/reviews"
        return self.make_request(url)


class CSVManager:
    """
    Manages CSV I/O operations for PR metrics.

    Handles reading existing CSV files, writing metrics data,
    and managing field definitions.
    """

    def get_fieldnames(self) -> List[str]:
        """Get CSV field names in order."""
        return [
            PR_NUMBER_FIELD,
            "title",
            "author",
            "created_at",
            "ready_for_review_at",
            "merged_at",
            "closed_at",
            "total_comment_count",
            "non_ai_bot_comment_count",
            "ai_bot_comment_count",
            "non_ai_bot_login_names",
            "ai_bot_login_names",
            "changes_requested_count",
            "unique_change_requesters",
            "approval_count",
            "status",
            "url",
            "errors",
        ]

    def read_csv(self, csv_file: str) -> Dict[int, Dict[str, Any]]:
        """
        Read existing CSV file and return dict keyed by PR number.
        Returns empty dict if file doesn't exist or can't be read.
        """
        if not os.path.exists(csv_file):
            return {}

        try:
            existing_data = {}
            with open(csv_file, "r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # Key by PR number for O(1) lookups
                    pr_number = int(row[PR_NUMBER_FIELD])
                    existing_data[pr_number] = row

            logger.info("Loaded %d existing PRs from %s", len(existing_data), csv_file)
            return existing_data
        except Exception as e:
            logger.warning("Failed to read existing CSV %s: %s", csv_file, e)
            return {}

    def write_csv(
        self,
        metrics: List[Dict[str, Any]],
        output_file: Optional[str],
        merge_mode: bool = False,
        force_write: bool = False,
    ) -> None:
        """
        Write metrics to CSV file or stdout.

        If merge_mode is True and output_file exists, existing PR data is loaded,
        updated with new metrics, and written back.

        If force_write is True, write headers even if metrics list is empty.
        """
        if not metrics and not force_write:
            logger.warning("No pull requests to output")
            return

        fieldnames = self.get_fieldnames()

        # Merge with existing data if requested
        if merge_mode and output_file:
            existing_data = self.read_csv(output_file)

            # Update existing data with new metrics
            for metric in metrics:
                pr_number = metric[PR_NUMBER_FIELD]
                existing_data[pr_number] = metric

            # Convert back to list
            all_metrics = list(existing_data.values())
            logger.info("Merged data: %d total PRs (%d new/updated)", len(all_metrics), len(metrics))
        else:
            all_metrics = metrics

        if output_file:
            # Write to temp file first, then atomically rename
            output_path = Path(output_file)
            temp_fd, temp_path = tempfile.mkstemp(
                dir=output_path.parent if output_path.parent.exists() else tempfile.gettempdir(),
                prefix=".tmp_pr_metrics_",
                suffix=".csv",
            )

            try:
                with os.fdopen(temp_fd, "w", newline="", encoding="utf-8") as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_metrics)

                # Atomic rename
                os.rename(temp_path, output_file)
                logger.info("Writing output to %s", output_file)
            except Exception as e:
                # Keep temp file for debugging
                logger.error("Failed to write CSV output. Temp file retained at: %s", temp_path)
                raise e
        else:
            writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_metrics)


class LoggingManager:
    """
    Manages application logging with quota-aware prefixes.
    
    Wraps standard logging to inject API quota status into all log messages.
    """

    def __init__(self, quota_manager: QuotaManager):
        """Initialize logging manager with reference to quota manager."""
        self._quota_manager = quota_manager

    def info(self, msg: str, *args, **kwargs) -> None:
        """Log INFO with API quota prefix."""
        logging.info(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def warning(self, msg: str, *args, **kwargs) -> None:
        """Log WARNING with API quota prefix."""
        logging.warning(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def error(self, msg: str, *args, **kwargs) -> None:
        """Log ERROR with API quota prefix."""
        logging.error(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def debug(self, msg: str, *args, **kwargs) -> None:
        """Log DEBUG with API quota prefix."""
        logging.debug(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)


# ============================================================================
# Global Manager Instances
# ============================================================================

quota_manager = QuotaManager()
logger = LoggingManager(quota_manager)
state_manager = StateManager()
github_client = None  # Initialized in main() with token
csv_manager = CSVManager()


# ============================================================================
# PR Processing Functions
# ============================================================================


def get_ready_for_review_time(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> str:
    """Get the timestamp when PR was last marked ready for review."""
    # If never a draft, use created_at
    if not pr.get("draft", False) and pr.get("created_at"):
        # Check events to see if it was ever a draft
        try:
            events = github_client.fetch_timeline_events(owner, repo, pr["number"])
            ready_events = [e for e in events if e.get("event") == "ready_for_review"]
            if ready_events:
                # Use the latest ready_for_review event
                return ready_events[-1]["created_at"]
        except GitHubAPIError as e:
            logger.debug("Failed to fetch events for PR #%d: %s", pr["number"], e)

    return pr["created_at"]


def count_comments(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], ai_bot_pattern: str = None
) -> tuple[int, int, int, str, str]:
    """
    Count total comments, non-AI bot comments, and AI bot comments.
    Returns (total_comments, non_ai_bot_comments, ai_bot_comments,
             non_ai_bot_login_names, ai_bot_login_names).
    Non-AI bot and AI bot login names are comma-separated strings.
    ai_bot_pattern: regex pattern to identify AI bots
    """
    import re

    total_comments = 0
    non_ai_bot_comments = 0
    ai_bot_comments = 0
    non_ai_bot_logins = set()
    ai_bot_logins = set()

    # Issue comments (general PR comments)
    comments_url = pr["comments_url"]
    try:
        comments = github_client.fetch_issue_comments(comments_url)
        for comment in comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                if ai_bot_pattern and re.match(ai_bot_pattern, login):
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch comments for PR #%d: %s", pr["number"], e)

    # Review comments (inline code comments)
    review_comments_url = pr["review_comments_url"]
    try:
        review_comments = github_client.fetch_review_comments(review_comments_url)
        for comment in review_comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                if ai_bot_pattern and re.match(ai_bot_pattern, login):
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch review comments for PR #%d: %s", pr["number"], e)

    # Convert to comma-separated strings, sorted for consistency
    non_ai_bot_login_names = ",".join(sorted(non_ai_bot_logins))
    ai_bot_login_names = ",".join(sorted(ai_bot_logins))

    return (
        total_comments,
        non_ai_bot_comments,
        ai_bot_comments,
        non_ai_bot_login_names,
        ai_bot_login_names,
    )


def get_review_metrics(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> tuple[int, int, int]:
    """
    Get review metrics: changes requested count, unique change requesters, approvals.
    Returns (changes_requested_count, unique_change_requesters, approval_count).
    """
    try:
        reviews = github_client.fetch_reviews(owner, repo, pr["number"])
    except GitHubAPIError as e:
        logger.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
        return 0, 0, 0

    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r["user"]["login"]
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review["user"]["login"]
            # Later reviews override earlier ones
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def determine_pr_status(pr: Dict[str, Any]) -> str:
    """Determine the current status of the PR."""
    if pr.get("draft"):
        return "draft"
    elif pr.get("merged_at"):
        return "merged"
    elif pr.get("state") == "closed":
        return "closed"
    else:
        return "open"


def process_pr(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], ai_bot_pattern: str = None
) -> Dict[str, Any]:
    """Process a single PR and extract all metrics."""
    pr_number = pr["number"]
    logger.debug("Processing PR #%d: %s", pr_number, pr["title"])

    errors = []
    metrics: Dict[str, Any] = {
        PR_NUMBER_FIELD: pr_number,
        "title": pr["title"],
        "author": pr["user"]["login"] if pr.get("user") else "",
        "created_at": pr["created_at"],
        "url": pr["html_url"],
    }

    # Ready for review time
    try:
        metrics["ready_for_review_at"] = get_ready_for_review_time(pr, owner, repo, token)
    except Exception as e:
        logger.debug("Error getting ready time for PR #%d: %s", pr_number, e)
        metrics["ready_for_review_at"] = pr["created_at"]
        errors.append(f"ready_time: {e}")

    # Comment counts
    try:
        total_comments, non_ai_bot_comments, ai_bot_comments, non_ai_bot_names, ai_bot_names = (
            count_comments(pr, owner, repo, token, ai_bot_pattern)
        )
        metrics["total_comment_count"] = total_comments
        metrics["non_ai_bot_comment_count"] = non_ai_bot_comments
        metrics["ai_bot_comment_count"] = ai_bot_comments
        metrics["non_ai_bot_login_names"] = non_ai_bot_names
        metrics["ai_bot_login_names"] = ai_bot_names
    except Exception as e:
        logger.debug("Error counting comments for PR #%d: %s", pr_number, e)
        metrics["total_comment_count"] = 0
        metrics["non_ai_bot_comment_count"] = 0
        metrics["ai_bot_comment_count"] = 0
        metrics["non_ai_bot_login_names"] = ""
        metrics["ai_bot_login_names"] = ""
        errors.append(f"comments: {e}")

    # Review metrics
    try:
        changes_requested, unique_requesters, approvals = get_review_metrics(pr, owner, repo, token)
        metrics["changes_requested_count"] = changes_requested
        metrics["unique_change_requesters"] = unique_requesters
        metrics["approval_count"] = approvals
    except Exception as e:
        logger.debug("Error getting reviews for PR #%d: %s", pr_number, e)
        metrics["changes_requested_count"] = 0
        metrics["unique_change_requesters"] = 0
        metrics["approval_count"] = 0
        errors.append(f"reviews: {e}")

    # Status
    metrics["status"] = determine_pr_status(pr)

    # Merged and closed timestamps
    metrics["merged_at"] = pr.get("merged_at") or ""
    metrics["closed_at"] = pr.get("closed_at") or ""

    # Errors
    metrics["errors"] = "; ".join(errors) if errors else ""

    return metrics


# ============================================================================
# Main Business Logic
# ============================================================================


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate CSV reports of GitHub pull request metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze PRs from current repository (last year)
  gh-pr-metrics

  # Specify repository explicitly
  gh-pr-metrics --owner microsoft --repo vscode

  # Custom time range
  gh-pr-metrics --start 2024-01-01 --end 2024-12-31

  # Output to file (stores path in state for future updates)
  gh-pr-metrics --output metrics.csv

  # Initialize repositories (validates access, creates state entries)
  gh-pr-metrics --init --owner ansible --output "data/{owner}-{repo}.csv" --start "2024-11-01"

  # Update specific repo (uses stored CSV path from state)
  gh-pr-metrics --owner microsoft --repo vscode --update

  # Update all tracked repositories
  gh-pr-metrics --update-all

Environment Variables:
  GITHUB_TOKEN    GitHub personal access token for authentication
        """,
    )

    parser.add_argument("--owner", help="Repository owner (default: auto-detect from git)")
    parser.add_argument("--repo", help="Repository name (default: auto-detect from git)")
    parser.add_argument(
        "--start",
        help="Start timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: 365 days ago",
    )
    parser.add_argument(
        "--end",
        help="End timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: now",
    )
    parser.add_argument("--output", "-o", help="Output file path (default: stdout)")
    parser.add_argument(
        "--init",
        action="store_true",
        help="Initialize repositories in state file. Validates access and stores --start date. "
        "Requires --owner and --output. If only --owner provided, initializes all repos. "
        "Output supports patterns like 'data/{owner}-{repo}.csv'",
    )
    parser.add_argument(
        "--update",
        action="store_true",
        help="Update mode: fetch only PRs since last update and merge with existing CSV. "
        "Requires --owner and --repo. Uses CSV path stored in state file. "
        "Cannot be used with --output (uses stored path).",
    )
    parser.add_argument(
        "--update-all",
        action="store_true",
        help="Update all tracked repositories. Cannot be used with --owner/--repo or --output.",
    )
    parser.add_argument(
        "--wait",
        action="store_true",
        help="Wait for rate limit to reset if quota exhausted, then continue processing.",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument(
        "--workers",
        type=int,
        default=DEFAULT_WORKERS,
        help=f"Number of parallel workers for processing PRs (default: {DEFAULT_WORKERS})",
    )
    parser.add_argument(
        "--ai-bot-regex",
        default="cursor\\[bot\\]",
        help="Regex pattern to identify AI bots (default: cursor\\[bot\\])",
    )
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    return parser.parse_args()


def process_repository(
    owner: str,
    repo: str,
    output_file: str,
    start_date: datetime,
    end_date: datetime,
    token: Optional[str],
    workers: int,
    ai_bot_regex: str,
    merge_mode: bool = False,
) -> tuple[int, int, int]:
    """
    Process a single repository and generate/update metrics CSV using page-based processing.

    Fetches PRs one page at a time (sorted by updated date), processes each page,
    and checks rate limit before fetching the next page. Saves progress after each page.

    Returns (exit_code, pages_completed, total_pages_fetched).
    exit_code: 0 on success, 1 on error/stopped
    pages_completed: Number of pages successfully processed
    total_pages_fetched: Total pages fetched (may be more than completed if stopped mid-page)
    """
    repo_ctx = f"{owner}/{repo}"

    logger.info("[%s] Starting processing", repo_ctx)
    logger.info("[%s] Date range: %s to %s", repo_ctx, start_date.isoformat(), end_date.isoformat())

    # Check quota before starting any work
    max_prs_can_process = quota_manager.calculate_max_prs()
    if max_prs_can_process == 0:
        remaining, limit, _ = quota_manager.get_current_quota()
        logger.error(
            "[%s] Cannot process any PRs with current quota (exhausted or below reserve)",
            repo_ctx,
        )
        logger.error(
            "[%s] Current quota: %d/%d remaining",
            repo_ctx,
            remaining,
            limit,
        )
        # Note: per-repo wait not implemented, only at update-all level
        return 1, 0, 0

    logger.info(
        "[%s] Quota check: can process up to ~%d PRs before hitting reserve",
        repo_ctx,
        max_prs_can_process,
    )

    # Page-based processing: fetch and process one page at a time
    page = 1
    pages_completed = 0
    total_pages_fetched = 0
    last_processed_timestamp = start_date
    per_page = 100

    logger.info("[%s] Fetching PRs sorted by updated date (oldest first)", repo_ctx)

    try:
        while True:
            # Fetch one page of PRs
            prs, has_more = github_client.fetch_prs_page(
                owner, repo, start_date, end_date, page, per_page
            )
            total_pages_fetched = page

            if not prs:
                if page == 1:
                    # No PRs found at all
                    logger.info("[%s] No pull requests found in the specified date range", repo_ctx)
                    pages_completed = 1  # Consider it one "empty" page completed
                else:
                    # Exhausted PR list - update state to last processed or end_date
                    logger.info("[%s] Page %d: No more PRs in date range", repo_ctx, page)
                # Update to end_date so we don't reprocess this range
                state_manager.update_repo(owner, repo, end_date, output_file)
                break

            total_prs = len(prs)
            logger.info("[%s] Page %d: Found %d PRs", repo_ctx, page, total_prs)

            # Check rate limit and truncate page if needed
            estimated_calls = estimate_api_calls_for_prs(total_prs)
            page_info_str = f"Page {page}: "
            sufficient, max_prs = quota_manager.check_sufficient(
                estimated_calls, repo_ctx, page_info_str
            )

            if not sufficient:
                # Can't process full page, but maybe we can process some PRs
                if max_prs > 0:
                    # Truncate to what we can handle
                    prs_to_process = prs[:max_prs]
                    logger.warning(
                        "[%s] Page %d: Truncating from %d to %d PRs due to quota limit",
                        repo_ctx,
                        page,
                        total_prs,
                        max_prs,
                    )
                    prs = prs_to_process
                    total_prs = len(prs)
                    # Will process these PRs, then stop (don't fetch more pages)
                    has_more = False  # Force stop after this partial page
                else:
                    # Can't process any PRs
                    logger.error(
                        "[%s] Stopping at page %d due to insufficient rate limit", repo_ctx, page
                    )
                    logger.error(
                        "[%s] Progress saved. Resume with --update (or --update-all) "
                        "after rate limit resets.",
                        repo_ctx,
                    )
                    break  # Exit while loop

            # Process this page of PRs
            metrics = []
            completed = 0

            logger.info(
                "[%s] Page %d: Processing %d PRs with %d workers",
                repo_ctx,
                page,
                total_prs,
                workers,
            )

            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(process_pr, pr, owner, repo, token, ai_bot_regex): pr
                    for pr in prs
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    pr_number = pr["number"]
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)
                        logger.info(
                            "[%s] Page %d: Completed PR %d/%d: #%d",
                            repo_ctx,
                            page,
                            completed,
                            total_prs,
                            pr_number,
                        )
                    except Exception as e:
                        logger.error(
                            "[%s] Page %d: Failed to process PR #%d: %s",
                            repo_ctx,
                            page,
                            pr_number,
                            e,
                        )

            # Write output for this page (always merge after first page)
            page_merge_mode = merge_mode or (page > 1)
            csv_manager.write_csv(metrics, output_file, merge_mode=page_merge_mode)

            # Find the newest updated_at timestamp from this page for resume point
            if prs:
                # PRs are sorted by updated asc, so last one is newest (how far we got)
                newest_pr_updated = date_parser.parse(prs[-1]["updated_at"])
                last_processed_timestamp = newest_pr_updated

                # Update state file with the oldest timestamp from this page
                state_manager.update_repo(owner, repo, last_processed_timestamp, output_file)
                pages_completed += 1

                logger.info(
                    "[%s] Page %d: Processed %d PRs, updated state to %s",
                    repo_ctx,
                    page,
                    len(metrics),
                    last_processed_timestamp.isoformat(),
                )

            # Check if there are more pages
            if not has_more:
                logger.info("[%s] Reached end of PR list after %d pages", repo_ctx, page)
                break

            page += 1

    except GitHubAPIError as e:
        logger.error("[%s] Page %d: GitHub API error: %s", repo_ctx, page, e)
        return 1, pages_completed, total_pages_fetched
    except Exception as e:
        logger.error("[%s] Page %d: Unexpected error: %s", repo_ctx, page, e, exc_info=True)
        return 1, pages_completed, total_pages_fetched

    logger.info("[%s] Successfully completed all %d pages", repo_ctx, pages_completed)
    return 0, pages_completed, total_pages_fetched


def main() -> int:
    """Main entry point."""
    args = parse_arguments()
    setup_logging(args.debug)

    logger.info("GitHub PR Metrics Tool v%s", __version__)
    logger.debug("Arguments: %s", args)

    # Validate argument combinations
    if args.init and not args.owner:
        logger.error("--init requires --owner")
        return 1

    if args.init and not args.output:
        logger.error("--init requires --output (supports patterns like 'data/{owner}-{repo}.csv')")
        return 1

    if args.init and args.end:
        logger.error("--init cannot be used with --end (uses current time)")
        return 1

    if args.init and (args.update or args.update_all):
        logger.error("--init cannot be used with --update or --update-all")
        return 1

    if args.update and args.output:
        logger.error("--update and --output cannot be used together (update uses stored CSV path)")
        return 1

    if args.update and (args.start or args.end):
        logger.error(
            "--update cannot be used with --start or --end (uses last update date from state)"
        )
        return 1

    if args.update_all and args.output:
        logger.error("--update-all and --output cannot be used together (uses stored CSV paths)")
        return 1

    if args.update_all and (args.start or args.end):
        logger.error(
            "--update-all cannot be used with --start or --end (uses last update dates from state)"
        )
        return 1

    if args.update_all and (args.owner or args.repo):
        logger.error("--update-all cannot be used with --owner or --repo")
        return 1

    if args.update and args.update_all:
        logger.error("--update and --update-all cannot be used together")
        return 1

    # Get GitHub token and initialize client
    token = get_github_token()
    global github_client
    github_client = GitHubClient(token)

    if not token:
        logger.warning(
            "No GITHUB_TOKEN found. API rate limits will be restrictive "
            "for unauthenticated requests."
        )
        # Force single worker to avoid hitting rate limits too quickly
        workers = 1 if args.workers > 1 else args.workers
        if args.workers > 1:
            logger.warning("Forcing --workers=1 due to missing GITHUB_TOKEN")
    else:
        workers = args.workers

    # Check and display rate limit status
    rate_info = quota_manager.initialize(token)
    if rate_info:
        remaining = rate_info.get("remaining", "unknown")
        limit = rate_info.get("limit", "unknown")
        reset_timestamp = rate_info.get("reset", 0)

        if reset_timestamp:
            reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
            now = datetime.now(timezone.utc)
            time_until_reset = reset_time - now
            minutes_until_reset = time_until_reset.total_seconds() / 60
            reset_str = f"{minutes_until_reset:.1f} minutes"
        else:
            reset_str = "unknown"

        logger.info(
            "Rate limit: %s/%s remaining (resets in %s)",
            remaining,
            limit,
            reset_str,
        )

    # Handle init mode
    if args.init:
        owner = args.owner
        repos_to_init = []

        # If repo specified, just use that one
        if args.repo:
            repos_to_init = [args.repo]
            logger.info("Initializing single repository: %s/%s", owner, args.repo)
        else:
            # List all repos for owner
            logger.info("Listing all repositories for owner: %s", owner)
            try:
                repos_to_init = github_client.list_repos(owner)
                logger.info("Found %d repositories for %s", len(repos_to_init), owner)
            except GitHubAPIError as e:
                logger.error("Failed to list repositories for %s: %s", owner, e)
                return 1

        if not repos_to_init:
            logger.error("No repositories found for owner: %s", owner)
            return 1

        # Determine start date
        if args.start:
            start_date = parse_timestamp(args.start)
        else:
            start_date = datetime.now(timezone.utc) - timedelta(days=DEFAULT_DAYS_BACK)

        logger.info("Using start date: %s", start_date.isoformat())

        # Filter out already-tracked repos to get accurate count
        repos_needing_validation = []
        already_tracked_count = 0
        for repo in repos_to_init:
            if state_manager.get_repo_state(owner, repo):
                already_tracked_count += 1
            else:
                repos_needing_validation.append(repo)

        if already_tracked_count > 0:
            logger.info(
                "%d repositories already tracked, %d need validation",
                already_tracked_count,
                len(repos_needing_validation),
            )

        # Check rate limit before validating repos (1 API call per repo)
        if repos_needing_validation:
            estimated_calls = len(repos_needing_validation)
            sufficient, _ = quota_manager.check_sufficient(estimated_calls, f"{owner}/*")
            if not sufficient:
                logger.error(
                    "Insufficient API rate limit to validate %d repositories. "
                    "Each repo requires 1 API call for access validation.",
                    len(repos_needing_validation),
                )
                return 1

        # Validate access and initialize each repo
        successful = 0
        failed = 0
        skipped = 0

        for repo in repos_to_init:
            # Check if already tracked
            existing_state = state_manager.get_repo_state(owner, repo)
            if existing_state:
                logger.info(
                    "[%s/%s] Already tracked (last update: %s), skipping",
                    owner,
                    repo,
                    existing_state["timestamp"].isoformat(),
                )
                skipped += 1
                continue

            # Validate access
            logger.info("[%s/%s] Validating access...", owner, repo)
            if not github_client.validate_repo_access(owner, repo):
                logger.warning("[%s/%s] Cannot access repository, skipping", owner, repo)
                failed += 1
                continue

            # Expand output pattern
            csv_file = expand_output_pattern(args.output, owner, repo)

            # Create parent directory if needed
            csv_path = Path(csv_file)
            if csv_path.parent and not csv_path.parent.exists():
                logger.info("Creating directory: %s", csv_path.parent)
                csv_path.parent.mkdir(parents=True, exist_ok=True)

            # Initialize empty CSV file
            logger.info("[%s/%s] Initializing CSV: %s", owner, repo, csv_file)
            csv_manager.write_csv([], csv_file, merge_mode=False, force_write=True)

            # Update state file
            state_manager.update_repo(owner, repo, start_date, csv_file)
            logger.info("[%s/%s] Initialized (start date: %s)", owner, repo, start_date.isoformat())
            successful += 1

        # Summary
        logger.info("=" * 80)
        logger.info("Initialization complete:")
        logger.info("  Successful: %d", successful)
        logger.info("  Skipped (already tracked): %d", skipped)
        logger.info("  Failed (no access): %d", failed)

        if failed > 0:
            return 1
        return 0

    # Handle update-all mode
    if args.update_all:
        try:
            tracked_repos = state_manager.get_all_tracked_repos()
        except Exception as e:
            logger.error("Failed to load state file: %s", e)
            return 1

        if not tracked_repos:
            logger.error(
                "No tracked repositories found in state file. "
                "Run without --update-all first to track repositories."
            )
            return 1

        # Sort repositories by timestamp (oldest first)
        tracked_repos.sort(key=lambda r: r["timestamp"])

        logger.info("Updating %d tracked repositories", len(tracked_repos))

        # Check quota before starting update-all
        max_prs = quota_manager.calculate_max_prs()
        if max_prs == 0:
            remaining, limit, _ = quota_manager.get_current_quota()
            logger.error(
                "Cannot start update-all: quota exhausted or below reserve (%d/%d remaining)",
                remaining,
                limit,
            )
            if args.wait:
                if not quota_manager.wait_for_reset():
                    return 1
                # After waiting, refresh quota like startup
                quota_manager.initialize(token)
                max_prs = quota_manager.calculate_max_prs()
                if max_prs == 0:
                    logger.error("Quota still exhausted after reset, aborting")
                    return 1
                logger.info("Resuming update-all with refreshed quota")
            else:
                logger.error("Use --wait to automatically wait for reset, or try again later")
                return 1

        logger.info("Quota check: can process up to ~%d PRs total across all repos", max_prs)

        completed_repos = 0
        partial_repo_info = None
        unprocessed_count = 0

        for idx, repo_info in enumerate(tracked_repos):
            owner = repo_info["owner"]
            repo = repo_info["repo"]
            csv_file = repo_info["csv_file"]
            last_update = repo_info["timestamp"]

            if not csv_file:
                logger.warning("Skipping %s/%s: no CSV file stored in state", owner, repo)
                unprocessed_count += 1
                continue

            logger.info("=" * 80)

            start_date = last_update
            end_date = datetime.now(timezone.utc)

            exit_code, pages_done, pages_fetched = process_repository(
                owner,
                repo,
                csv_file,
                start_date,
                end_date,
                token,
                workers,
                args.ai_bot_regex,
                merge_mode=True,
            )

            if exit_code == 0:
                completed_repos += 1
            else:
                # Stopped early (likely rate limit)
                partial_repo_info = {
                    "owner": owner,
                    "repo": repo,
                    "pages_done": pages_done,
                    "pages_fetched": pages_fetched,
                }
                # Count remaining repos as unprocessed
                unprocessed_count = len(tracked_repos) - idx - 1

                logger.warning(
                    "Stopping update-all: %s/%s partially completed "
                    "(%d pages processed, %d pages fetched)",
                    owner,
                    repo,
                    pages_done,
                    pages_fetched,
                )

                # If --wait flag set, wait for quota reset then continue
                if args.wait:
                    logger.info("--wait flag set, will wait for quota reset and continue")
                    if not quota_manager.wait_for_reset():
                        logger.error("Failed to wait for quota reset, aborting")
                        break
                    # Refresh quota like startup
                    quota_manager.initialize(token)
                    logger.info(
                        "Quota refreshed, continuing with remaining %d repos", unprocessed_count
                    )
                    # Don't break - continue to next repo
                    partial_repo_info = None  # Clear partial info since we're continuing
                    continue

                if unprocessed_count > 0:
                    logger.warning(
                        "%d repositories not yet processed (will resume on next --update-all)",
                        unprocessed_count,
                    )
                break

        logger.info("=" * 80)

        # Summary output
        if partial_repo_info:
            logger.info("Completed: %d repos fully processed", completed_repos)
            logger.info(
                "Partial: %s/%s (%d pages processed)",
                partial_repo_info["owner"],
                partial_repo_info["repo"],
                partial_repo_info["pages_done"],
            )
            if unprocessed_count > 0:
                logger.info("Not processed: %d repos", unprocessed_count)
            return 1  # Incomplete
        else:
            logger.info("Successfully processed all %d repositories", completed_repos)
            return 0

    # Get repository info
    owner = args.owner
    repo = args.repo

    if not owner or not repo:
        detected_owner, detected_repo = get_repo_from_git()
        owner = owner or detected_owner
        repo = repo or detected_repo

    if not owner or not repo:
        logger.error(
            "Could not determine repository. "
            "Use --owner and --repo or run from a git repository."
        )
        return 1

    # Handle update mode for specific repo
    if args.update:
        if not owner or not repo:
            logger.error("--update requires --owner and --repo to be specified")
            return 1

        try:
            repo_state = state_manager.get_repo_state(owner, repo)
        except ValueError as e:
            logger.error("State file validation failed: %s", e)
            return 1

        if not repo_state:
            logger.error(
                "Repository %s/%s not found in state file. "
                "Run without --update first to track this repository.",
                owner,
                repo,
            )
            return 1

        csv_file = repo_state["csv_file"]
        start_date = repo_state["timestamp"]
        end_date = datetime.now(timezone.utc)

        exit_code, _, _ = process_repository(
            owner,
            repo,
            csv_file,
            start_date,
            end_date,
            token,
            workers,
            args.ai_bot_regex,
            merge_mode=True,
        )
        return exit_code

    # Regular mode (non-update)
    if not args.output:
        output_file = None  # stdout
    else:
        output_file = args.output

    # Get time range from args or defaults
    end_date = parse_timestamp(args.end) if args.end else datetime.now(timezone.utc)
    start_date = (
        parse_timestamp(args.start) if args.start else end_date - timedelta(days=DEFAULT_DAYS_BACK)
    )

    if start_date >= end_date:
        logger.error("Start date must be before end date")
        return 1

    # Only store state if output_file is specified (not stdout)
    if output_file:
        exit_code, _, _ = process_repository(
            owner, repo, output_file, start_date, end_date, token, workers, args.ai_bot_regex
        )
        return exit_code
    else:
        # stdout mode - fetch all PRs then process (no state tracking)
        try:
            all_prs = []
            page = 1

            while True:
                prs, has_more = github_client.fetch_prs_page(
                    owner, repo, start_date, end_date, page
                )

                if prs:
                    all_prs.extend(prs)

                if not has_more:
                    break

                page += 1

            if not all_prs:
                logger.warning("No pull requests found in the specified date range")
                return 0

            metrics = []
            total_prs = len(all_prs)
            completed = 0

            logger.info("Processing %d PRs with %d workers", total_prs, workers)

            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(process_pr, pr, owner, repo, token, args.ai_bot_regex): pr
                    for pr in all_prs
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)
                        logger.info("Completed PR %d/%d: #%d", completed, total_prs, pr["number"])
                    except Exception as e:
                        logger.error("Failed to process PR #%d: %s", pr["number"], e)

            csv_manager.write_csv(metrics, None, merge_mode=False)
            logger.info("Successfully processed %d pull requests", len(metrics))
            return 0

        except GitHubAPIError as e:
            logger.error("GitHub API error: %s", e)
            return 1
        except Exception as e:
            logger.error("Unexpected error: %s", e, exc_info=args.debug)
            return 1


# ============================================================================
# Utility Functions
# ============================================================================




def setup_logging(debug: bool = False) -> None:
    """Configure logging based on debug flag."""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        stream=sys.stderr,
    )


def estimate_api_calls_for_prs(pr_count: int) -> int:
    """
    Estimate API calls needed to process pr_count PRs.
    Each PR requires: timeline events, reviews, review comments, issue comments.
    """
    return pr_count * API_CALLS_PER_PR


def get_github_token() -> Optional[str]:
    """Get GitHub token from environment."""
    return os.getenv("GITHUB_TOKEN")


def get_repo_from_git() -> tuple[Optional[str], Optional[str]]:
    """
    Get repository owner and name from git config.
    Returns (owner, repo) or (None, None) if not found.
    """
    try:
        remote_url = subprocess.check_output(
            ["git", "config", "--get", "remote.origin.url"],
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()

        # Parse SSH URL (git@github.com:owner/repo.git)
        if remote_url.startswith("git@"):
            _, path = remote_url.split(":", 1)
        # Parse HTTPS URL (https://github.com/owner/repo.git)
        elif remote_url.startswith("https://"):
            parts = remote_url.split("/")
            if len(parts) >= 5:
                path = f"{parts[3]}/{parts[4]}"
            else:
                return None, None
        else:
            return None, None

        # Remove .git suffix
        if path.endswith(".git"):
            path = path[:-4]

        owner, repo = path.split("/", 1)
        return owner, repo
    except Exception as e:
        logger.debug("Failed to get repo from git: %s", e)
        return None, None


def parse_timestamp(timestamp_str: str) -> datetime:
    """
    Parse various timestamp formats to datetime (always returns timezone-aware).

    Raises ValueError with clear message if timestamp cannot be parsed.
    """
    if not timestamp_str or not timestamp_str.strip():
        raise ValueError("Timestamp string is empty or None")

    try:
        # Try Unix timestamp first
        dt = datetime.fromtimestamp(float(timestamp_str))
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, OverflowError):
        try:
            # Try ISO 8601 / RFC 3339
            dt = date_parser.parse(timestamp_str)
            # Make timezone-aware if naive
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError) as e:
            raise ValueError(f"Invalid timestamp format: {timestamp_str}") from e


def expand_output_pattern(pattern: str, owner: str, repo: str) -> str:
    """
    Expand output pattern with owner and repo placeholders.
    Supports {owner} and {repo} placeholders.
    """
    return pattern.replace("{owner}", owner).replace("{repo}", repo)


if __name__ == "__main__":
    sys.exit(main())
