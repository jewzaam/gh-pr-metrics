#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub Pull Request Metrics Tool

A command-line tool to generate CSV reports containing key metrics for all pull
requests in a GitHub repository within a specified time range.
"""

import argparse
import csv
import json
import logging
import os
import subprocess
import sys
import tempfile
import threading
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

try:
    import argcomplete

    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False

from dateutil import parser as date_parser

from github_api import (
    GitHubAPIError,
    GitHubClient,
    QuotaManager,
    API_CALLS_PER_PR,
)


# Version
__version__ = "0.1.0"

# Constants
DEFAULT_DAYS_BACK = 365
DEFAULT_WORKERS = 4
DEFAULT_RAW_DATA_DIR = "data/raw"
STATE_FILE = Path.home() / ".gh-pr-metrics-state.yaml"
PR_NUMBER_FIELD = "pr_number"
MAX_CHUNK_DAYS = 30  # Maximum days to process in a single chunk


# ============================================================================
# Configuration Management
# ============================================================================


class ConditionalBotConfig:
    """Configuration for conditional AI bot detection."""

    def __init__(self, name: str, content_patterns: List[str], match_any: bool = True):
        self.name = name
        self.content_patterns = content_patterns
        self.match_any = match_any

    def matches_content(self, comment_body: str) -> bool:
        """Check if comment body matches AI patterns using regex."""
        import re

        if not comment_body:
            return False

        if self.match_any:
            # Any pattern match = AI
            return any(re.search(pattern, comment_body) for pattern in self.content_patterns)
        else:
            # All patterns required = AI
            return all(re.search(pattern, comment_body) for pattern in self.content_patterns)


class Config:
    """Application configuration loaded from YAML file."""

    def __init__(self, config_dict: Dict[str, Any]):
        """Initialize config from parsed YAML dict."""
        # AI bot detection
        ai_bots = config_dict.get("ai_bots", {})
        self.always_ai_bots: List[str] = ai_bots.get("always", [])
        self.conditional_bots: List[ConditionalBotConfig] = []

        for bot_config in ai_bots.get("conditional", []):
            self.conditional_bots.append(
                ConditionalBotConfig(
                    name=bot_config["name"],
                    content_patterns=bot_config.get("content_patterns", []),
                    match_any=bot_config.get("match_any", True),
                )
            )

        # Processing settings
        self.workers: int = config_dict.get("workers", DEFAULT_WORKERS)
        self.output_pattern: Optional[str] = config_dict.get("output_pattern")
        self.log_file: str = config_dict.get("log_file", "gh-pr-metrics.log")
        self.default_days_back: int = config_dict.get("default_days_back", DEFAULT_DAYS_BACK)
        self.raw_data_dir: str = config_dict.get("raw_data_dir", DEFAULT_RAW_DATA_DIR)

        # Quota settings
        quota = config_dict.get("quota", {})
        self.quota_reserve: int = quota.get("reserve", 100)
        self.quota_min_buffer: int = quota.get("min_buffer", 50)

    def is_ai_bot(self, login: str, comment_body: str = "") -> bool:
        """
        Determine if a login/comment is from an AI bot.

        Args:
            login: The GitHub login name
            comment_body: The comment text (optional, for conditional detection)

        Returns:
            True if identified as AI bot, False otherwise
        """
        # Check always-AI bots with regex
        import re

        for pattern in self.always_ai_bots:
            if re.match(pattern, login):
                return True

        # Check conditional bots with regex
        for bot_config in self.conditional_bots:
            if re.match(bot_config.name, login):
                return bot_config.matches_content(comment_body)

        return False


def load_config(config_path: str) -> Config:
    """
    Load and validate configuration from YAML file.

    Args:
        config_path: Path to configuration file

    Returns:
        Validated Config object (uses defaults if file missing or empty)

    Raises:
        ValueError: If config YAML is invalid or output_pattern lacks placeholders
    """
    config_file = Path(config_path)

    # Missing config file = use defaults
    if not config_file.exists():
        return Config({})

    try:
        with open(config_file, "r", encoding="utf-8") as f:
            config_dict = yaml.safe_load(f)

        # Empty config file = use defaults
        if config_dict is None:
            return Config({})

        # Validate output_pattern has placeholders if set
        output_pattern = config_dict.get("output_pattern")
        if output_pattern and "{owner}" not in output_pattern and "{repo}" not in output_pattern:
            raise ValueError(
                f"Invalid output_pattern '{output_pattern}': "
                "must contain {{owner}} and/or {{repo}} placeholders"
            )

        return Config(config_dict)

    except yaml.YAMLError as e:
        raise ValueError(f"Invalid YAML in configuration file: {e}")
    except KeyError as e:
        raise ValueError(f"Missing required configuration key: {e}")


class StateManager:
    """
    Manages state file for tracking repository processing progress.

    State file stores last update timestamp and CSV file path per repository.
    """

    def __init__(self, state_file_path: Path = None, logger=None):
        """Initialize state manager with path to state file and logger."""
        self._state_file = state_file_path or STATE_FILE
        self._logger = logger

    def load(self) -> Dict[str, Any]:
        """Load state file containing last update dates per repo."""
        if not self._state_file.exists():
            return {}

        try:
            with open(self._state_file, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}
                return data
        except Exception as e:
            if self._logger:
                self._logger.warning("Failed to load state file %s: %s", self._state_file, e)
            return {}

    def save(self, state: Dict[str, Any]) -> None:
        """Save state file with last update dates per repo."""
        try:
            with open(self._state_file, "w", encoding="utf-8") as f:
                yaml.safe_dump(state, f, default_flow_style=False, sort_keys=True)

            if self._logger:
                self._logger.debug("Saved state file to %s", self._state_file)
        except Exception as e:
            if self._logger:
                self._logger.error("Failed to save state file %s: %s", self._state_file, e)
            raise

    def get_repo_remote_url(self, owner: str, repo: str) -> str:
        """
        Get repository remote URL key for state tracking.
        Uses format: https://github.com/owner/repo
        """
        return f"https://github.com/{owner}/{repo}"

    def get_repo_state(self, owner: str, repo: str) -> Optional[Dict[str, Any]]:
        """
        Get state for a repository (timestamp and csv_file).
        Returns dict with 'timestamp' and 'csv_file' keys, or None if not found.
        Raises ValueError if state is malformed.
        """
        state = self.load()
        repo_key = self.get_repo_remote_url(owner, repo)

        if repo_key not in state:
            return None

        entry = state[repo_key]

        if not isinstance(entry, dict):
            raise ValueError(
                f"Invalid state format for {repo_key} in state file. "
                f"Expected dict with 'timestamp' and 'csv_file', got {type(entry).__name__}"
            )

        timestamp_str = entry.get("timestamp", "")
        csv_file = entry.get("csv_file")

        if not timestamp_str:
            raise ValueError(
                f"Missing or empty 'timestamp' field for {repo_key} in state file. "
                f"State file location: {self._state_file}"
            )

        if not csv_file:
            raise ValueError(
                f"Missing or empty 'csv_file' field for {repo_key} in state file. "
                f"State file location: {self._state_file}"
            )

        try:
            timestamp = parse_timestamp(timestamp_str)
        except ValueError as e:
            raise ValueError(
                f"Invalid timestamp for {repo_key} in state file: {e}. "
                f"State file location: {self._state_file}"
            ) from e

        return {"timestamp": timestamp, "csv_file": csv_file}

    def update_repo(self, owner: str, repo: str, timestamp: datetime, csv_file: str) -> None:
        """Update state file with new last update date and csv file for a repository."""
        state = self.load()
        repo_key = self.get_repo_remote_url(owner, repo)
        # Store as UTC timestamp without timezone component
        utc_timestamp = timestamp.astimezone(timezone.utc).replace(tzinfo=None)
        state[repo_key] = {"timestamp": utc_timestamp.isoformat(), "csv_file": csv_file}
        self.save(state)

    def get_all_tracked_repos(self) -> List[Dict[str, Any]]:
        """
        Get all tracked repositories from state file.
        Returns list of dicts with 'url', 'owner', 'repo', 'timestamp', 'csv_file'.
        """
        state = self.load()
        repos = []

        for repo_url, entry in state.items():
            # Parse URL to get owner/repo
            # Format: https://github.com/owner/repo
            try:
                parts = repo_url.rstrip("/").split("/")
                if len(parts) >= 2:
                    owner = parts[-2]
                    repo = parts[-1]

                    if isinstance(entry, dict):
                        timestamp = parse_timestamp(entry.get("timestamp", ""))
                        csv_file = entry.get("csv_file")
                    else:
                        if self._logger:
                            self._logger.warning("Skipping %s: invalid state format", repo_url)
                        continue

                    repos.append(
                        {
                            "url": repo_url,
                            "owner": owner,
                            "repo": repo,
                            "timestamp": timestamp,
                            "csv_file": csv_file,
                        }
                    )
            except Exception as e:
                if self._logger:
                    self._logger.warning("Failed to parse state entry for %s: %s", repo_url, e)
                continue

        return repos


class CSVManager:
    """
    Manages CSV I/O operations for PR metrics.

    Handles reading existing CSV files, writing metrics data,
    and managing field definitions. Thread-safe for concurrent access.
    """

    def __init__(self, logger=None):
        """Initialize CSV manager with logger."""
        self._logger = logger
        self._lock = threading.Lock()

    def get_fieldnames(self) -> List[str]:
        """Get CSV field names in order."""
        return [
            PR_NUMBER_FIELD,
            "title",
            "author",
            "created_at",
            "ready_for_review_at",
            "merged_at",
            "closed_at",
            "days_open",
            "days_in_review",
            "total_comment_count",
            "non_ai_bot_comment_count",
            "ai_bot_comment_count",
            "non_ai_bot_login_names",
            "ai_bot_login_names",
            "changes_requested_count",
            "unique_change_requesters",
            "approval_count",
            "status",
            "url",
            "errors",
            "lines_added",
            "lines_deleted",
            "files_changed",
            "total_line_changes",
        ]

    def read_csv(self, csv_file: str) -> Dict[int, Dict[str, Any]]:
        """
        Read existing CSV file and return dict keyed by PR number (thread-safe).
        Returns empty dict if file doesn't exist or can't be read.
        """
        with self._lock:
            if not os.path.exists(csv_file):
                return {}

            try:
                existing_data = {}
                with open(csv_file, "r", newline="", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        # Key by PR number for O(1) lookups
                        pr_number = int(row[PR_NUMBER_FIELD])
                        existing_data[pr_number] = row

                if self._logger:
                    self._logger.debug(
                        "Loaded %d existing PRs from %s", len(existing_data), csv_file
                    )
                return existing_data
            except Exception as e:
                if self._logger:
                    self._logger.warning("Failed to read existing CSV %s: %s", csv_file, e)
                return {}

    def write_csv(
        self,
        metrics: List[Dict[str, Any]],
        output_file: Optional[str],
        merge_mode: bool = False,
        force_write: bool = False,
    ) -> None:
        """
        Write metrics to CSV file or stdout (thread-safe).

        If merge_mode is True and output_file exists, existing PR data is loaded,
        updated with new metrics, and written back.

        If force_write is True, write headers even if metrics list is empty.
        """
        with self._lock:
            if not metrics and not force_write:
                if self._logger:
                    self._logger.warning("No pull requests to output")
                return

            fieldnames = self.get_fieldnames()

            # Merge with existing data if requested
            if merge_mode and output_file:
                # Re-read inside lock to get latest data
                if not os.path.exists(output_file):
                    existing_data = {}
                else:
                    try:
                        existing_data = {}
                        with open(output_file, "r", newline="", encoding="utf-8") as f:
                            reader = csv.DictReader(f)
                            for row in reader:
                                pr_number = int(row[PR_NUMBER_FIELD])
                                existing_data[pr_number] = row
                    except Exception:
                        existing_data = {}

                # Update existing data with new metrics
                for metric in metrics:
                    pr_number = metric[PR_NUMBER_FIELD]
                    existing_data[pr_number] = metric

                # Convert back to list
                all_metrics = list(existing_data.values())
                if self._logger:
                    self._logger.debug(
                        "Merged data: %d total PRs (%d new/updated)", len(all_metrics), len(metrics)
                    )
            else:
                all_metrics = metrics

            if output_file:
                # Write to temp file first, then atomically rename
                output_path = Path(output_file)
                temp_fd, temp_path = tempfile.mkstemp(
                    dir=(
                        output_path.parent if output_path.parent.exists() else tempfile.gettempdir()
                    ),
                    prefix=".tmp_pr_metrics_",
                    suffix=".csv",
                )

                try:
                    with os.fdopen(temp_fd, "w", newline="", encoding="utf-8") as f:
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        writer.writerows(all_metrics)

                    # Atomic rename
                    os.rename(temp_path, output_file)
                    if self._logger:
                        self._logger.debug("Wrote %d PRs to %s", len(all_metrics), output_file)
                except Exception as e:
                    # Keep temp file for debugging
                    if self._logger:
                        self._logger.error(
                            "Failed to write CSV output. Temp file retained at: %s", temp_path
                        )
                    raise e
            else:
                writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_metrics)


def build_pr_json_data(pr: Dict[str, Any], owner: str, repo: str) -> Dict[str, Any]:
    """
    Build complete PR JSON data including all comments and reviews.

    Args:
        pr: Base PR object from GitHub API
        owner: Repository owner
        repo: Repository name

    Returns:
        Complete PR data dictionary ready for JSON serialization
    """
    pr_number = pr["number"]

    # Fetch issue comments
    issue_comments = []
    try:
        comments = github_client.fetch_issue_comments(pr["comments_url"])
        for comment in comments:
            issue_comments.append(
                {
                    "id": comment.get("id"),
                    "user": comment.get("user", {}).get("login"),
                    "user_type": comment.get("user", {}).get("type"),
                    "body": comment.get("body", ""),
                    "created_at": comment.get("created_at"),
                    "updated_at": comment.get("updated_at"),
                }
            )
    except GitHubAPIError as e:
        logger.debug("Failed to fetch issue comments for PR #%d: %s", pr_number, e)

    # Fetch review comments
    review_comments = []
    try:
        comments = github_client.fetch_review_comments(pr["review_comments_url"])
        for comment in comments:
            review_comments.append(
                {
                    "id": comment.get("id"),
                    "user": comment.get("user", {}).get("login"),
                    "user_type": comment.get("user", {}).get("type"),
                    "body": comment.get("body", ""),
                    "path": comment.get("path"),
                    "line": comment.get("line"),
                    "created_at": comment.get("created_at"),
                    "updated_at": comment.get("updated_at"),
                }
            )
    except GitHubAPIError as e:
        logger.debug("Failed to fetch review comments for PR #%d: %s", pr_number, e)

    # Fetch reviews
    reviews = []
    try:
        review_list = github_client.fetch_reviews(owner, repo, pr_number)
        for review in review_list:
            reviews.append(
                {
                    "id": review.get("id"),
                    "user": review.get("user", {}).get("login"),
                    "user_type": review.get("user", {}).get("type"),
                    "state": review.get("state"),
                    "body": review.get("body", ""),
                    "submitted_at": review.get("submitted_at"),
                }
            )
    except GitHubAPIError as e:
        logger.debug("Failed to fetch reviews for PR #%d: %s", pr_number, e)

    # Build complete PR data
    return {
        "pr_number": pr_number,
        "title": pr["title"],
        "author": pr.get("user", {}).get("login"),
        "author_type": pr.get("user", {}).get("type"),
        "state": pr["state"],
        "created_at": pr["created_at"],
        "updated_at": pr["updated_at"],
        "merged_at": pr.get("merged_at"),
        "closed_at": pr.get("closed_at"),
        "draft": pr.get("draft", False),
        "url": pr["html_url"],
        "additions": pr.get("additions", 0),
        "deletions": pr.get("deletions", 0),
        "changed_files": pr.get("changed_files", 0),
        "issue_comments": issue_comments,
        "review_comments": review_comments,
        "reviews": reviews,
    }


def write_pr_json(raw_data_dir: str, owner: str, repo: str, pr_data: Dict[str, Any]) -> None:
    """
    Write PR data to JSON file in provider-aware directory structure.

    Args:
        raw_data_dir: Base directory for raw data (e.g., "data/raw")
        owner: Repository owner
        repo: Repository name
        pr_data: Complete PR data dictionary
    """
    # Provider-aware path: raw_data_dir/github.com/owner/repo/
    output_dir = Path(raw_data_dir) / "github.com" / owner / repo
    output_dir.mkdir(parents=True, exist_ok=True)

    pr_number = pr_data["pr_number"]
    json_file = output_dir / f"{pr_number}.json"

    try:
        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(pr_data, f, indent=2, ensure_ascii=False)
        logger.debug("Wrote PR #%d JSON to %s", pr_number, json_file)
    except Exception as e:
        logger.warning("Failed to write PR #%d JSON: %s", pr_number, e)


class LoggingManager:
    """
    Manages application logging with quota-aware prefixes.

    Wraps standard logging to inject API quota status into all log messages.
    """

    def __init__(self, quota_manager: QuotaManager):
        """Initialize logging manager with reference to quota manager."""
        self._quota_manager = quota_manager

    def info(self, msg: str, *args, **kwargs) -> None:
        """Log INFO with API quota prefix."""
        logging.info(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def warning(self, msg: str, *args, **kwargs) -> None:
        """Log WARNING with API quota prefix."""
        logging.warning(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def error(self, msg: str, *args, **kwargs) -> None:
        """Log ERROR with API quota prefix."""
        logging.error(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)

    def debug(self, msg: str, *args, **kwargs) -> None:
        """Log DEBUG with API quota prefix."""
        logging.debug(f"{self._quota_manager.get_quota_prefix()} {msg}", *args, **kwargs)


# ============================================================================
# Global Manager Instances
# ============================================================================

quota_manager = QuotaManager()
logger = LoggingManager(quota_manager)
state_manager = StateManager(logger=logger)
github_client = None  # Initialized in main() with token, quota_manager, and logger
csv_manager = CSVManager(logger=logger)


# ============================================================================
# PR Processing Functions
# ============================================================================


def get_ready_for_review_time(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> str:
    """Get the timestamp when PR was last marked ready for review."""
    # If never a draft, use created_at
    if not pr.get("draft", False) and pr.get("created_at"):
        # Check events to see if it was ever a draft
        try:
            events = github_client.fetch_timeline_events(owner, repo, pr["number"])
            ready_events = [e for e in events if e.get("event") == "ready_for_review"]
            if ready_events:
                # Use the latest ready_for_review event
                return ready_events[-1]["created_at"]
        except GitHubAPIError as e:
            logger.debug("Failed to fetch events for PR #%d: %s", pr["number"], e)

    return pr["created_at"]


def count_comments_from_json(
    pr_json_data: Dict[str, Any],
    config: Config,
) -> tuple[int, int, int, str, str]:
    """
    Count comments from pre-fetched PR JSON data.
    Returns (total_comments, non_ai_bot_comments, ai_bot_comments, non_ai_bot_names, ai_bot_names).
    """
    total_comments = 0
    non_ai_bot_comments = 0
    ai_bot_comments = 0
    non_ai_bot_logins = set()
    ai_bot_logins = set()

    # Count issue comments
    for comment in pr_json_data.get("issue_comments", []):
        if comment.get("user_type") == "Bot":
            total_comments += 1
            login = comment.get("user", "")
            body = comment.get("body", "")
            if config.is_ai_bot(login, body):
                ai_bot_logins.add(login)
                ai_bot_comments += 1
            else:
                non_ai_bot_logins.add(login)
                non_ai_bot_comments += 1

    # Count review comments
    for comment in pr_json_data.get("review_comments", []):
        if comment.get("user_type") == "Bot":
            total_comments += 1
            login = comment.get("user", "")
            body = comment.get("body", "")
            if config.is_ai_bot(login, body):
                ai_bot_logins.add(login)
                ai_bot_comments += 1
            else:
                non_ai_bot_logins.add(login)
                non_ai_bot_comments += 1

    # Count review bodies
    for review in pr_json_data.get("reviews", []):
        body = review.get("body", "")
        if body and review.get("user_type") == "Bot":
            total_comments += 1
            login = review.get("user", "")
            if config.is_ai_bot(login, body):
                ai_bot_logins.add(login)
                ai_bot_comments += 1
            else:
                non_ai_bot_logins.add(login)
                non_ai_bot_comments += 1

    return (
        total_comments,
        non_ai_bot_comments,
        ai_bot_comments,
        ",".join(sorted(non_ai_bot_logins)),
        ",".join(sorted(ai_bot_logins)),
    )


def count_comments(
    pr: Dict[str, Any],
    owner: str,
    repo: str,
    token: Optional[str],
    config: Config,
    reviews: List[Dict[str, Any]] = None,
) -> tuple[int, int, int, str, str]:
    """
    Count total comments, non-AI bot comments, and AI bot comments.
    Returns (total_comments, non_ai_bot_comments, ai_bot_comments,
             non_ai_bot_login_names, ai_bot_login_names).
    Non-AI bot and AI bot login names are comma-separated strings.
    Uses config for AI bot detection (both always and conditional patterns).

    Args:
        reviews: Optional pre-fetched reviews list to avoid duplicate API calls
    """
    total_comments = 0
    non_ai_bot_comments = 0
    ai_bot_comments = 0
    non_ai_bot_logins = set()
    ai_bot_logins = set()

    # Issue comments (general PR comments)
    comments_url = pr["comments_url"]
    try:
        comments = github_client.fetch_issue_comments(comments_url)
        for comment in comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                comment_body = comment.get("body", "")

                is_ai = config.is_ai_bot(login, comment_body)
                logger.debug(
                    "PR #%d issue comment: bot=%s, is_ai=%s, body_preview=%s",
                    pr["number"],
                    login,
                    is_ai,
                    comment_body[:100] if comment_body else "(empty)",
                )

                if is_ai:
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch comments for PR #%d: %s", pr["number"], e)

    # Review comments (inline code comments)
    review_comments_url = pr["review_comments_url"]
    try:
        review_comments = github_client.fetch_review_comments(review_comments_url)
        for comment in review_comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                comment_body = comment.get("body", "")

                is_ai = config.is_ai_bot(login, comment_body)
                logger.debug(
                    "PR #%d review comment: bot=%s, is_ai=%s, body_preview=%s",
                    pr["number"],
                    login,
                    is_ai,
                    comment_body[:100] if comment_body else "(empty)",
                )

                if is_ai:
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logger.debug("Failed to fetch review comments for PR #%d: %s", pr["number"], e)

    # Review bodies (summary text in reviews)
    # Use pre-fetched reviews if provided, otherwise fetch
    if reviews is None:
        try:
            reviews = github_client.fetch_reviews(owner, repo, pr["number"])
        except GitHubAPIError as e:
            logger.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
            reviews = []

    for review in reviews:
        # Only count reviews with body text as comments
        review_body = review.get("body", "")
        if review_body and review.get("user", {}).get("type") == "Bot":
            total_comments += 1
            login = review["user"]["login"]

            is_ai = config.is_ai_bot(login, review_body)
            logger.debug(
                "PR #%d review body: bot=%s, is_ai=%s, body_preview=%s",
                pr["number"],
                login,
                is_ai,
                review_body[:100] if review_body else "(empty)",
            )

            if is_ai:
                ai_bot_logins.add(login)
                ai_bot_comments += 1
            else:
                non_ai_bot_logins.add(login)
                non_ai_bot_comments += 1

    # Convert to comma-separated strings, sorted for consistency
    non_ai_bot_login_names = ",".join(sorted(non_ai_bot_logins))
    ai_bot_login_names = ",".join(sorted(ai_bot_logins))

    return (
        total_comments,
        non_ai_bot_comments,
        ai_bot_comments,
        non_ai_bot_login_names,
        ai_bot_login_names,
    )


def get_review_metrics_from_json(reviews: List[Dict[str, Any]]) -> tuple[int, int, int]:
    """
    Get review metrics from pre-fetched reviews.
    Returns (changes_requested_count, unique_change_requesters, approval_count).
    """
    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r.get("user", "")
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review.get("user")
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def get_review_metrics(
    pr: Dict[str, Any],
    owner: str,
    repo: str,
    token: Optional[str],
    reviews: List[Dict[str, Any]] = None,
) -> tuple[int, int, int]:
    """
    Get review metrics: changes requested count, unique change requesters, approvals.
    Returns (changes_requested_count, unique_change_requesters, approval_count).

    Args:
        reviews: Optional pre-fetched reviews list to avoid duplicate API calls
    """
    # Use pre-fetched reviews if provided, otherwise fetch
    if reviews is None:
        try:
            reviews = github_client.fetch_reviews(owner, repo, pr["number"])
        except GitHubAPIError as e:
            logger.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
            return 0, 0, 0

    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r["user"]["login"]
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review["user"]["login"]
            # Later reviews override earlier ones
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def determine_pr_status(pr: Dict[str, Any]) -> str:
    """Determine the current status of the PR."""
    if pr.get("draft"):
        return "draft"
    elif pr.get("merged_at"):
        return "merged"
    elif pr.get("state") == "closed":
        return "closed"
    else:
        return "open"


def process_pr(
    pr: Dict[str, Any],
    pr_json_data: Dict[str, Any],
    owner: str,
    repo: str,
    token: Optional[str],
    config: Config,
) -> Dict[str, Any]:
    """
    Process a single PR and extract metrics from fetched JSON data.

    Args:
        pr: Base PR object
        pr_json_data: Complete PR data from build_pr_json_data (includes comments, reviews)
        owner: Repository owner
        repo: Repository name
        token: GitHub token
        config: Configuration object

    Returns:
        Metrics dictionary for CSV
    """
    pr_number = pr["number"]
    logger.debug("Processing PR #%d: %s", pr_number, pr["title"])

    errors = []
    metrics: Dict[str, Any] = {
        PR_NUMBER_FIELD: pr_number,
        "title": pr["title"],
        "author": pr["user"]["login"] if pr.get("user") else "",
        "created_at": pr["created_at"],
        "url": pr["html_url"],
    }

    # Ready for review time
    try:
        metrics["ready_for_review_at"] = get_ready_for_review_time(pr, owner, repo, token)
    except Exception as e:
        logger.debug("Error getting ready time for PR #%d: %s", pr_number, e)
        metrics["ready_for_review_at"] = pr["created_at"]
        errors.append(f"ready_time: {e}")

    # Use reviews from pr_json_data
    reviews = pr_json_data.get("reviews", [])

    # Comment counts (including review bodies) - use data from pr_json_data
    try:
        total_comments, non_ai_bot_comments, ai_bot_comments, non_ai_bot_names, ai_bot_names = (
            count_comments_from_json(pr_json_data, config)
        )
        metrics["total_comment_count"] = total_comments
        metrics["non_ai_bot_comment_count"] = non_ai_bot_comments
        metrics["ai_bot_comment_count"] = ai_bot_comments
        metrics["non_ai_bot_login_names"] = non_ai_bot_names
        metrics["ai_bot_login_names"] = ai_bot_names
    except Exception as e:
        logger.debug("Error counting comments for PR #%d: %s", pr_number, e)
        metrics["total_comment_count"] = 0
        metrics["non_ai_bot_comment_count"] = 0
        metrics["ai_bot_comment_count"] = 0
        metrics["non_ai_bot_login_names"] = ""
        metrics["ai_bot_login_names"] = ""
        errors.append(f"comments: {e}")

    # Review metrics - use reviews from pr_json_data
    try:
        changes_requested, unique_requesters, approvals = get_review_metrics_from_json(reviews)
        metrics["changes_requested_count"] = changes_requested
        metrics["unique_change_requesters"] = unique_requesters
        metrics["approval_count"] = approvals
    except Exception as e:
        logger.debug("Error getting reviews for PR #%d: %s", pr_number, e)
        metrics["changes_requested_count"] = 0
        metrics["unique_change_requesters"] = 0
        metrics["approval_count"] = 0
        errors.append(f"reviews: {e}")

    # Status
    metrics["status"] = determine_pr_status(pr)

    # Merged and closed timestamps
    metrics["merged_at"] = pr.get("merged_at") or ""
    metrics["closed_at"] = pr.get("closed_at") or ""

    # Calculate derived time metrics
    try:
        created_at = date_parser.parse(pr["created_at"])
        ready_at = date_parser.parse(metrics["ready_for_review_at"])

        # Determine end time (merged, closed, or now)
        if metrics["merged_at"]:
            end_time = date_parser.parse(metrics["merged_at"])
        elif metrics["closed_at"]:
            end_time = date_parser.parse(metrics["closed_at"])
        else:
            end_time = datetime.now(timezone.utc)

        # days_open: created → end
        days_open = (end_time - created_at).total_seconds() / 86400
        metrics["days_open"] = round(days_open, 2)

        # days_in_review: ready_for_review → end
        days_in_review = (end_time - ready_at).total_seconds() / 86400
        metrics["days_in_review"] = round(days_in_review, 2)

    except Exception as e:
        logger.debug("Error calculating time metrics for PR #%d: %s", pr_number, e)
        metrics["days_open"] = ""
        metrics["days_in_review"] = ""
        errors.append(f"time_metrics: {e}")

    # Complexity metrics
    metrics["lines_added"] = pr.get("additions", 0)
    metrics["lines_deleted"] = pr.get("deletions", 0)
    metrics["files_changed"] = pr.get("changed_files", 0)
    metrics["total_line_changes"] = metrics["lines_added"] + metrics["lines_deleted"]

    # Errors
    metrics["errors"] = "; ".join(errors) if errors else ""

    return metrics


# ============================================================================
# Main Business Logic
# ============================================================================


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate CSV reports of GitHub pull request metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze PRs from current repository (last year)
  gh-pr-metrics

  # Specify repository explicitly
  gh-pr-metrics --owner microsoft --repo vscode

  # Custom time range
  gh-pr-metrics --start 2024-01-01 --end 2024-12-31

  # Output to file (stores path in state for future updates)
  gh-pr-metrics --output metrics.csv

  # Initialize repositories (validates access, creates state entries)
  gh-pr-metrics --init --owner ansible --output "data/{owner}-{repo}.csv" --start "2024-11-01"

  # Update specific repo (uses stored CSV path from state)
  gh-pr-metrics --owner microsoft --repo vscode --update

  # Update all tracked repositories
  gh-pr-metrics --update-all

Environment Variables:
  GITHUB_TOKEN    GitHub personal access token for authentication
        """,
    )

    parser.add_argument("--owner", help="Repository owner (default: auto-detect from git)")
    parser.add_argument("--repo", help="Repository name (default: auto-detect from git)")
    parser.add_argument(
        "--pr",
        type=int,
        help="Update a specific PR number only (surgically update existing CSV)",
    )
    parser.add_argument(
        "--start",
        help="Start timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: 365 days ago",
    )
    parser.add_argument(
        "--end",
        help="End timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: now",
    )
    parser.add_argument("--output", "-o", help="Output file path (default: stdout)")
    parser.add_argument(
        "--init",
        action="store_true",
        help="Initialize repositories in state file. Validates access and stores --start date. "
        "Requires --owner and --output. If only --owner provided, initializes all repos. "
        "Output supports patterns like 'data/{owner}-{repo}.csv'",
    )
    parser.add_argument(
        "--update",
        action="store_true",
        help="Update mode: fetch only PRs since last update and merge with existing CSV. "
        "Requires --owner and --repo. Uses CSV path stored in state file. "
        "Cannot be used with --output (uses stored path).",
    )
    parser.add_argument(
        "--update-all",
        action="store_true",
        help="Update all tracked repositories. Cannot be used with --owner/--repo or --output.",
    )
    parser.add_argument(
        "--wait",
        action="store_true",
        help="Wait for rate limit to reset if quota exhausted, then continue processing.",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument(
        "--workers",
        type=int,
        default=None,
        help="Number of parallel workers for processing PRs (default: from config file)",
    )
    parser.add_argument(
        "--config",
        default=".gh-pr-metrics.yaml",
        help="Configuration file path (default: .gh-pr-metrics.yaml)",
    )
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    return parser.parse_args()


def show_update_all_progress_summary(
    tracked_repos: List[Dict[str, Any]], completed_repos: int, stopped_at: Optional[str] = None
) -> None:
    """
    Show progress summary for update-all operation.

    Args:
        tracked_repos: List of all tracked repositories
        completed_repos: Number of repos completed in this pass
        stopped_at: Optional repo name where processing stopped (if partial)
    """
    logger.info("=" * 80)
    logger.info("Progress Summary:")

    if stopped_at:
        logger.info("  Completed this pass: %d repos", completed_repos)
        logger.info("  Stopped at (partial): %s", stopped_at)
    else:
        logger.info("  Completed: %d repos (not started yet)", completed_repos)

    # Analyze state file to show what's current
    now = datetime.now(timezone.utc)
    cutoff = now - timedelta(hours=12)

    recently_updated = 0
    needs_processing = 0

    for repo_info in tracked_repos:
        if repo_info["timestamp"] > cutoff:
            recently_updated += 1
        else:
            needs_processing += 1

    logger.info("  Recently updated (< 12 hours): %d repos", recently_updated)
    logger.info("  Needs processing (> 12 hours): %d repos", needs_processing)


def fetch_and_process_pr(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], config: Config
) -> Dict[str, Any]:
    """
    Fetch complete PR data, process metrics, and write JSON.

    Returns metrics dict for CSV.
    """
    pr_json_data = build_pr_json_data(pr, owner, repo)
    pr_metrics = process_pr(pr, pr_json_data, owner, repo, token, config)
    write_pr_json(config.raw_data_dir, owner, repo, pr_json_data)
    return pr_metrics


def update_single_pr(
    owner: str,
    repo: str,
    pr_number: int,
    output_file: str,
    token: Optional[str],
    config: Config,
) -> int:
    """
    Update a single PR in an existing CSV file.

    Returns 0 on success, 1 on error.
    """
    logger.info("Fetching PR #%d from %s/%s", pr_number, owner, repo)

    try:
        pr = github_client.fetch_single_pr(owner, repo, pr_number)
    except GitHubAPIError as e:
        logger.error("Failed to fetch PR #%d: %s", pr_number, e)
        return 1

    logger.info("Processing PR #%d", pr_number)
    try:
        pr_metrics = fetch_and_process_pr(pr, owner, repo, token, config)
    except Exception as e:
        logger.error("Failed to process PR #%d: %s", pr_number, e)
        return 1

    # Read existing CSV
    if not Path(output_file).exists():
        logger.error("CSV file not found: %s", output_file)
        logger.error("Use regular mode first to create the CSV file")
        return 1

    # Read existing CSV as dict keyed by PR number
    logger.info("Reading existing CSV: %s", output_file)
    existing_data = csv_manager.read_csv(output_file)

    # Update the PR in the dictionary
    if pr_number in existing_data:
        logger.info("Updating existing PR #%d in CSV", pr_number)
    else:
        logger.info("Adding new PR #%d to CSV", pr_number)

    existing_data[pr_number] = pr_metrics

    # Convert dict to list and write back to CSV
    all_metrics = list(existing_data.values())
    logger.info("Writing updated CSV: %s", output_file)
    csv_manager.write_csv(all_metrics, output_file, merge_mode=False)

    logger.info("Successfully updated PR #%d", pr_number)
    return 0


def process_repository(
    owner: str,
    repo: str,
    output_file: str,
    start_date: datetime,
    end_date: datetime,
    token: Optional[str],
    workers: int,
    config: Config,
    merge_mode: bool = False,
) -> tuple[int, int, int]:
    """
    Process a single repository and generate/update metrics CSV using page-based processing.

    Fetches PRs one page at a time (sorted by updated date), processes each page,
    and checks rate limit before fetching the next page. Saves progress after each page.

    Returns (exit_code, chunks_completed, total_chunks_fetched).
    exit_code: 0 on success, 1 on error/stopped
    chunks_completed: Number of chunks successfully processed
    total_chunks_fetched: Total chunks fetched (may be more than completed if stopped mid-chunk)
    """
    repo_ctx = f"{owner}/{repo}"

    logger.info("[%s] Starting processing", repo_ctx)
    logger.info("[%s] Date range: %s to %s", repo_ctx, start_date.isoformat(), end_date.isoformat())

    # Check quota before starting any work
    max_prs_can_process = quota_manager.calculate_max_prs()
    if max_prs_can_process == 0:
        remaining, limit, _ = quota_manager.get_current_quota()
        logger.error(
            "[%s] Cannot process any PRs with current quota (exhausted or below reserve)",
            repo_ctx,
        )
        logger.error(
            "[%s] Current quota: %d/%d remaining",
            repo_ctx,
            remaining,
            limit,
        )
        # Note: per-repo wait not implemented, only at update-all level
        return 1, 0, 0

    logger.info(
        "[%s] Quota check: can process up to ~%d PRs before hitting reserve",
        repo_ctx,
        max_prs_can_process,
    )

    logger.info("[%s] Fetching PRs sorted by updated date (oldest first)", repo_ctx)

    # Step 1: Fetch all PRs in date range (internally descending, returns ascending)
    total_chunks_fetched = 0  # Will be calculated from PR count
    chunks_completed = 0  # Initialize early for error handlers

    try:
        all_prs = github_client.fetch_all_prs(owner, repo, start_date, end_date)

        if not all_prs:
            logger.info("[%s] No pull requests found in the specified date range", repo_ctx)
            state_manager.update_repo(owner, repo, end_date, output_file)
            return 0, 1, 0

        logger.info("[%s] Collected %d PRs total", repo_ctx, len(all_prs))
        # Estimate chunks fetched (for return value compatibility)
        total_chunks_fetched = (len(all_prs) + 99) // 100  # Round up

        # Step 2: Process PRs in chunks with incremental saves
        # PRs are already in ascending order (oldest-first) from fetch_all_prs
        chunk_size = 100
        chunk_num = 0
        last_processed_timestamp = start_date

        for chunk_start in range(0, len(all_prs), chunk_size):
            chunk_num += 1
            chunk_end = min(chunk_start + chunk_size, len(all_prs))
            prs_chunk = all_prs[chunk_start:chunk_end]
            total_prs = len(prs_chunk)

            chunk_info_str = f"Chunk {chunk_num}/{total_chunks_fetched}: "

            logger.info("[%s] %sProcessing %d PRs", repo_ctx, chunk_info_str, total_prs)

            # Check rate limit and truncate chunk if needed
            estimated_calls = estimate_api_calls_for_prs(total_prs)
            sufficient, max_prs = quota_manager.check_sufficient(
                estimated_calls, repo_ctx, chunk_info_str, get_github_token, logger
            )

            if not sufficient:
                # Can't process full chunk, but maybe we can process some PRs
                if max_prs > 0:
                    # Truncate to what we can handle
                    logger.warning(
                        "[%s] Chunk %d: Truncating from %d to %d PRs due to quota limit",
                        repo_ctx,
                        chunk_num,
                        total_prs,
                        max_prs,
                    )
                    prs_chunk = prs_chunk[:max_prs]
                    total_prs = len(prs_chunk)
                else:
                    # Can't process any PRs
                    logger.error(
                        "[%s] Stopping at chunk %d due to insufficient rate limit",
                        repo_ctx,
                        chunk_num,
                    )
                    logger.error(
                        "[%s] Progress saved. Resume with --update (or --update-all) "
                        "after rate limit resets.",
                        repo_ctx,
                    )
                    return 1, chunks_completed, total_chunks_fetched

            # Process this chunk of PRs
            metrics = []
            completed = 0

            logger.info(
                "[%s] %sProcessing %d PRs with %d workers",
                repo_ctx,
                chunk_info_str,
                total_prs,
                workers,
            )

            # Process PRs in parallel - each writes to CSV immediately (thread-safe)
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(fetch_and_process_pr, pr, owner, repo, token, config): pr
                    for pr in prs_chunk
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    pr_number = pr["number"]
                    try:
                        pr_metrics = future.result()

                        # Write to CSV immediately (thread-safe)
                        csv_manager.write_csv([pr_metrics], output_file, merge_mode=True)
                        metrics.append(pr_metrics)

                        logger.info(
                            "[%s] %s: Completed PR %d/%d: #%d (updated: %s)",
                            repo_ctx,
                            chunk_info_str,
                            completed,
                            total_prs,
                            pr_number,
                            pr.get("updated_at", "unknown"),
                        )
                    except Exception as e:
                        logger.error(
                            "[%s] %s: Failed to process PR #%d: %s",
                            repo_ctx,
                            chunk_info_str,
                            pr_number,
                            e,
                        )

            # CSV already written incrementally during processing
            # Find the timestamp to set in the state file
            if prs_chunk:
                # Default to the updated_at of the last PR in this chunk
                last_processed_timestamp = date_parser.parse(prs_chunk[-1]["updated_at"])

            # Check if this is the last chunk and we had sufficient quota
            is_last_chunk = chunk_end >= len(all_prs)
            if is_last_chunk and sufficient:
                # Completed all PRs with sufficient quota - set to query time
                last_processed_timestamp = end_date

            # Update state file
            state_manager.update_repo(owner, repo, last_processed_timestamp, output_file)
            chunks_completed += 1
            logger.info(
                "[%s] %s: Processed %d PRs, updated state to %s",
                repo_ctx,
                chunk_info_str,
                len(metrics),
                last_processed_timestamp.isoformat(),
            )

            # If quota insufficient and more chunks remain, stop with error
            if not sufficient and not is_last_chunk:
                # Stopped mid-processing due to quota - return error code
                logger.error(
                    "[%s] Stopped at chunk %d/%d due to quota exhaustion",
                    repo_ctx,
                    chunks_completed,
                    total_chunks_fetched,
                )
                return 1, chunks_completed, total_chunks_fetched

    except GitHubAPIError as e:
        logger.error("[%s] GitHub API error: %s", repo_ctx, e)
        return 1, chunks_completed, total_chunks_fetched
    except Exception as e:
        logger.error("[%s] Unexpected error: %s", repo_ctx, e, exc_info=True)
        return 1, chunks_completed, total_chunks_fetched

    # Successfully completed all chunks
    logger.info("[%s] Successfully completed all %d chunks", repo_ctx, chunks_completed)
    return 0, chunks_completed, total_chunks_fetched


def main() -> int:
    """Main entry point."""
    args = parse_arguments()

    # Load configuration (uses defaults if file missing)
    try:
        config = load_config(args.config)
    except ValueError as e:
        print(f"Error loading configuration: {e}", file=sys.stderr)
        print(f"Configuration file: {args.config}", file=sys.stderr)
        return 1

    setup_logging(args.debug, config.log_file)

    logger.info("GitHub PR Metrics Tool v%s", __version__)
    logger.info("Configuration file: %s", args.config)
    logger.info("Logging to: %s", config.log_file)
    logger.debug("Arguments: %s", args)

    # Validate argument combinations
    if args.init and not args.owner:
        logger.error("--init requires --owner")
        return 1

    if args.init and not args.output:
        logger.error("--init requires --output (supports patterns like 'data/{owner}-{repo}.csv')")
        return 1

    if args.init and args.end:
        logger.error("--init cannot be used with --end (uses current time)")
        return 1

    if args.init and (args.update or args.update_all):
        logger.error("--init cannot be used with --update or --update-all")
        return 1

    if args.update and args.output:
        logger.error("--update and --output cannot be used together (update uses stored CSV path)")
        return 1

    if args.update and (args.start or args.end):
        logger.error(
            "--update cannot be used with --start or --end (uses last update date from state)"
        )
        return 1

    if args.update_all and args.output:
        logger.error("--update-all and --output cannot be used together (uses stored CSV paths)")
        return 1

    if args.update_all and (args.start or args.end):
        logger.error(
            "--update-all cannot be used with --start or --end (uses last update dates from state)"
        )
        return 1

    if args.update_all and (args.owner or args.repo):
        logger.error("--update-all cannot be used with --owner or --repo")
        return 1

    if args.update and args.update_all:
        logger.error("--update and --update-all cannot be used together")
        return 1

    if args.wait and not args.update_all:
        logger.error("--wait can only be used with --update-all")
        return 1

    if args.pr and (args.init or args.update or args.update_all):
        logger.error("--pr cannot be used with --init, --update, or --update-all")
        return 1

    if args.pr and not args.output and not config.output_pattern:
        logger.error("--pr requires --output or config output_pattern to specify the CSV file")
        return 1

    # Get GitHub token and initialize client
    token = get_github_token()
    global github_client
    github_client = GitHubClient(token, quota_manager, logger)

    # Determine workers: CLI override takes precedence, otherwise use config default
    workers = args.workers if args.workers else config.workers

    if not token:
        logger.warning(
            "No GITHUB_TOKEN found. API rate limits will be restrictive "
            "for unauthenticated requests."
        )
        # Force single worker to avoid hitting rate limits too quickly
        if workers > 1:
            logger.warning("Forcing --workers=1 due to missing GITHUB_TOKEN")
            workers = 1

    # Check and display rate limit status
    rate_info = quota_manager.initialize(token)
    if rate_info:
        remaining = rate_info.get("remaining", "unknown")
        limit = rate_info.get("limit", "unknown")
        reset_timestamp = rate_info.get("reset", 0)

        if reset_timestamp:
            reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
            now = datetime.now(timezone.utc)
            time_until_reset = reset_time - now
            minutes_until_reset = time_until_reset.total_seconds() / 60
            reset_str = f"{minutes_until_reset:.1f} minutes"
        else:
            reset_str = "unknown"

        logger.info(
            "Rate limit: %s/%s remaining (resets in %s)",
            remaining,
            limit,
            reset_str,
        )

    # Handle init mode
    if args.init:
        owner = args.owner
        repos_to_init = []

        # If repo specified, just use that one
        if args.repo:
            repos_to_init = [args.repo]
            logger.info("Initializing single repository: %s/%s", owner, args.repo)
        else:
            # List all repos for owner
            logger.info("Listing all repositories for owner: %s", owner)
            try:
                repos_to_init = github_client.list_repos(owner)
                logger.info("Found %d repositories for %s", len(repos_to_init), owner)
            except GitHubAPIError as e:
                logger.error("Failed to list repositories for %s: %s", owner, e)
                return 1

        if not repos_to_init:
            logger.error("No repositories found for owner: %s", owner)
            return 1

        # Determine start date
        if args.start:
            start_date = parse_timestamp(args.start)
        else:
            start_date = datetime.now(timezone.utc) - timedelta(days=DEFAULT_DAYS_BACK)

        logger.info("Using start date: %s", start_date.isoformat())

        # Filter out already-tracked repos to get accurate count
        repos_needing_validation = []
        already_tracked_count = 0
        for repo in repos_to_init:
            if state_manager.get_repo_state(owner, repo):
                already_tracked_count += 1
            else:
                repos_needing_validation.append(repo)

        if already_tracked_count > 0:
            logger.info(
                "%d repositories already tracked, %d need initialization",
                already_tracked_count,
                len(repos_needing_validation),
            )

        # Initialize repos without validation (errors will appear during actual processing)
        successful = 0
        skipped = 0

        for repo in repos_to_init:
            # Check if already tracked
            existing_state = state_manager.get_repo_state(owner, repo)
            if existing_state:
                logger.info(
                    "[%s/%s] Already tracked (last update: %s), skipping",
                    owner,
                    repo,
                    existing_state["timestamp"].isoformat(),
                )
                skipped += 1
                continue

            # Expand output pattern
            csv_file = expand_output_pattern(args.output, owner, repo)

            # Create parent directory if needed
            csv_path = Path(csv_file)
            if csv_path.parent and not csv_path.parent.exists():
                logger.info("Creating directory: %s", csv_path.parent)
                csv_path.parent.mkdir(parents=True, exist_ok=True)

            # Update state file (CSV will be created on first update with actual data)
            state_manager.update_repo(owner, repo, start_date, csv_file)
            logger.info(
                "[%s/%s] Initialized (start date: %s)",
                owner,
                repo,
                start_date.isoformat(),
            )
            successful += 1

        # Summary
        logger.info("=" * 80)
        logger.info("Initialization complete:")
        logger.info("  Initialized: %d", successful)
        logger.info("  Skipped (already tracked): %d", skipped)
        return 0

    # Handle update-all mode
    if args.update_all:
        # Process all repos - reload state and restart on any failure
        while True:
            # CRITICAL: Reload state file on each iteration to get updated timestamps
            try:
                tracked_repos = state_manager.get_all_tracked_repos()
            except Exception as e:
                logger.error("Failed to load state file: %s", e)
                return 1

            if not tracked_repos:
                logger.error(
                    "No tracked repositories found in state file. "
                    "Run without --update-all first to track repositories."
                )
                return 1

            # Sort repositories by timestamp (oldest first)
            tracked_repos.sort(key=lambda r: r["timestamp"])

            logger.info("Updating %d tracked repositories", len(tracked_repos))

            # Check quota before starting update-all
            max_prs = quota_manager.calculate_max_prs()
            if max_prs == 0:
                remaining, limit, _ = quota_manager.get_current_quota()
                logger.error(
                    "Cannot start update-all: quota exhausted or below reserve (%d/%d remaining)",
                    remaining,
                    limit,
                )
                if args.wait:
                    # Show progress summary before waiting
                    show_update_all_progress_summary(tracked_repos, 0)
                    logger.info("Will wait for quota reset, then start processing")

                    if not quota_manager.wait_for_reset(logger):
                        return 1
                    # After waiting, refresh quota and reload state
                    quota_manager.initialize(token)
                    max_prs = quota_manager.calculate_max_prs()
                    if max_prs == 0:
                        logger.error("Quota still exhausted after reset, aborting")
                        return 1
                    logger.info("Resuming update-all with refreshed quota")
                    continue  # Restart loop to reload state
                else:
                    logger.error("Use --wait to automatically wait for reset, or try again later")
                    return 1

            logger.info("Quota check: can process up to ~%d PRs total across all repos", max_prs)

            completed_repos = 0
            failed_repo = None

            for repo_info in tracked_repos:
                owner = repo_info["owner"]
                repo = repo_info["repo"]
                csv_file = repo_info["csv_file"]
                last_update = repo_info["timestamp"]

                if not csv_file:
                    logger.warning("Skipping %s/%s: no CSV file stored in state", owner, repo)
                    continue

                logger.info("=" * 80)

                start_date = last_update
                end_date = datetime.now(timezone.utc)

                exit_code, chunks_done, chunks_fetched = process_repository(
                    owner,
                    repo,
                    csv_file,
                    start_date,
                    end_date,
                    token,
                    workers,
                    config,
                    merge_mode=True,
                )

                if exit_code == 0:
                    completed_repos += 1
                else:
                    # Repo partially processed - record and stop this pass
                    failed_repo = f"{owner}/{repo}"
                    logger.warning(
                        "Partially processed %s (%d chunks completed, quota exhausted)",
                        failed_repo,
                        chunks_done,
                    )
                    break

            # Check if we completed all repos
            if failed_repo is None:
                logger.info("=" * 80)
                logger.info("Successfully processed all %d repositories", completed_repos)
                return 0

            # Failed at least one repo
            if args.wait:
                # Show progress summary before waiting
                show_update_all_progress_summary(tracked_repos, completed_repos, failed_repo)
                logger.info("Will wait for quota reset, then restart from beginning")

                if not quota_manager.wait_for_reset(logger):
                    logger.error("Failed to wait for quota reset, aborting")
                    return 1
                # Refresh quota and restart from beginning
                quota_manager.initialize(token)
                logger.info("Quota refreshed, restarting update-all from beginning")
                continue  # Restart while loop
            else:
                logger.error(
                    "Incomplete: processed %d repos, stopped at %s (quota exhausted). "
                    "Use --wait to retry from beginning, or run again later.",
                    completed_repos,
                    failed_repo,
                )
                return 1

    # Get repository info
    owner = args.owner
    repo = args.repo

    if not owner or not repo:
        detected_owner, detected_repo = get_repo_from_git()
        owner = owner or detected_owner
        repo = repo or detected_repo

    if not owner or not repo:
        logger.error(
            "Could not determine repository. "
            "Use --owner and --repo or run from a git repository."
        )
        return 1

    # Handle update mode for specific repo
    if args.update:
        if not owner or not repo:
            logger.error("--update requires --owner and --repo to be specified")
            return 1

        try:
            repo_state = state_manager.get_repo_state(owner, repo)
        except ValueError as e:
            logger.error("State file validation failed: %s", e)
            return 1

        if not repo_state:
            logger.error(
                "Repository %s/%s not found in state file. "
                "Run without --update first to track this repository.",
                owner,
                repo,
            )
            return 1

        csv_file = repo_state["csv_file"]
        start_date = repo_state["timestamp"]
        end_date = datetime.now(timezone.utc)

        exit_code, _, _ = process_repository(
            owner,
            repo,
            csv_file,
            start_date,
            end_date,
            token,
            workers,
            config,
            merge_mode=True,
        )
        return exit_code

    # Handle single PR update mode
    if args.pr:
        output_pattern = args.output or config.output_pattern
        output_file = expand_output_pattern(output_pattern, owner, repo)
        exit_code = update_single_pr(owner, repo, args.pr, output_file, token, config)
        return exit_code

    # Regular mode (non-update)
    if not args.output:
        output_file = None  # stdout
    else:
        # Expand output pattern with owner/repo placeholders
        output_file = expand_output_pattern(args.output, owner, repo)

    # Get time range from args or defaults
    end_date = parse_timestamp(args.end) if args.end else datetime.now(timezone.utc)
    start_date = (
        parse_timestamp(args.start) if args.start else end_date - timedelta(days=DEFAULT_DAYS_BACK)
    )

    if start_date >= end_date:
        logger.error("Start date must be before end date")
        return 1

    # Only store state if output_file is specified (not stdout)
    if output_file:
        # Create parent directory if needed
        output_path = Path(output_file)
        if output_path.parent and not output_path.parent.exists():
            logger.info("Creating directory: %s", output_path.parent)
            output_path.parent.mkdir(parents=True, exist_ok=True)

        exit_code, _, _ = process_repository(
            owner, repo, output_file, start_date, end_date, token, workers, config
        )
        return exit_code
    else:
        # stdout mode - fetch all PRs then process (no state tracking)
        try:
            all_prs = github_client.fetch_all_prs(owner, repo, start_date, end_date)

            if not all_prs:
                logger.warning("No pull requests found in the specified date range")
                return 0

            metrics = []
            total_prs = len(all_prs)
            completed = 0

            logger.info("Processing %d PRs with %d workers", total_prs, workers)

            # Process PRs in parallel
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(fetch_and_process_pr, pr, owner, repo, token, config): pr
                    for pr in all_prs
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)

                        logger.info(
                            "Completed PR %d/%d: #%d (updated: %s)",
                            completed,
                            total_prs,
                            pr["number"],
                            pr.get("updated_at", "unknown"),
                        )
                    except Exception as e:
                        logger.error("Failed to process PR #%d: %s", pr["number"], e)

            csv_manager.write_csv(metrics, None, merge_mode=False)
            logger.info("Successfully processed %d pull requests", len(metrics))
            return 0

        except GitHubAPIError as e:
            logger.error("GitHub API error: %s", e)
            return 1
        except Exception as e:
            logger.error("Unexpected error: %s", e, exc_info=args.debug)
            return 1


# ============================================================================
# Utility Functions
# ============================================================================


def setup_logging(debug: bool = False, log_file: str = "gh-pr-metrics.log") -> None:
    """Configure logging based on debug flag with file and stderr output."""
    level = logging.DEBUG if debug else logging.INFO

    # Create formatters
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    # Setup root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(level)

    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Add stderr handler
    stderr_handler = logging.StreamHandler(sys.stderr)
    stderr_handler.setLevel(level)
    stderr_handler.setFormatter(formatter)
    root_logger.addHandler(stderr_handler)

    # Add file handler
    try:
        file_handler = logging.FileHandler(log_file, mode="a", encoding="utf-8")
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
    except Exception as e:
        logging.warning(f"Could not create log file {log_file}: {e}")


def estimate_api_calls_for_prs(pr_count: int) -> int:
    """
    Estimate API calls needed to process pr_count PRs.
    Each PR requires: timeline events, reviews, review comments, issue comments.
    """
    return pr_count * API_CALLS_PER_PR


def get_github_token() -> Optional[str]:
    """Get GitHub token from environment."""
    return os.getenv("GITHUB_TOKEN")


def get_repo_from_git() -> tuple[Optional[str], Optional[str]]:
    """
    Get repository owner and name from git config.
    Returns (owner, repo) or (None, None) if not found.
    """
    try:
        remote_url = subprocess.check_output(
            ["git", "config", "--get", "remote.origin.url"],
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()

        # Parse SSH URL (git@github.com:owner/repo.git)
        if remote_url.startswith("git@"):
            _, path = remote_url.split(":", 1)
        # Parse HTTPS URL (https://github.com/owner/repo.git)
        elif remote_url.startswith("https://"):
            parts = remote_url.split("/")
            if len(parts) >= 5:
                path = f"{parts[3]}/{parts[4]}"
            else:
                return None, None
        else:
            return None, None

        # Remove .git suffix
        if path.endswith(".git"):
            path = path[:-4]

        owner, repo = path.split("/", 1)
        return owner, repo
    except Exception as e:
        logger.debug("Failed to get repo from git: %s", e)
        return None, None


def parse_timestamp(timestamp_str: str) -> datetime:
    """
    Parse various timestamp formats to datetime (always returns timezone-aware).

    Raises ValueError with clear message if timestamp cannot be parsed.
    """
    if not timestamp_str or not timestamp_str.strip():
        raise ValueError("Timestamp string is empty or None")

    try:
        # Try Unix timestamp first
        dt = datetime.fromtimestamp(float(timestamp_str))
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, OverflowError):
        try:
            # Try ISO 8601 / RFC 3339
            dt = date_parser.parse(timestamp_str)
            # Make timezone-aware if naive
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError) as e:
            raise ValueError(f"Invalid timestamp format: {timestamp_str}") from e


def expand_output_pattern(pattern: str, owner: str, repo: str) -> str:
    """
    Expand output pattern with owner and repo placeholders.
    Supports {owner} and {repo} placeholders.
    """
    return pattern.replace("{owner}", owner).replace("{repo}", repo)


if __name__ == "__main__":
    sys.exit(main())
