#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub Pull Request Metrics Tool

A command-line tool to generate CSV reports containing key metrics for all pull
requests in a GitHub repository within a specified time range.
"""

import argparse
import csv
import logging
import os
import subprocess
import sys
import tempfile
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

try:
    import argcomplete

    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False

import requests
from dateutil import parser as date_parser


# Version
__version__ = "0.1.0"

# Constants
GITHUB_API_BASE = "https://api.github.com"
DEFAULT_DAYS_BACK = 365
DEFAULT_WORKERS = 4
STATE_FILE = Path.home() / ".gh-pr-metrics-state.yaml"
PR_NUMBER_FIELD = "pr_number"
MAX_CHUNK_DAYS = 30  # Maximum days to process in a single chunk
API_CALLS_PER_PR = 4  # Estimate: timeline events, reviews, comments, review comments
API_SAFETY_BUFFER = 10  # Reserve for safety (unauthenticated limit is 60/hour total)


class GitHubAPIError(Exception):
    """Exception raised for GitHub API errors."""

    pass


def setup_logging(debug: bool = False) -> None:
    """Configure logging based on debug flag."""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        stream=sys.stderr,
    )


def load_state_file() -> Dict[str, str]:
    """Load state file containing last update dates per repo."""
    if not STATE_FILE.exists():
        return {}

    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
            return data
    except Exception as e:
        logging.warning("Failed to load state file %s: %s", STATE_FILE, e)
        return {}


def save_state_file(state: Dict[str, str]) -> None:
    """Save state file with last update dates per repo."""
    try:
        with open(STATE_FILE, "w", encoding="utf-8") as f:
            yaml.safe_dump(state, f, default_flow_style=False, sort_keys=True)

        logging.debug("Saved state file to %s", STATE_FILE)
    except Exception as e:
        logging.error("Failed to save state file %s: %s", STATE_FILE, e)
        raise


def get_repo_remote_url(owner: str, repo: str) -> str:
    """
    Get repository remote URL key for state tracking.
    Uses format: https://github.com/owner/repo
    Future-proof for GitLab, GitHub Enterprise, etc.
    """
    return f"https://github.com/{owner}/{repo}"


def get_repo_state(owner: str, repo: str) -> Optional[Dict[str, Any]]:
    """
    Get state for a repository (timestamp and csv_file).
    Returns dict with 'timestamp' and 'csv_file' keys, or None if not found.
    Raises ValueError if state is malformed with clear error message.
    """
    state = load_state_file()
    repo_key = get_repo_remote_url(owner, repo)

    if repo_key not in state:
        return None

    entry = state[repo_key]

    if not isinstance(entry, dict):
        raise ValueError(
            f"Invalid state format for {repo_key} in state file. "
            f"Expected dict with 'timestamp' and 'csv_file', got {type(entry).__name__}"
        )

    timestamp_str = entry.get("timestamp", "")
    csv_file = entry.get("csv_file")

    if not timestamp_str:
        raise ValueError(
            f"Missing or empty 'timestamp' field for {repo_key} in state file. "
            f"State file location: {STATE_FILE}"
        )

    if not csv_file:
        raise ValueError(
            f"Missing or empty 'csv_file' field for {repo_key} in state file. "
            f"State file location: {STATE_FILE}"
        )

    try:
        timestamp = parse_timestamp(timestamp_str)
    except ValueError as e:
        raise ValueError(
            f"Invalid timestamp for {repo_key} in state file: {e}. "
            f"State file location: {STATE_FILE}"
        ) from e

    return {"timestamp": timestamp, "csv_file": csv_file}


def get_last_update_date(owner: str, repo: str) -> Optional[datetime]:
    """Get last update date for a repository from state file."""
    repo_state = get_repo_state(owner, repo)
    return repo_state["timestamp"] if repo_state else None


def get_all_tracked_repos() -> List[Dict[str, Any]]:
    """
    Get all tracked repositories from state file.
    Returns list of dicts with 'url', 'owner', 'repo', 'timestamp', 'csv_file'.
    """
    state = load_state_file()
    repos = []

    for repo_url, entry in state.items():
        # Parse URL to get owner/repo
        # Format: https://github.com/owner/repo
        try:
            parts = repo_url.rstrip("/").split("/")
            if len(parts) >= 2:
                owner = parts[-2]
                repo = parts[-1]

                if isinstance(entry, dict):
                    timestamp = parse_timestamp(entry.get("timestamp", ""))
                    csv_file = entry.get("csv_file")
                else:
                    logging.warning("Skipping %s: invalid state format", repo_url)
                    continue

                repos.append(
                    {
                        "url": repo_url,
                        "owner": owner,
                        "repo": repo,
                        "timestamp": timestamp,
                        "csv_file": csv_file,
                    }
                )
        except Exception as e:
            logging.warning("Failed to parse state entry for %s: %s", repo_url, e)
            continue

    return repos


def update_state_file(owner: str, repo: str, timestamp: datetime, csv_file: str) -> None:
    """Update state file with new last update date and csv file for a repository."""
    state = load_state_file()
    repo_key = get_repo_remote_url(owner, repo)
    # Store as UTC timestamp without timezone component to avoid confusion
    utc_timestamp = timestamp.astimezone(timezone.utc).replace(tzinfo=None)
    state[repo_key] = {"timestamp": utc_timestamp.isoformat(), "csv_file": csv_file}
    save_state_file(state)


def chunk_date_range(
    start_date: datetime, end_date: datetime, max_days: int = MAX_CHUNK_DAYS
) -> List[tuple[datetime, datetime]]:
    """
    Split a date range into chunks of max_days with 1-day overlap.
    Returns list of (chunk_start, chunk_end) tuples.

    Overlap ensures no PRs are missed due to timestamp precision.
    Duplicates are handled by CSV merge (PR number deduplication).
    """
    chunks = []
    current_start = start_date

    while current_start < end_date:
        # Calculate chunk end (either max_days from start or end_date, whichever is earlier)
        chunk_end = min(current_start + timedelta(days=max_days), end_date)
        chunks.append((current_start, chunk_end))

        # Next chunk starts 1 day before this chunk ended (overlap to prevent gaps)
        # Duplicates will be deduplicated by PR number during CSV merge
        current_start = chunk_end - timedelta(days=1)

        # Ensure we don't create infinite loops if chunk_end == end_date
        if chunk_end >= end_date:
            break

    return chunks


def estimate_api_calls_for_prs(pr_count: int) -> int:
    """
    Estimate API calls needed to process pr_count PRs.
    Each PR requires: timeline events, reviews, review comments, issue comments.
    """
    return pr_count * API_CALLS_PER_PR


def check_sufficient_rate_limit(
    estimated_calls: int, token: Optional[str], repo_ctx: str, chunk_info: str = ""
) -> bool:
    """
    Check if sufficient API rate limit is available.
    Returns True if sufficient, False otherwise.
    Logs appropriate messages.

    chunk_info: Optional string like "Chunk 1/14: " for context
    """
    rate_info = get_rate_limit_info(token)

    if not rate_info:
        logging.warning("[%s] Could not check rate limit, proceeding with caution", repo_ctx)
        return True

    remaining = rate_info["remaining"]
    limit = rate_info["limit"]
    reset_timestamp = rate_info["reset"]
    reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
    reset_str = reset_time.strftime("%Y-%m-%d %H:%M:%S UTC")

    # Always show current rate limit status (consistent with initial check format)
    logging.info(
        "[%s] API rate limit: %d/%d remaining (resets at %s)", repo_ctx, remaining, limit, reset_str
    )

    # Show estimated calls for this chunk/operation
    logging.info("[%s] %sEstimated API calls: ~%d", repo_ctx, chunk_info, estimated_calls)

    if remaining <= (estimated_calls + API_SAFETY_BUFFER):
        logging.error(
            "[%s] %sInsufficient API rate limit: need ~%d calls + %d buffer, only %d available",
            repo_ctx,
            chunk_info,
            estimated_calls,
            API_SAFETY_BUFFER,
            remaining,
        )
        return False

    return True


def get_github_token() -> Optional[str]:
    """Get GitHub token from environment."""
    return os.getenv("GITHUB_TOKEN")


def get_repo_from_git() -> tuple[Optional[str], Optional[str]]:
    """
    Get repository owner and name from git config.
    Returns (owner, repo) or (None, None) if not found.
    """
    try:
        remote_url = subprocess.check_output(
            ["git", "config", "--get", "remote.origin.url"],
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()

        # Parse SSH URL (git@github.com:owner/repo.git)
        if remote_url.startswith("git@"):
            _, path = remote_url.split(":", 1)
        # Parse HTTPS URL (https://github.com/owner/repo.git)
        elif remote_url.startswith("https://"):
            parts = remote_url.split("/")
            if len(parts) >= 5:
                path = f"{parts[3]}/{parts[4]}"
            else:
                return None, None
        else:
            return None, None

        # Remove .git suffix
        if path.endswith(".git"):
            path = path[:-4]

        owner, repo = path.split("/", 1)
        return owner, repo
    except Exception as e:
        logging.debug("Failed to get repo from git: %s", e)
        return None, None


def parse_timestamp(timestamp_str: str) -> datetime:
    """
    Parse various timestamp formats to datetime (always returns timezone-aware).

    Raises ValueError with clear message if timestamp cannot be parsed.
    """
    if not timestamp_str or not timestamp_str.strip():
        raise ValueError("Timestamp string is empty or None")

    try:
        # Try Unix timestamp first
        dt = datetime.fromtimestamp(float(timestamp_str))
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, OverflowError):
        try:
            # Try ISO 8601 / RFC 3339
            dt = date_parser.parse(timestamp_str)
            # Make timezone-aware if naive
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except (ValueError, TypeError) as e:
            raise ValueError(f"Invalid timestamp format: {timestamp_str}") from e


def get_rate_limit_info(token: Optional[str] = None) -> Dict[str, Any]:
    """
    Get GitHub API rate limit information.
    Returns dict with 'remaining', 'limit', 'reset' keys.
    Returns empty dict on error.
    """
    headers = {
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"

    try:
        response = requests.get(f"{GITHUB_API_BASE}/rate_limit", headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()

        core = data.get("resources", {}).get("core", {})
        return {
            "remaining": core.get("remaining", 0),
            "limit": core.get("limit", 0),
            "reset": core.get("reset", 0),
        }
    except Exception as e:
        logging.debug("Could not check rate limit: %s", e)
        return {}


def check_rate_limit(token: Optional[str] = None) -> None:
    """Check and display current GitHub API rate limit status."""
    rate_info = get_rate_limit_info(token)

    if not rate_info:
        return

    remaining = rate_info.get("remaining", "unknown")
    limit = rate_info.get("limit", "unknown")
    reset_timestamp = rate_info.get("reset", 0)

    if reset_timestamp:
        reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
        reset_str = reset_time.strftime("%Y-%m-%d %H:%M:%S UTC")
    else:
        reset_str = "unknown"

    logging.info(
        "API rate limit: %s/%s remaining (resets at %s)",
        remaining,
        limit,
        reset_str,
    )


def make_github_request(
    url: str, token: Optional[str] = None, params: Optional[Dict[str, Any]] = None
) -> Any:
    """Make a GitHub API request with error handling."""
    headers = {
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"

    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()

        # Log rate limit info on first request (debug only)
        if logging.getLogger().isEnabledFor(logging.DEBUG):
            rate_limit = response.headers.get("X-RateLimit-Remaining")
            rate_limit_reset = response.headers.get("X-RateLimit-Reset")
            if rate_limit:
                logging.debug(
                    "API rate limit: %s remaining (resets at %s)",
                    rate_limit,
                    rate_limit_reset,
                )

        return response.json()
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 403:
            # Check if it's actually a rate limit error
            rate_limit = e.response.headers.get("X-RateLimit-Remaining")
            if rate_limit == "0":
                reset_time = e.response.headers.get("X-RateLimit-Reset", "unknown")
                auth_status = "authenticated" if token else "unauthenticated"
                raise GitHubAPIError(
                    f"GitHub API rate limit exceeded ({auth_status}). "
                    f"Rate limit resets at {reset_time}. "
                    f"Limit: {e.response.headers.get('X-RateLimit-Limit', 'unknown')}"
                )
            else:
                # 403 but not rate limit - likely permissions
                raise GitHubAPIError(
                    "GitHub API forbidden: insufficient permissions or invalid token."
                )
        elif e.response.status_code == 401:
            raise GitHubAPIError("GitHub API unauthorized: invalid or expired token.")
        elif e.response.status_code == 404:
            raise GitHubAPIError("Repository not found or insufficient permissions.")
        else:
            raise GitHubAPIError(f"GitHub API error: {e}")
    except requests.exceptions.RequestException as e:
        raise GitHubAPIError(f"Network error: {e}")


def fetch_pull_requests(
    owner: str, repo: str, start_date: datetime, end_date: datetime, token: Optional[str]
) -> List[Dict[str, Any]]:
    """Fetch all pull requests within the date range."""
    url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls"
    all_prs = []
    page = 1
    per_page = 100

    logging.info("Fetching pull requests from %s/%s (page size: %d)", owner, repo, per_page)

    while True:
        params = {
            "state": "all",
            "sort": "created",
            "direction": "desc",
            "page": page,
            "per_page": per_page,
        }

        prs = make_github_request(url, token, params)

        if not prs:
            break

        prs_in_page = len(prs)
        prs_in_range = 0

        for pr in prs:
            created_at = date_parser.parse(pr["created_at"])
            # Stop if we're before the start date
            if created_at < start_date:
                logging.info(
                    "Page %d: fetched %d PRs, %d in date range, %d cumulative (reached start date)",
                    page,
                    prs_in_page,
                    prs_in_range,
                    len(all_prs),
                )
                logging.info("Finished fetching: %d total PRs in date range", len(all_prs))
                return all_prs

            # Only include PRs within date range
            if start_date <= created_at <= end_date:
                all_prs.append(pr)
                prs_in_range += 1

        logging.info(
            "Page %d: fetched %d PRs, %d in date range, %d cumulative",
            page,
            prs_in_page,
            prs_in_range,
            len(all_prs),
        )
        page += 1

        # Break if we got fewer results than requested
        if len(prs) < per_page:
            logging.info("Finished fetching: %d total PRs in date range", len(all_prs))
            break

    return all_prs


def get_ready_for_review_time(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> str:
    """Get the timestamp when PR was last marked ready for review."""
    # If never a draft, use created_at
    if not pr.get("draft", False) and pr.get("created_at"):
        # Check events to see if it was ever a draft
        events_url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/issues/{pr['number']}/events"
        try:
            events = make_github_request(events_url, token)
            ready_events = [e for e in events if e.get("event") == "ready_for_review"]
            if ready_events:
                # Use the latest ready_for_review event
                return ready_events[-1]["created_at"]
        except GitHubAPIError as e:
            logging.debug("Failed to fetch events for PR #%d: %s", pr["number"], e)

    return pr["created_at"]


def count_comments(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], ai_bot_pattern: str = None
) -> tuple[int, int, int, str, str]:
    """
    Count total comments, non-AI bot comments, and AI bot comments.
    Returns (total_comments, non_ai_bot_comments, ai_bot_comments,
             non_ai_bot_login_names, ai_bot_login_names).
    Non-AI bot and AI bot login names are comma-separated strings.
    ai_bot_pattern: regex pattern to identify AI bots
    """
    import re

    total_comments = 0
    non_ai_bot_comments = 0
    ai_bot_comments = 0
    non_ai_bot_logins = set()
    ai_bot_logins = set()

    # Issue comments (general PR comments)
    comments_url = pr["comments_url"]
    try:
        comments = make_github_request(comments_url, token)
        for comment in comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                if ai_bot_pattern and re.match(ai_bot_pattern, login):
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logging.debug("Failed to fetch comments for PR #%d: %s", pr["number"], e)

    # Review comments (inline code comments)
    review_comments_url = pr["review_comments_url"]
    try:
        review_comments = make_github_request(review_comments_url, token)
        for comment in review_comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                login = comment["user"]["login"]
                if ai_bot_pattern and re.match(ai_bot_pattern, login):
                    ai_bot_logins.add(login)
                    ai_bot_comments += 1
                else:
                    non_ai_bot_logins.add(login)
                    non_ai_bot_comments += 1
    except GitHubAPIError as e:
        logging.debug("Failed to fetch review comments for PR #%d: %s", pr["number"], e)

    # Convert to comma-separated strings, sorted for consistency
    non_ai_bot_login_names = ",".join(sorted(non_ai_bot_logins))
    ai_bot_login_names = ",".join(sorted(ai_bot_logins))

    return (
        total_comments,
        non_ai_bot_comments,
        ai_bot_comments,
        non_ai_bot_login_names,
        ai_bot_login_names,
    )


def get_review_metrics(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> tuple[int, int, int]:
    """
    Get review metrics: changes requested count, unique change requesters, approvals.
    Returns (changes_requested_count, unique_change_requesters, approval_count).
    """
    reviews_url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls/{pr['number']}/reviews"

    try:
        reviews = make_github_request(reviews_url, token)
    except GitHubAPIError as e:
        logging.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
        return 0, 0, 0

    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r["user"]["login"]
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review["user"]["login"]
            # Later reviews override earlier ones
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def determine_pr_status(pr: Dict[str, Any]) -> str:
    """Determine the current status of the PR."""
    if pr.get("draft"):
        return "draft"
    elif pr.get("merged_at"):
        return "merged"
    elif pr.get("state") == "closed":
        return "closed"
    else:
        return "open"


def process_pr(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str], ai_bot_pattern: str = None
) -> Dict[str, Any]:
    """Process a single PR and extract all metrics."""
    pr_number = pr["number"]
    logging.debug("Processing PR #%d: %s", pr_number, pr["title"])

    errors = []
    metrics: Dict[str, Any] = {
        PR_NUMBER_FIELD: pr_number,
        "title": pr["title"],
        "author": pr["user"]["login"] if pr.get("user") else "",
        "created_at": pr["created_at"],
        "url": pr["html_url"],
    }

    # Ready for review time
    try:
        metrics["ready_for_review_at"] = get_ready_for_review_time(pr, owner, repo, token)
    except Exception as e:
        logging.debug("Error getting ready time for PR #%d: %s", pr_number, e)
        metrics["ready_for_review_at"] = pr["created_at"]
        errors.append(f"ready_time: {e}")

    # Comment counts
    try:
        total_comments, non_ai_bot_comments, ai_bot_comments, non_ai_bot_names, ai_bot_names = (
            count_comments(pr, owner, repo, token, ai_bot_pattern)
        )
        metrics["total_comment_count"] = total_comments
        metrics["non_ai_bot_comment_count"] = non_ai_bot_comments
        metrics["ai_bot_comment_count"] = ai_bot_comments
        metrics["non_ai_bot_login_names"] = non_ai_bot_names
        metrics["ai_bot_login_names"] = ai_bot_names
    except Exception as e:
        logging.debug("Error counting comments for PR #%d: %s", pr_number, e)
        metrics["total_comment_count"] = 0
        metrics["non_ai_bot_comment_count"] = 0
        metrics["ai_bot_comment_count"] = 0
        metrics["non_ai_bot_login_names"] = ""
        metrics["ai_bot_login_names"] = ""
        errors.append(f"comments: {e}")

    # Review metrics
    try:
        changes_requested, unique_requesters, approvals = get_review_metrics(pr, owner, repo, token)
        metrics["changes_requested_count"] = changes_requested
        metrics["unique_change_requesters"] = unique_requesters
        metrics["approval_count"] = approvals
    except Exception as e:
        logging.debug("Error getting reviews for PR #%d: %s", pr_number, e)
        metrics["changes_requested_count"] = 0
        metrics["unique_change_requesters"] = 0
        metrics["approval_count"] = 0
        errors.append(f"reviews: {e}")

    # Status
    metrics["status"] = determine_pr_status(pr)

    # Merged and closed timestamps
    metrics["merged_at"] = pr.get("merged_at") or ""
    metrics["closed_at"] = pr.get("closed_at") or ""

    # Errors
    metrics["errors"] = "; ".join(errors) if errors else ""

    return metrics


def read_existing_csv(csv_file: str) -> Dict[int, Dict[str, Any]]:
    """
    Read existing CSV file and return dict keyed by PR number.
    Returns empty dict if file doesn't exist or can't be read.
    """
    if not os.path.exists(csv_file):
        return {}

    try:
        existing_data = {}
        with open(csv_file, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Key by PR number for O(1) lookups
                pr_number = int(row[PR_NUMBER_FIELD])
                existing_data[pr_number] = row

        logging.info("Loaded %d existing PRs from %s", len(existing_data), csv_file)
        return existing_data
    except Exception as e:
        logging.warning("Failed to read existing CSV %s: %s", csv_file, e)
        return {}


def write_csv_output(
    metrics: List[Dict[str, Any]], output_file: Optional[str], merge_mode: bool = False
) -> None:
    """
    Write metrics to CSV file or stdout.

    If merge_mode is True and output_file exists, existing PR data is loaded,
    updated with new metrics, and written back (replacing existing PRs and adding new ones).
    """
    if not metrics:
        logging.warning("No pull requests to output")
        return

    fieldnames = [
        PR_NUMBER_FIELD,
        "title",
        "author",
        "created_at",
        "ready_for_review_at",
        "merged_at",
        "closed_at",
        "total_comment_count",
        "non_ai_bot_comment_count",
        "ai_bot_comment_count",
        "non_ai_bot_login_names",
        "ai_bot_login_names",
        "changes_requested_count",
        "unique_change_requesters",
        "approval_count",
        "status",
        "url",
        "errors",
    ]

    # Merge with existing data if requested
    if merge_mode and output_file:
        existing_data = read_existing_csv(output_file)

        # Update existing data with new metrics
        for metric in metrics:
            pr_number = metric[PR_NUMBER_FIELD]
            existing_data[pr_number] = metric

        # Convert back to list (order doesn't matter per requirements)
        all_metrics = list(existing_data.values())
        logging.info("Merged data: %d total PRs (%d new/updated)", len(all_metrics), len(metrics))
    else:
        all_metrics = metrics

    if output_file:
        # Write to temp file first, then atomically rename
        output_path = Path(output_file)
        temp_fd, temp_path = tempfile.mkstemp(
            dir=output_path.parent if output_path.parent.exists() else tempfile.gettempdir(),
            prefix=".tmp_pr_metrics_",
            suffix=".csv",
        )

        try:
            with os.fdopen(temp_fd, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(all_metrics)

            # Atomic rename
            os.rename(temp_path, output_file)
            logging.info("Writing output to %s", output_file)
        except Exception as e:
            # Keep temp file for debugging, just log its location
            logging.error("Failed to write CSV output. Temp file retained at: %s", temp_path)
            raise e
    else:
        writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_metrics)


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate CSV reports of GitHub pull request metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze PRs from current repository (last year)
  gh-pr-metrics

  # Specify repository explicitly
  gh-pr-metrics --owner microsoft --repo vscode

  # Custom time range
  gh-pr-metrics --start 2024-01-01 --end 2024-12-31

  # Output to file (stores path in state for future updates)
  gh-pr-metrics --output metrics.csv

  # Update specific repo (uses stored CSV path from state)
  gh-pr-metrics --owner microsoft --repo vscode --update

  # Update all tracked repositories
  gh-pr-metrics --update

Environment Variables:
  GITHUB_TOKEN    GitHub personal access token for authentication
        """,
    )

    parser.add_argument("--owner", help="Repository owner (default: auto-detect from git)")
    parser.add_argument("--repo", help="Repository name (default: auto-detect from git)")
    parser.add_argument(
        "--start",
        help="Start timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: 365 days ago",
    )
    parser.add_argument(
        "--end",
        help="End timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: now",
    )
    parser.add_argument("--output", "-o", help="Output file path (default: stdout)")
    parser.add_argument(
        "--update",
        action="store_true",
        help="Update mode: fetch only PRs since last update and merge with existing CSV. "
        "Requires --owner and --repo. Uses CSV path stored in state file. "
        "Cannot be used with --output (uses stored path).",
    )
    parser.add_argument(
        "--update-all",
        action="store_true",
        help="Update all tracked repositories. Cannot be used with --owner/--repo or --output.",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument(
        "--workers",
        type=int,
        default=DEFAULT_WORKERS,
        help=f"Number of parallel workers for processing PRs (default: {DEFAULT_WORKERS})",
    )
    parser.add_argument(
        "--ai-bot-regex",
        default="cursor\\[bot\\]",
        help="Regex pattern to identify AI bots (default: cursor\\[bot\\])",
    )
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    return parser.parse_args()


def process_repository(
    owner: str,
    repo: str,
    output_file: str,
    start_date: datetime,
    end_date: datetime,
    token: Optional[str],
    workers: int,
    ai_bot_regex: str,
    merge_mode: bool = False,
) -> tuple[int, int, int]:
    """
    Process a single repository and generate/update metrics CSV.
    Automatically chunks large date ranges to manage API rate limits.
    Returns (exit_code, chunks_completed, total_chunks).
    exit_code: 0 on success, 1 on error
    """
    repo_ctx = f"{owner}/{repo}"

    logging.info("[%s] Starting processing", repo_ctx)
    logging.info(
        "[%s] Date range: %s to %s", repo_ctx, start_date.isoformat(), end_date.isoformat()
    )

    # Calculate if chunking is needed
    total_days = (end_date - start_date).days
    chunks = chunk_date_range(start_date, end_date, MAX_CHUNK_DAYS)
    total_chunks = len(chunks)

    if total_chunks > 1:
        logging.info(
            "[%s] Date range spans %d days, splitting into %d chunks of up to %d days each",
            repo_ctx,
            total_days,
            total_chunks,
            MAX_CHUNK_DAYS,
        )

    # Track processed PR numbers to avoid duplicate processing across chunks
    processed_pr_numbers = set()
    chunks_completed = 0

    # Process each chunk
    for chunk_idx, (chunk_start, chunk_end) in enumerate(chunks, 1):
        logging.info(
            "[%s] Processing chunk %d/%d: %s to %s",
            repo_ctx,
            chunk_idx,
            len(chunks),
            chunk_start.isoformat(),
            chunk_end.isoformat(),
        )

        try:
            # Fetch PRs for this chunk
            prs = fetch_pull_requests(owner, repo, chunk_start, chunk_end, token)

            if not prs:
                logging.info(
                    "[%s] Chunk %d/%d: No pull requests found",
                    repo_ctx,
                    chunk_idx,
                    total_chunks,
                )
                # Update state to this chunk's end date and continue
                update_state_file(owner, repo, chunk_end, output_file)
                chunks_completed += 1
                continue

            # Filter out PRs already processed (due to overlap)
            new_prs = [pr for pr in prs if pr["number"] not in processed_pr_numbers]
            duplicate_count = len(prs) - len(new_prs)

            if duplicate_count > 0:
                logging.info(
                    "[%s] Chunk %d/%d: Skipping %d duplicate PRs "
                    "(already processed in previous chunk)",
                    repo_ctx,
                    chunk_idx,
                    len(chunks),
                    duplicate_count,
                )

            if not new_prs:
                logging.info(
                    "[%s] Chunk %d/%d: All %d PRs already processed",
                    repo_ctx,
                    chunk_idx,
                    total_chunks,
                    len(prs),
                )
                # Update state to this chunk's end date and continue
                update_state_file(owner, repo, chunk_end, output_file)
                chunks_completed += 1
                continue

            total_prs = len(new_prs)

            # Check rate limit before processing
            estimated_calls = estimate_api_calls_for_prs(total_prs)
            chunk_info_str = f"Chunk {chunk_idx}/{total_chunks}: "
            if not check_sufficient_rate_limit(estimated_calls, token, repo_ctx, chunk_info_str):
                logging.error(
                    "[%s] Stopping at chunk %d/%d due to insufficient rate limit",
                    repo_ctx,
                    chunk_idx,
                    total_chunks,
                )
                logging.error(
                    "[%s] Progress saved. Resume with --update (or --update-all) "
                    "after rate limit resets.",
                    repo_ctx,
                )
                return 1, chunks_completed, total_chunks

            # Process each PR with thread pool
            metrics = []
            completed = 0

            logging.info(
                "[%s] Chunk %d/%d: Processing %d PRs with %d workers",
                repo_ctx,
                chunk_idx,
                len(chunks),
                total_prs,
                workers,
            )

            with ThreadPoolExecutor(max_workers=workers) as executor:
                # Submit all PR processing tasks (only new PRs, not duplicates)
                future_to_pr = {
                    executor.submit(process_pr, pr, owner, repo, token, ai_bot_regex): pr
                    for pr in new_prs
                }

                # Collect results as they complete
                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    pr_number = pr["number"]
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)
                        # Mark this PR as processed
                        processed_pr_numbers.add(pr_number)
                        logging.info(
                            "[%s] Chunk %d/%d: Completed PR %d/%d: #%d",
                            repo_ctx,
                            chunk_idx,
                            len(chunks),
                            completed,
                            total_prs,
                            pr_number,
                        )
                    except Exception as e:
                        logging.error(
                            "[%s] Chunk %d/%d: Failed to process PR #%d: %s",
                            repo_ctx,
                            chunk_idx,
                            len(chunks),
                            pr_number,
                            e,
                        )
                        # Continue processing other PRs

            # Write output for this chunk (always merge mode after first chunk)
            chunk_merge_mode = merge_mode or (chunk_idx > 1)
            write_csv_output(metrics, output_file, merge_mode=chunk_merge_mode)

            # Update state file with this chunk's end date
            update_state_file(owner, repo, chunk_end, output_file)
            chunks_completed += 1
            logging.info(
                "[%s] Chunk %d/%d: Processed %d pull requests, updated state to %s",
                repo_ctx,
                chunk_idx,
                total_chunks,
                len(metrics),
                chunk_end.isoformat(),
            )

        except GitHubAPIError as e:
            logging.error(
                "[%s] Chunk %d/%d: GitHub API error: %s", repo_ctx, chunk_idx, total_chunks, e
            )
            return 1, chunks_completed, total_chunks
        except Exception as e:
            logging.error(
                "[%s] Chunk %d/%d: Unexpected error: %s",
                repo_ctx,
                chunk_idx,
                total_chunks,
                e,
                exc_info=True,
            )
            return 1, chunks_completed, total_chunks

    logging.info("[%s] Successfully completed all %d chunks", repo_ctx, total_chunks)
    return 0, chunks_completed, total_chunks


def main() -> int:
    """Main entry point."""
    args = parse_arguments()
    setup_logging(args.debug)

    logging.info("GitHub PR Metrics Tool v%s", __version__)
    logging.debug("Arguments: %s", args)

    # Validate argument combinations
    if args.update and args.output:
        logging.error("--update and --output cannot be used together (update uses stored CSV path)")
        return 1

    if args.update and (args.start or args.end):
        logging.error(
            "--update cannot be used with --start or --end (uses last update date from state)"
        )
        return 1

    if args.update_all and args.output:
        logging.error("--update-all and --output cannot be used together (uses stored CSV paths)")
        return 1

    if args.update_all and (args.start or args.end):
        logging.error(
            "--update-all cannot be used with --start or --end (uses last update dates from state)"
        )
        return 1

    if args.update_all and (args.owner or args.repo):
        logging.error("--update-all cannot be used with --owner or --repo")
        return 1

    if args.update and args.update_all:
        logging.error("--update and --update-all cannot be used together")
        return 1

    # Get GitHub token
    token = get_github_token()
    if not token:
        logging.warning(
            "No GITHUB_TOKEN found. API rate limits will be restrictive "
            "for unauthenticated requests."
        )
        # Force single worker to avoid hitting rate limits too quickly
        workers = 1 if args.workers > 1 else args.workers
        if args.workers > 1:
            logging.warning("Forcing --workers=1 due to missing GITHUB_TOKEN")
    else:
        workers = args.workers

    # Check and display rate limit status
    check_rate_limit(token)

    # Handle update-all mode
    if args.update_all:
        try:
            tracked_repos = get_all_tracked_repos()
        except Exception as e:
            logging.error("Failed to load state file: %s", e)
            return 1

        if not tracked_repos:
            logging.error(
                "No tracked repositories found in state file. "
                "Run without --update-all first to track repositories."
            )
            return 1

        logging.info("Updating %d tracked repositories", len(tracked_repos))

        completed_repos = 0
        partial_repo_info = None
        unprocessed_count = 0

        for idx, repo_info in enumerate(tracked_repos):
            owner = repo_info["owner"]
            repo = repo_info["repo"]
            csv_file = repo_info["csv_file"]
            last_update = repo_info["timestamp"]

            if not csv_file:
                logging.warning("Skipping %s/%s: no CSV file stored in state", owner, repo)
                unprocessed_count += 1
                continue

            logging.info("=" * 80)

            start_date = last_update
            end_date = datetime.now(timezone.utc)

            exit_code, chunks_done, total_chunks = process_repository(
                owner,
                repo,
                csv_file,
                start_date,
                end_date,
                token,
                workers,
                args.ai_bot_regex,
                merge_mode=True,
            )

            if exit_code == 0:
                completed_repos += 1
            else:
                # Stopped early (likely rate limit)
                partial_repo_info = {
                    "owner": owner,
                    "repo": repo,
                    "chunks_done": chunks_done,
                    "total_chunks": total_chunks,
                }
                # Count remaining repos as unprocessed
                unprocessed_count = len(tracked_repos) - idx - 1

                logging.warning(
                    "Stopping update-all: %s/%s partially completed (%d/%d chunks)",
                    owner,
                    repo,
                    chunks_done,
                    total_chunks,
                )
                if unprocessed_count > 0:
                    logging.warning(
                        "%d repositories not yet processed (will resume on next --update-all)",
                        unprocessed_count,
                    )
                break

        logging.info("=" * 80)

        # Summary output
        if partial_repo_info:
            logging.info("Completed: %d repos fully processed", completed_repos)
            logging.info(
                "Partial: %s/%s (%d/%d chunks completed)",
                partial_repo_info["owner"],
                partial_repo_info["repo"],
                partial_repo_info["chunks_done"],
                partial_repo_info["total_chunks"],
            )
            if unprocessed_count > 0:
                logging.info("Not processed: %d repos", unprocessed_count)
            return 1  # Incomplete
        else:
            logging.info("Successfully processed all %d repositories", completed_repos)
            return 0

    # Get repository info
    owner = args.owner
    repo = args.repo

    if not owner or not repo:
        detected_owner, detected_repo = get_repo_from_git()
        owner = owner or detected_owner
        repo = repo or detected_repo

    if not owner or not repo:
        logging.error(
            "Could not determine repository. "
            "Use --owner and --repo or run from a git repository."
        )
        return 1

    # Handle update mode for specific repo
    if args.update:
        if not owner or not repo:
            logging.error("--update requires --owner and --repo to be specified")
            return 1

        try:
            repo_state = get_repo_state(owner, repo)
        except ValueError as e:
            logging.error("State file validation failed: %s", e)
            return 1

        if not repo_state:
            logging.error(
                "Repository %s/%s not found in state file. "
                "Run without --update first to track this repository.",
                owner,
                repo,
            )
            return 1

        csv_file = repo_state["csv_file"]
        start_date = repo_state["timestamp"]
        end_date = datetime.now(timezone.utc)

        exit_code, _, _ = process_repository(
            owner,
            repo,
            csv_file,
            start_date,
            end_date,
            token,
            workers,
            args.ai_bot_regex,
            merge_mode=True,
        )
        return exit_code

    # Regular mode (non-update)
    if not args.output:
        output_file = None  # stdout
    else:
        output_file = args.output

    # Get time range from args or defaults
    end_date = parse_timestamp(args.end) if args.end else datetime.now(timezone.utc)
    start_date = (
        parse_timestamp(args.start) if args.start else end_date - timedelta(days=DEFAULT_DAYS_BACK)
    )

    if start_date >= end_date:
        logging.error("Start date must be before end date")
        return 1

    # Only store state if output_file is specified (not stdout)
    if output_file:
        exit_code, _, _ = process_repository(
            owner, repo, output_file, start_date, end_date, token, workers, args.ai_bot_regex
        )
        return exit_code
    else:
        # stdout mode - don't store state
        try:
            prs = fetch_pull_requests(owner, repo, start_date, end_date, token)

            if not prs:
                logging.warning("No pull requests found in the specified date range")
                return 0

            metrics = []
            total_prs = len(prs)
            completed = 0

            logging.info("Processing %d PRs with %d workers", total_prs, workers)

            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_pr = {
                    executor.submit(process_pr, pr, owner, repo, token, args.ai_bot_regex): pr
                    for pr in prs
                }

                for future in as_completed(future_to_pr):
                    pr = future_to_pr[future]
                    completed += 1
                    try:
                        pr_metrics = future.result()
                        metrics.append(pr_metrics)
                        logging.info("Completed PR %d/%d: #%d", completed, total_prs, pr["number"])
                    except Exception as e:
                        logging.error("Failed to process PR #%d: %s", pr["number"], e)

            write_csv_output(metrics, None, merge_mode=False)
            logging.info("Successfully processed %d pull requests", len(metrics))
            return 0

        except GitHubAPIError as e:
            logging.error("GitHub API error: %s", e)
            return 1
        except Exception as e:
            logging.error("Unexpected error: %s", e, exc_info=args.debug)
            return 1


if __name__ == "__main__":
    sys.exit(main())
