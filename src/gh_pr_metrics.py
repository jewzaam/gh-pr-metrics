#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub Pull Request Metrics Tool

A command-line tool to generate CSV reports containing key metrics for all pull
requests in a GitHub repository within a specified time range.
"""

import argparse
import csv
import logging
import os
import subprocess
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Dict, Any

try:
    import argcomplete

    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False

import requests
from dateutil import parser as date_parser


# Version
__version__ = "0.1.0"

# Constants
GITHUB_API_BASE = "https://api.github.com"
DEFAULT_DAYS_BACK = 365
DEFAULT_WORKERS = 4


class GitHubAPIError(Exception):
    """Exception raised for GitHub API errors."""

    pass


def setup_logging(debug: bool = False) -> None:
    """Configure logging based on debug flag."""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        stream=sys.stderr,
    )


def get_github_token() -> Optional[str]:
    """Get GitHub token from environment."""
    return os.getenv("GITHUB_TOKEN")


def get_repo_from_git() -> tuple[Optional[str], Optional[str]]:
    """
    Get repository owner and name from git config.
    Returns (owner, repo) or (None, None) if not found.
    """
    try:
        remote_url = subprocess.check_output(
            ["git", "config", "--get", "remote.origin.url"],
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()

        # Parse SSH URL (git@github.com:owner/repo.git)
        if remote_url.startswith("git@"):
            _, path = remote_url.split(":", 1)
        # Parse HTTPS URL (https://github.com/owner/repo.git)
        elif remote_url.startswith("https://"):
            parts = remote_url.split("/")
            if len(parts) >= 5:
                path = f"{parts[3]}/{parts[4]}"
            else:
                return None, None
        else:
            return None, None

        # Remove .git suffix
        if path.endswith(".git"):
            path = path[:-4]

        owner, repo = path.split("/", 1)
        return owner, repo
    except Exception as e:
        logging.debug("Failed to get repo from git: %s", e)
        return None, None


def parse_timestamp(timestamp_str: str) -> datetime:
    """Parse various timestamp formats to datetime (always returns timezone-aware)."""
    try:
        # Try Unix timestamp first
        dt = datetime.fromtimestamp(float(timestamp_str))
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except (ValueError, OverflowError):
        # Try ISO 8601 / RFC 3339
        dt = date_parser.parse(timestamp_str)
        # Make timezone-aware if naive
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt


def check_rate_limit(token: Optional[str] = None) -> None:
    """Check and display current GitHub API rate limit status."""
    headers = {
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"

    try:
        response = requests.get(f"{GITHUB_API_BASE}/rate_limit", headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()

        core = data.get("resources", {}).get("core", {})
        remaining = core.get("remaining", "unknown")
        limit = core.get("limit", "unknown")
        reset_timestamp = core.get("reset", 0)

        if reset_timestamp:
            from datetime import datetime

            reset_time = datetime.fromtimestamp(reset_timestamp, tz=timezone.utc)
            reset_str = reset_time.strftime("%Y-%m-%d %H:%M:%S UTC")
        else:
            reset_str = "unknown"

        logging.info(
            "API rate limit: %s/%s remaining (resets at %s)",
            remaining,
            limit,
            reset_str,
        )
    except Exception as e:
        logging.debug("Could not check rate limit: %s", e)


def make_github_request(
    url: str, token: Optional[str] = None, params: Optional[Dict[str, Any]] = None
) -> Any:
    """Make a GitHub API request with error handling."""
    headers = {
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
    }
    if token:
        headers["Authorization"] = f"Bearer {token}"

    try:
        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()

        # Log rate limit info on first request (debug only)
        if logging.getLogger().isEnabledFor(logging.DEBUG):
            rate_limit = response.headers.get("X-RateLimit-Remaining")
            rate_limit_reset = response.headers.get("X-RateLimit-Reset")
            if rate_limit:
                logging.debug(
                    "API rate limit: %s remaining (resets at %s)",
                    rate_limit,
                    rate_limit_reset,
                )

        return response.json()
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 403:
            # Check if it's actually a rate limit error
            rate_limit = e.response.headers.get("X-RateLimit-Remaining")
            if rate_limit == "0":
                reset_time = e.response.headers.get("X-RateLimit-Reset", "unknown")
                auth_status = "authenticated" if token else "unauthenticated"
                raise GitHubAPIError(
                    f"GitHub API rate limit exceeded ({auth_status}). "
                    f"Rate limit resets at {reset_time}. "
                    f"Limit: {e.response.headers.get('X-RateLimit-Limit', 'unknown')}"
                )
            else:
                # 403 but not rate limit - likely permissions
                raise GitHubAPIError(
                    "GitHub API forbidden: insufficient permissions or invalid token."
                )
        elif e.response.status_code == 401:
            raise GitHubAPIError("GitHub API unauthorized: invalid or expired token.")
        elif e.response.status_code == 404:
            raise GitHubAPIError("Repository not found or insufficient permissions.")
        else:
            raise GitHubAPIError(f"GitHub API error: {e}")
    except requests.exceptions.RequestException as e:
        raise GitHubAPIError(f"Network error: {e}")


def fetch_pull_requests(
    owner: str, repo: str, start_date: datetime, end_date: datetime, token: Optional[str]
) -> List[Dict[str, Any]]:
    """Fetch all pull requests within the date range."""
    url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls"
    all_prs = []
    page = 1
    per_page = 100

    logging.info("Fetching pull requests from %s/%s (page size: %d)", owner, repo, per_page)

    while True:
        params = {
            "state": "all",
            "sort": "created",
            "direction": "desc",
            "page": page,
            "per_page": per_page,
        }

        prs = make_github_request(url, token, params)

        if not prs:
            break

        prs_in_page = len(prs)
        prs_in_range = 0

        for pr in prs:
            created_at = date_parser.parse(pr["created_at"])
            # Stop if we're before the start date
            if created_at < start_date:
                logging.info(
                    "Page %d: fetched %d PRs, %d in date range, %d cumulative (reached start date)",
                    page,
                    prs_in_page,
                    prs_in_range,
                    len(all_prs),
                )
                logging.info("Finished fetching: %d total PRs in date range", len(all_prs))
                return all_prs

            # Only include PRs within date range
            if start_date <= created_at <= end_date:
                all_prs.append(pr)
                prs_in_range += 1

        logging.info(
            "Page %d: fetched %d PRs, %d in date range, %d cumulative",
            page,
            prs_in_page,
            prs_in_range,
            len(all_prs),
        )
        page += 1

        # Break if we got fewer results than requested
        if len(prs) < per_page:
            logging.info("Finished fetching: %d total PRs in date range", len(all_prs))
            break

    return all_prs


def get_ready_for_review_time(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> str:
    """Get the timestamp when PR was last marked ready for review."""
    # If never a draft, use created_at
    if not pr.get("draft", False) and pr.get("created_at"):
        # Check events to see if it was ever a draft
        events_url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/issues/{pr['number']}/events"
        try:
            events = make_github_request(events_url, token)
            ready_events = [e for e in events if e.get("event") == "ready_for_review"]
            if ready_events:
                # Use the latest ready_for_review event
                return ready_events[-1]["created_at"]
        except GitHubAPIError as e:
            logging.debug("Failed to fetch events for PR #%d: %s", pr["number"], e)

    return pr["created_at"]


def count_comments(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> tuple[int, int]:
    """
    Count total comments and bot comments.
    Returns (total_comments, bot_comments).
    """
    total_comments = 0
    bot_comments = 0

    # Issue comments (general PR comments)
    comments_url = pr["comments_url"]
    try:
        comments = make_github_request(comments_url, token)
        for comment in comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                bot_comments += 1
    except GitHubAPIError as e:
        logging.debug("Failed to fetch comments for PR #%d: %s", pr["number"], e)

    # Review comments (inline code comments)
    review_comments_url = pr["review_comments_url"]
    try:
        review_comments = make_github_request(review_comments_url, token)
        for comment in review_comments:
            total_comments += 1
            if comment.get("user", {}).get("type") == "Bot":
                bot_comments += 1
    except GitHubAPIError as e:
        logging.debug("Failed to fetch review comments for PR #%d: %s", pr["number"], e)

    return total_comments, bot_comments


def get_review_metrics(
    pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]
) -> tuple[int, int, int]:
    """
    Get review metrics: changes requested count, unique change requesters, approvals.
    Returns (changes_requested_count, unique_change_requesters, approval_count).
    """
    reviews_url = f"{GITHUB_API_BASE}/repos/{owner}/{repo}/pulls/{pr['number']}/reviews"

    try:
        reviews = make_github_request(reviews_url, token)
    except GitHubAPIError as e:
        logging.debug("Failed to fetch reviews for PR #%d: %s", pr["number"], e)
        return 0, 0, 0

    # Count total change requests
    changes_requested_count = sum(1 for r in reviews if r.get("state") == "CHANGES_REQUESTED")

    # Count unique reviewers who requested changes
    change_requesters = {
        r["user"]["login"]
        for r in reviews
        if r.get("state") == "CHANGES_REQUESTED" and r.get("user")
    }
    unique_change_requesters = len(change_requesters)

    # Get most recent review state per reviewer for approvals
    reviewer_states: Dict[str, str] = {}
    for review in reviews:
        if review.get("user") and review.get("state"):
            user = review["user"]["login"]
            # Later reviews override earlier ones
            reviewer_states[user] = review["state"]

    approval_count = sum(1 for state in reviewer_states.values() if state == "APPROVED")

    return changes_requested_count, unique_change_requesters, approval_count


def determine_pr_status(pr: Dict[str, Any]) -> str:
    """Determine the current status of the PR."""
    if pr.get("draft"):
        return "draft"
    elif pr.get("merged_at"):
        return "merged"
    elif pr.get("state") == "closed":
        return "closed"
    else:
        return "open"


def process_pr(pr: Dict[str, Any], owner: str, repo: str, token: Optional[str]) -> Dict[str, Any]:
    """Process a single PR and extract all metrics."""
    pr_number = pr["number"]
    logging.debug("Processing PR #%d: %s", pr_number, pr["title"])

    errors = []
    metrics: Dict[str, Any] = {
        "pr_number": pr_number,
        "title": pr["title"],
        "author": pr["user"]["login"] if pr.get("user") else "",
        "created_at": pr["created_at"],
        "url": pr["html_url"],
    }

    # Ready for review time
    try:
        metrics["ready_for_review_at"] = get_ready_for_review_time(pr, owner, repo, token)
    except Exception as e:
        logging.debug("Error getting ready time for PR #%d: %s", pr_number, e)
        metrics["ready_for_review_at"] = pr["created_at"]
        errors.append(f"ready_time: {e}")

    # Comment counts
    try:
        total_comments, bot_comments = count_comments(pr, owner, repo, token)
        metrics["total_comment_count"] = total_comments
        metrics["bot_comment_count"] = bot_comments
    except Exception as e:
        logging.debug("Error counting comments for PR #%d: %s", pr_number, e)
        metrics["total_comment_count"] = 0
        metrics["bot_comment_count"] = 0
        errors.append(f"comments: {e}")

    # Review metrics
    try:
        changes_requested, unique_requesters, approvals = get_review_metrics(pr, owner, repo, token)
        metrics["changes_requested_count"] = changes_requested
        metrics["unique_change_requesters"] = unique_requesters
        metrics["approval_count"] = approvals
    except Exception as e:
        logging.debug("Error getting reviews for PR #%d: %s", pr_number, e)
        metrics["changes_requested_count"] = 0
        metrics["unique_change_requesters"] = 0
        metrics["approval_count"] = 0
        errors.append(f"reviews: {e}")

    # Status
    metrics["status"] = determine_pr_status(pr)

    # Merged and closed timestamps
    metrics["merged_at"] = pr.get("merged_at") or ""
    metrics["closed_at"] = pr.get("closed_at") or ""

    # Errors
    metrics["errors"] = "; ".join(errors) if errors else ""

    return metrics


def write_csv_output(metrics: List[Dict[str, Any]], output_file: Optional[str]) -> None:
    """Write metrics to CSV file or stdout."""
    if not metrics:
        logging.warning("No pull requests to output")
        return

    fieldnames = [
        "pr_number",
        "title",
        "author",
        "created_at",
        "ready_for_review_at",
        "merged_at",
        "closed_at",
        "total_comment_count",
        "bot_comment_count",
        "changes_requested_count",
        "unique_change_requesters",
        "approval_count",
        "status",
        "url",
        "errors",
    ]

    if output_file:
        logging.info("Writing output to %s", output_file)
        with open(output_file, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(metrics)
    else:
        writer = csv.DictWriter(sys.stdout, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(metrics)


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Generate CSV reports of GitHub pull request metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze PRs from current repository (last year)
  gh-pr-metrics

  # Specify repository explicitly
  gh-pr-metrics --owner microsoft --repo vscode

  # Custom time range
  gh-pr-metrics --start 2024-01-01 --end 2024-12-31

  # Output to file
  gh-pr-metrics --output metrics.csv

Environment Variables:
  GITHUB_TOKEN    GitHub personal access token for authentication
        """,
    )

    parser.add_argument("--owner", help="Repository owner (default: auto-detect from git)")
    parser.add_argument("--repo", help="Repository name (default: auto-detect from git)")
    parser.add_argument(
        "--start",
        help="Start timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: 365 days ago",
    )
    parser.add_argument(
        "--end",
        help="End timestamp (ISO 8601, RFC 3339, or Unix timestamp). Default: now",
    )
    parser.add_argument("--output", "-o", help="Output file path (default: stdout)")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument(
        "--workers",
        type=int,
        default=DEFAULT_WORKERS,
        help=f"Number of parallel workers for processing PRs (default: {DEFAULT_WORKERS})",
    )
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    return parser.parse_args()


def main() -> int:
    """Main entry point."""
    args = parse_arguments()
    setup_logging(args.debug)

    logging.info("GitHub PR Metrics Tool v%s", __version__)
    logging.debug("Arguments: %s", args)

    # Get repository info
    owner = args.owner
    repo = args.repo

    if not owner or not repo:
        detected_owner, detected_repo = get_repo_from_git()
        owner = owner or detected_owner
        repo = repo or detected_repo

    if not owner or not repo:
        logging.error(
            "Could not determine repository. "
            "Use --owner and --repo or run from a git repository."
        )
        return 1

    logging.info("Repository: %s/%s", owner, repo)

    # Get time range
    end_date = parse_timestamp(args.end) if args.end else datetime.now(timezone.utc)
    start_date = (
        parse_timestamp(args.start) if args.start else end_date - timedelta(days=DEFAULT_DAYS_BACK)
    )

    if start_date >= end_date:
        logging.error("Start date must be before end date")
        return 1

    logging.info("Date range: %s to %s", start_date.isoformat(), end_date.isoformat())

    # Get GitHub token
    token = get_github_token()
    if not token:
        logging.warning(
            "No GITHUB_TOKEN found. API rate limits will be restrictive "
            "for unauthenticated requests."
        )
        # Force single worker to avoid hitting rate limits too quickly
        if args.workers > 1:
            logging.warning("Forcing --workers=1 due to missing GITHUB_TOKEN")
            workers = 1
        else:
            workers = args.workers
    else:
        workers = args.workers

    # Check and display rate limit status
    check_rate_limit(token)

    try:
        # Fetch PRs
        prs = fetch_pull_requests(owner, repo, start_date, end_date, token)

        if not prs:
            logging.warning("No pull requests found in the specified date range")
            return 0

        # Process each PR with thread pool
        metrics = []
        total_prs = len(prs)
        completed = 0

        logging.info("Processing %d PRs with %d workers", total_prs, workers)

        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all PR processing tasks
            future_to_pr = {executor.submit(process_pr, pr, owner, repo, token): pr for pr in prs}

            # Collect results as they complete
            for future in as_completed(future_to_pr):
                pr = future_to_pr[future]
                completed += 1
                try:
                    pr_metrics = future.result()
                    metrics.append(pr_metrics)
                    logging.info("Completed PR %d/%d: #%d", completed, total_prs, pr["number"])
                except Exception as e:
                    logging.error("Failed to process PR #%d: %s", pr["number"], e)
                    # Continue processing other PRs

        # Write output
        write_csv_output(metrics, args.output)

        logging.info("Successfully processed %d pull requests", len(metrics))
        return 0

    except GitHubAPIError as e:
        logging.error("GitHub API error: %s", e)
        return 1
    except Exception as e:
        logging.error("Unexpected error: %s", e, exc_info=args.debug)
        return 1


if __name__ == "__main__":
    sys.exit(main())
