#!/usr/bin/env python3
# PYTHON_ARGCOMPLETE_OK
# Generated By: Cursor (Claude Sonnet 4.5)
"""
GitHub PR Comment Extraction Tool

Extracts detailed comment and review data from recent pull requests
for analysis purposes. Fetches full PR data including all comments,
reviews, and actor information.
"""

import argparse
import csv
import json
import logging
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, List, Dict, Any

try:
    import argcomplete

    ARGCOMPLETE_AVAILABLE = True
except ImportError:
    ARGCOMPLETE_AVAILABLE = False

import yaml
from dateutil import parser as date_parser

from github_api import GitHubAPIError, GitHubClient, QuotaManager, API_CALLS_PER_PR

# Version
__version__ = "0.1.0"

# Constants
STATE_FILE = Path.home() / ".gh-pr-metrics-state.yaml"
PR_NUMBER_FIELD = "pr_number"


def setup_logging(debug: bool = False) -> None:
    """Configure logging."""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(sys.stderr)],
    )


def load_state_file(state_file_path: Path) -> Dict[str, Any]:
    """Load state file containing tracked repositories."""
    if not state_file_path.exists():
        return {}

    try:
        with open(state_file_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}
            return data
    except Exception as e:
        logging.warning("Failed to load state file %s: %s", state_file_path, e)
        return {}


def get_tracked_repos_with_csv(state_file_path: Path, csv_dir: Path) -> List[Dict[str, str]]:
    """
    Get list of tracked repositories that have CSV files.
    Returns list of dicts with 'owner', 'repo', 'csv_file' keys.
    """
    state = load_state_file(state_file_path)
    repos = []

    for repo_url, entry in state.items():
        # Parse URL to get owner/repo
        # Format: https://github.com/owner/repo
        try:
            parts = repo_url.rstrip("/").split("/")
            if len(parts) >= 2:
                owner = parts[-2]
                repo = parts[-1]

                if not isinstance(entry, dict):
                    logging.warning("Skipping %s: invalid state format", repo_url)
                    continue

                csv_file = entry.get("csv_file")
                if not csv_file:
                    logging.debug("Skipping %s/%s: no CSV file in state", owner, repo)
                    continue

                # Check if CSV file exists
                csv_path = Path(csv_file)
                if not csv_path.exists():
                    logging.debug("Skipping %s/%s: CSV file not found: %s", owner, repo, csv_file)
                    continue

                repos.append({"owner": owner, "repo": repo, "csv_file": csv_file})
        except Exception as e:
            logging.warning("Failed to parse state entry for %s: %s", repo_url, e)
            continue

    return repos


def read_prs_from_csv(csv_file: str, start_date: datetime, count: int) -> List[Dict[str, Any]]:
    """
    Read PRs from CSV file, filter by start_date, and return top N most recently created.
    Returns list of dicts with pr_number and created_at.
    """
    if not os.path.exists(csv_file):
        return []

    try:
        prs = []
        with open(csv_file, "r", newline="", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Parse created_at
                created_at_str = row.get("created_at", "")
                if not created_at_str:
                    continue

                try:
                    created_at = date_parser.parse(created_at_str)
                    # Make timezone-aware if naive
                    if created_at.tzinfo is None:
                        created_at = created_at.replace(tzinfo=timezone.utc)
                except Exception:
                    continue

                # Filter by start date
                if created_at < start_date:
                    continue

                pr_number = int(row[PR_NUMBER_FIELD])
                prs.append({"pr_number": pr_number, "created_at": created_at})

        # Sort by created_at descending (most recent first)
        prs.sort(key=lambda x: x["created_at"], reverse=True)

        # Return top N
        return prs[:count]

    except Exception as e:
        logging.error("Failed to read CSV %s: %s", csv_file, e)
        return []


def fetch_pr_details(
    client: GitHubClient, owner: str, repo: str, pr_number: int
) -> Optional[Dict[str, Any]]:
    """
    Fetch complete PR details including all comments and reviews.
    Returns dict with structured data ready for JSON serialization.
    """
    try:
        # Fetch base PR data
        pr = client.fetch_single_pr(owner, repo, pr_number)

        # Fetch issue comments
        issue_comments = []
        try:
            comments = client.fetch_issue_comments(pr["comments_url"])
            for comment in comments:
                issue_comments.append(
                    {
                        "id": comment.get("id"),
                        "user": comment.get("user", {}).get("login"),
                        "user_type": comment.get("user", {}).get("type"),
                        "body": comment.get("body", ""),
                        "created_at": comment.get("created_at"),
                        "updated_at": comment.get("updated_at"),
                    }
                )
        except GitHubAPIError as e:
            logging.warning("Failed to fetch issue comments for PR #%d: %s", pr_number, e)

        # Fetch review comments
        review_comments = []
        try:
            comments = client.fetch_review_comments(pr["review_comments_url"])
            for comment in comments:
                review_comments.append(
                    {
                        "id": comment.get("id"),
                        "user": comment.get("user", {}).get("login"),
                        "user_type": comment.get("user", {}).get("type"),
                        "body": comment.get("body", ""),
                        "path": comment.get("path"),
                        "line": comment.get("line"),
                        "created_at": comment.get("created_at"),
                        "updated_at": comment.get("updated_at"),
                    }
                )
        except GitHubAPIError as e:
            logging.warning("Failed to fetch review comments for PR #%d: %s", pr_number, e)

        # Fetch reviews
        reviews = []
        try:
            review_list = client.fetch_reviews(owner, repo, pr_number)
            for review in review_list:
                reviews.append(
                    {
                        "id": review.get("id"),
                        "user": review.get("user", {}).get("login"),
                        "user_type": review.get("user", {}).get("type"),
                        "state": review.get("state"),
                        "body": review.get("body", ""),
                        "submitted_at": review.get("submitted_at"),
                    }
                )
        except GitHubAPIError as e:
            logging.warning("Failed to fetch reviews for PR #%d: %s", pr_number, e)

        # Build structured output
        return {
            "pr_number": pr["number"],
            "title": pr["title"],
            "author": pr.get("user", {}).get("login"),
            "author_type": pr.get("user", {}).get("type"),
            "state": pr["state"],
            "created_at": pr["created_at"],
            "updated_at": pr["updated_at"],
            "merged_at": pr.get("merged_at"),
            "closed_at": pr.get("closed_at"),
            "draft": pr.get("draft", False),
            "url": pr["html_url"],
            "additions": pr.get("additions", 0),
            "deletions": pr.get("deletions", 0),
            "changed_files": pr.get("changed_files", 0),
            "issue_comments": issue_comments,
            "review_comments": review_comments,
            "reviews": reviews,
        }

    except GitHubAPIError as e:
        logging.error("Failed to fetch PR #%d: %s", pr_number, e)
        return None


def write_pr_json(output_dir: Path, owner: str, repo: str, pr_data: Dict[str, Any]) -> None:
    """Write PR data to JSON file in nested directory structure."""
    # Create directory structure: output_dir/owner/repo/
    repo_dir = output_dir / owner / repo
    repo_dir.mkdir(parents=True, exist_ok=True)

    # Write JSON file
    pr_number = pr_data["pr_number"]
    output_file = repo_dir / f"{pr_number}.json"

    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(pr_data, f, indent=2, ensure_ascii=False)
        logging.info("Wrote PR data to %s", output_file)
    except Exception as e:
        logging.error("Failed to write %s: %s", output_file, e)
        raise


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Extract detailed comment data from recent GitHub pull requests",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Extract 1 most recent PR per repo (created since 2024-11-01)
  gh-pr-extract-comments --csv-dir data/ --output-dir pr-analysis/ --start-date 2024-11-01

  # Extract 5 most recent PRs per repo
  gh-pr-extract-comments --csv-dir data/ --output-dir pr-analysis/ --start-date 2024-11-01 --count 5

Environment Variables:
  GITHUB_TOKEN    GitHub personal access token for authentication
        """,
    )

    parser.add_argument(
        "--csv-dir",
        required=True,
        help="Directory containing CSV files from gh-pr-metrics",
    )
    parser.add_argument(
        "--output-dir",
        required=True,
        help="Output directory for PR JSON files (creates nested owner/repo/ structure)",
    )
    parser.add_argument(
        "--start-date",
        required=True,
        help="Only consider PRs created on or after this date (ISO 8601 format)",
    )
    parser.add_argument(
        "--count",
        type=int,
        default=1,
        help="Number of most recent PRs to extract per repository (default: 1)",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument("--version", action="version", version=f"%(prog)s {__version__}")

    if ARGCOMPLETE_AVAILABLE:
        argcomplete.autocomplete(parser)

    return parser.parse_args()


def main() -> int:
    """Main entry point."""
    args = parse_arguments()
    setup_logging(args.debug)

    logging.info("GitHub PR Comment Extraction Tool v%s", __version__)

    # Validate directories
    csv_dir = Path(args.csv_dir)
    if not csv_dir.exists():
        logging.error("CSV directory not found: %s", csv_dir)
        return 1

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Parse start date
    try:
        start_date = date_parser.parse(args.start_date)
        if start_date.tzinfo is None:
            start_date = start_date.replace(tzinfo=timezone.utc)
    except Exception as e:
        logging.error("Invalid start date '%s': %s", args.start_date, e)
        return 1

    logging.info("Start date: %s", start_date.isoformat())
    logging.info("PRs per repository: %d", args.count)

    # Get GitHub token
    token = os.getenv("GITHUB_TOKEN")
    if not token:
        logging.warning("No GITHUB_TOKEN found. API rate limits will be restrictive.")

    # Initialize GitHub client and quota manager
    quota_manager = QuotaManager()
    github_client = GitHubClient(token, quota_manager)

    # Initialize quota
    rate_info = quota_manager.initialize(token)
    if rate_info:
        logging.info(
            "Rate limit: %d/%d remaining",
            rate_info.get("remaining", "unknown"),
            rate_info.get("limit", "unknown"),
        )

    # Get tracked repositories with CSV files
    logging.info("Scanning for repositories with CSV files...")
    repos = get_tracked_repos_with_csv(STATE_FILE, csv_dir)

    if not repos:
        logging.error("No repositories with CSV files found in state file")
        return 1

    logging.info("Found %d repositories with CSV files", len(repos))

    # Collect PRs to process
    prs_to_process = []
    for repo_info in repos:
        owner = repo_info["owner"]
        repo = repo_info["repo"]
        csv_file = repo_info["csv_file"]

        logging.info("[%s/%s] Reading PRs from %s", owner, repo, csv_file)
        prs = read_prs_from_csv(csv_file, start_date, args.count)

        if not prs:
            logging.info("[%s/%s] No PRs found matching criteria", owner, repo)
            continue

        logging.info("[%s/%s] Selected %d PRs for extraction", owner, repo, len(prs))
        for pr in prs:
            prs_to_process.append(
                {
                    "owner": owner,
                    "repo": repo,
                    "pr_number": pr["pr_number"],
                    "created_at": pr["created_at"],
                }
            )

    if not prs_to_process:
        logging.error("No PRs found matching criteria across all repositories")
        return 1

    # Calculate total API calls
    total_api_calls = len(prs_to_process) * API_CALLS_PER_PR
    logging.info("=" * 80)
    logging.info("Total PRs to process: %d", len(prs_to_process))
    logging.info("Estimated API calls: %d (%d per PR)", total_api_calls, API_CALLS_PER_PR)

    # Check quota (all-or-nothing)
    remaining, limit, _ = quota_manager.get_current_quota()
    if remaining < total_api_calls:
        logging.error(
            "Insufficient API quota: need %d calls, only %d available (limit: %d)",
            total_api_calls,
            remaining,
            limit,
        )
        logging.error("Operation aborted - all-or-nothing policy")
        return 1

    logging.info("Quota check passed: %d/%d available", remaining, limit)
    logging.info("=" * 80)

    # Process PRs
    success_count = 0
    fail_count = 0

    for i, pr_info in enumerate(prs_to_process, 1):
        owner = pr_info["owner"]
        repo = pr_info["repo"]
        pr_number = pr_info["pr_number"]

        logging.info(
            "[%d/%d] Fetching %s/%s PR #%d (created: %s)",
            i,
            len(prs_to_process),
            owner,
            repo,
            pr_number,
            pr_info["created_at"].strftime("%Y-%m-%d"),
        )

        pr_data = fetch_pr_details(github_client, owner, repo, pr_number)
        if pr_data:
            try:
                write_pr_json(output_dir, owner, repo, pr_data)
                success_count += 1
            except Exception as e:
                logging.error("Failed to write PR data: %s", e)
                fail_count += 1
        else:
            fail_count += 1

    # Summary
    logging.info("=" * 80)
    logging.info("Extraction complete:")
    logging.info("  Success: %d", success_count)
    logging.info("  Failed: %d", fail_count)
    logging.info("  Output directory: %s", output_dir)

    return 0 if fail_count == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
