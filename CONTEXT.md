# GitHub Pull Request Metrics Tool - LLM Context

Generated By: Cursor (Claude Opus 4)

## Project Overview

This project implements a command-line tool that generates CSV reports containing key metrics for all pull requests in a GitHub repository within a specified time range. The tool is designed to help teams analyze their pull request workflow and identify bottlenecks or trends.

## Key Features

1. **Automated PR Metrics Collection**: Fetches comprehensive data about pull requests including creation time, review readiness, comments, reviews, and current status
2. **Flexible Time Range**: Analyze PRs from any time period (default: last year)
3. **CSV Output**: Machine-readable format for further analysis in spreadsheet tools or data pipelines
4. **Smart Defaults**: Works with minimal configuration by detecting repository information from local git config

## Architecture Overview

### Component Structure
```
gh-pull-request-metrics/
├── gh_pr_metrics.py          # Main application entry point
├── gh_pr_metrics/            # Core package
│   ├── __init__.py
│   ├── cli.py               # Command-line interface and argument parsing
│   ├── api_client.py        # GitHub API client and request handling
│   ├── data_processor.py    # PR data processing and metrics calculation
│   ├── csv_writer.py        # CSV generation and output
│   └── utils.py             # Shared utilities (date handling, etc.)
├── tests/                   # Unit tests
├── integration_tests/       # Integration tests (optional, real API)
├── docs/                    # Documentation
│   ├── requirements.md      # Detailed functional/non-functional requirements
│   └── unit-test-plan.md    # Comprehensive testing strategy
└── setup.py                # Package configuration
```

### Data Flow
1. **Input**: User provides repository info and time range via CLI
2. **API Interaction**: Tool queries GitHub API for PR data
3. **Processing**: Calculates metrics (ready time, comment counts, review states)
4. **Output**: Generates CSV with all metrics

## Key Technical Decisions

### API Choice
- Uses GitHub REST API v3 (stable, well-documented)
- Supports both authenticated and unauthenticated requests
- Handles pagination for large result sets

### Authentication Strategy
- Environment variable (`GITHUB_TOKEN`) for simplicity
- Falls back to unauthenticated for public repos
- Clear error messages for auth failures

### Date Handling
- All timestamps in ISO 8601 format for consistency
- UTC normalization for accurate comparisons
- Supports multiple input formats for user convenience

### Error Handling Philosophy
- Fail gracefully with informative messages
- Continue processing other PRs if one fails
- Log errors without terminating the process
- Retry transient failures (network issues)

## Implementation Guidelines

### Code Style
- Follow PEP 8 Python style guide
- Use type hints for better code clarity
- Write descriptive variable and function names
- Keep functions focused and testable

### Testing Approach
- Mock external dependencies (GitHub API, file system)
- Test edge cases and error conditions
- Separate unit tests from integration tests
- Aim for 80%+ code coverage

### Performance Considerations
- Minimize API calls through efficient pagination
- Process data iteratively to manage memory
- Implement progress indicators for user feedback
- Respect GitHub API rate limits

## Common Patterns and Solutions

### Calculating "Ready for Review" Time
```python
# PRs that were never drafts use creation time
if not pr.draft and no_draft_events:
    ready_for_review_at = pr.created_at

# PRs converted from draft use the latest ready_for_review event
else:
    events = fetch_pr_events(pr.number)
    ready_events = [e for e in events if e.event == "ready_for_review"]
    ready_for_review_at = ready_events[-1].created_at if ready_events else pr.created_at
```

### Counting Reviews by State
```python
# Get the most recent review from each reviewer
reviews_by_user = {}
for review in pr.reviews:
    user = review.user.login
    if user not in reviews_by_user or review.submitted_at > reviews_by_user[user].submitted_at:
        reviews_by_user[user] = review

# Count by state
approvals = sum(1 for r in reviews_by_user.values() if r.state == "APPROVED")
changes_requested = sum(1 for r in reviews_by_user.values() if r.state == "CHANGES_REQUESTED")
```

### CSV Special Character Handling
```python
import csv
import io

def write_csv_row(writer, row_data):
    """CSV library handles escaping automatically"""
    writer.writerow([
        row_data.get('pr_number', ''),
        row_data.get('title', ''),  # Commas, quotes handled by csv module
        # ... other fields
    ])
```

## External Dependencies

### Required Python Packages
- `requests`: HTTP client for GitHub API
- `python-dateutil`: Flexible date parsing
- `argparse`: Command-line argument parsing (stdlib)
- `csv`: CSV generation (stdlib)

### Development Dependencies
- `pytest`: Testing framework
- `pytest-cov`: Coverage reporting
- `pytest-mock`: Mocking utilities
- `requests-mock`: Mock HTTP responses
- `freezegun`: Mock system time

## GitHub API Endpoints Used

1. **List Pull Requests**: `GET /repos/{owner}/{repo}/pulls`
   - Parameters: state=all, sort=created, direction=desc
   - Returns: Basic PR information

2. **Get Pull Request**: `GET /repos/{owner}/{repo}/pulls/{number}`
   - Returns: Detailed PR information

3. **List PR Events**: `GET /repos/{owner}/{repo}/issues/{number}/events`
   - Returns: Timeline events (including ready_for_review)

4. **List PR Reviews**: `GET /repos/{owner}/{repo}/pulls/{number}/reviews`
   - Returns: Review states and reviewers

5. **List PR Comments**: `GET /repos/{owner}/{repo}/pulls/{number}/comments`
   - Returns: Inline review comments

6. **List Issue Comments**: `GET /repos/{owner}/{repo}/issues/{number}/comments`
   - Returns: General PR comments

## Common Challenges and Solutions

### Challenge: API Rate Limits
**Solution**: 
- Check rate limit headers in responses
- Implement exponential backoff for 403 responses
- Provide clear messages about token usage
- Cache results when possible

### Challenge: Large Repositories
**Solution**:
- Use pagination efficiently (max page size)
- Process PRs iteratively, not all in memory
- Show progress indicators
- Allow interruption and resumption

### Challenge: Data Consistency
**Solution**:
- Fetch all data for a PR together
- Use the most recent state for calculations
- Handle missing or null values gracefully
- Document any assumptions

### Challenge: Time Zone Issues
**Solution**:
- Convert all times to UTC internally
- Accept timezone-aware inputs
- Output in ISO 8601 format
- Document timezone behavior

## Usage Examples

### Basic Usage
```bash
# Analyze PRs from current repository (last year)
gh-pr-metrics

# Specify repository explicitly
gh-pr-metrics --owner microsoft --repo vscode

# Custom time range
gh-pr-metrics --start 2024-01-01 --end 2024-12-31

# Output to file
gh-pr-metrics --output metrics.csv
```

### With Authentication
```bash
# Set token for private repos or higher rate limits
export GITHUB_TOKEN=ghp_xxxxxxxxxxxxx
gh-pr-metrics --owner myorg --repo private-repo
```

### For Large Repositories
```bash
# Process with progress indication
gh-pr-metrics --owner kubernetes --repo kubernetes --progress

# Limited time range to reduce data
gh-pr-metrics --start 2024-10-01 --end 2024-10-31
```

## Debugging and Troubleshooting

### Common Issues

1. **"Repository not found"**
   - Check repository exists and is accessible
   - Verify authentication for private repos
   - Confirm owner/repo names are correct

2. **"API rate limit exceeded"**
   - Add GITHUB_TOKEN for higher limits
   - Wait for rate limit reset
   - Reduce time range to fetch less data

3. **"Invalid date format"**
   - Use ISO 8601: 2024-01-01 or 2024-01-01T00:00:00Z
   - Check start is before end date
   - Verify timezone if specified

### Debug Mode
```bash
# Enable debug logging
gh-pr-metrics --debug

# Dry run (show what would be fetched)
gh-pr-metrics --dry-run
```

## Future Enhancements (Not in Current Scope)

1. **GraphQL Support**: More efficient data fetching
2. **Caching**: Store results for repeated queries
3. **Filtering**: By labels, authors, or files changed
4. **Additional Metrics**: Time to merge, review turnaround
5. **Multiple Repos**: Analyze organization-wide metrics
6. **Visualizations**: Generate charts from CSV data
7. **Real-time Mode**: Watch for new PRs via webhooks
8. **Export Formats**: JSON, Excel, HTML reports

## Related Tools and Inspiration

- **gh-pulls-summary**: Similar tool focused on open PR summaries
- **prreview**: PR review tool with worktree management
- **GitHub CLI (gh)**: Official GitHub command-line tool
- **GitHub Insights**: GitHub's built-in analytics (limited)

## Contributing Guidelines

When working on this codebase:

1. **Read the requirements** (docs/requirements.md) first
2. **Follow the test plan** (docs/unit-test-plan.md)
3. **Maintain backwards compatibility** for CLI arguments
4. **Add tests for new features** before implementing
5. **Update documentation** for any changes
6. **Use meaningful commit messages**
7. **Run tests and linting** before submitting

## Security Considerations

- Never commit tokens or credentials
- Validate all user inputs
- Sanitize data for CSV injection
- Use HTTPS for all API calls
- Follow GitHub's security best practices
- Be cautious with file system operations
